{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creazione dataset test per complicanze"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fa2424f01e0a4ca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "excel_path = \"Excel/Bilanciamento/RandomForest_Results_Bilanced_complicanze111.xlsx\"\n",
    "\n",
    "# Funzione per metriche\n",
    "def get_metrics(y_true, y_predicted):\n",
    "    matrix = confusion_matrix(y_true, y_predicted)\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "    precision = precision_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    f1score = f1_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    return matrix, accuracy, precision, recall, f1score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-01T20:51:02.792520200Z",
     "start_time": "2025-07-01T20:51:02.788160Z"
    }
   },
   "id": "c8de3268aa77741c",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T20:51:03.279844200Z",
     "start_time": "2025-07-01T20:51:02.792520200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# 1) Vengono letti i due file\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m parte1 = \u001B[43mpd\u001B[49m.read_csv(\u001B[33m\"\u001B[39m\u001B[33mExcel/Bilanciamento/parte1_bilanced.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m parte2 = pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33mExcel/Bilanciamento/parte2_v2_bilanced.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# 1.1) Viene rinominata la feature ID in Patiene_ID per perettere il merge\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 1) Vengono letti i due file\n",
    "parte1 = pd.read_csv(\"Excel/Bilanciamento/parte1_bilanced.csv\")\n",
    "parte2 = pd.read_csv(\"Excel/Bilanciamento/parte2_v2_bilanced.csv\")\n",
    "\n",
    "# 1.1) Viene rinominata la feature ID in Patiene_ID per perettere il merge\n",
    "parte2 = parte2.rename(columns={\"ID\": \"Patient_ID\"})\n",
    "\n",
    "# 1.2) Viene fatto un inner‐merge su Patient_ID:\n",
    "clust = parte1.merge(parte2, on=\"Patient_ID\", how=\"inner\")\n",
    "clust = clust.fillna(clust.mean(numeric_only=True))\n",
    "\n",
    "# 1.3) Vangono riempiti i valori null del dataset\n",
    "clust = clust.fillna(clust.median(numeric_only=True))\n",
    "\n",
    "# 2) Viene letto Diagnostics.csv per ricavare i codici associati a ciascun paziente\n",
    "diag = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "# 2.1) Per ogni paziente, viene prenso il Code con il maggior numero di occorrenze\n",
    "diag_unique = (\n",
    "    diag\n",
    "    .groupby(\"Patient_ID\")[\"Code\"]\n",
    "    .agg(lambda codes: codes.value_counts().idxmax())\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Code\": \"CodeStrat\"})\n",
    ")\n",
    "\n",
    "# 2.2) Se hai codici troppo rari vengono raggruppati sotto \"RARE\" per evitare errori di stratify\n",
    "code_counts = diag_unique[\"CodeStrat\"].value_counts()\n",
    "rare_codes = set(code_counts[code_counts < 2].index)\n",
    "diag_unique[\"CodeStrat\"] = diag_unique[\"CodeStrat\"].apply(\n",
    "    lambda c: \"RARE\" if c in rare_codes else c\n",
    ")\n",
    "\n",
    "# 3) Suddividi i soli pazienti “diagnosticati” 80/20, stratificando su CodeStrat\n",
    "diag_train_patients, diag_test_patients = train_test_split(\n",
    "    diag_unique[\"Patient_ID\"],\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=diag_unique[\"CodeStrat\"]\n",
    ")\n",
    "\n",
    "# 4) Trova i pazienti in clust che NON compaiono in diag_unique (ossia “senza diagnosi”)\n",
    "all_clust_patients = set(clust[\"Patient_ID\"].unique())\n",
    "diag_patients = set(diag_unique[\"Patient_ID\"].unique())\n",
    "no_diag_patients = list(all_clust_patients - diag_patients)\n",
    "print(f\"Numero pazienti senza diagnosi: {len(no_diag_patients)}\")\n",
    "\n",
    "# 5) Dividi i pazienti “senza diagnosi” (%) 80/20 in modo casuale\n",
    "no_diag_train, no_diag_test = train_test_split(\n",
    "    no_diag_patients,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) Costruisci la lista finale di test (diagnosticati nel 20% + senza diagnosi nel 20%)\n",
    "test_patients  = set(diag_test_patients)  | set(no_diag_test)\n",
    "train_patients = set(diag_train_patients) | set(no_diag_train)\n",
    "\n",
    "# Verifica che non ci siano sovrapposizioni\n",
    "assert train_patients.isdisjoint(test_patients), \"Errore: un paziente è in entrambi i set!\"\n",
    "\n",
    "# 7) Filtra clust in base a Patient_ID: \n",
    "#    - se Patient_ID ∈ test_patients → va in df_test\n",
    "#    - altrimenti va in df_train\n",
    "df_train = clust[~clust[\"Patient_ID\"].isin(test_patients)].copy()\n",
    "df_test  = clust[ clust[\"Patient_ID\"].isin(test_patients)].copy()\n",
    "\n",
    "# 8) Rimuovi di nuovo la colonna Patient_ID, come richiesto, prima di salvare\n",
    "#df_train = df_train.drop(columns=[\"Patient_ID\", \"Complicanze\"])\n",
    "#df_test = df_test.drop(columns=[\"Complicanze\"])\n",
    "\n",
    "# 9) Salva i due file risultanti\n",
    "df_train.to_csv(\"Excel/Bilanciamento/Classification_train_bilanced.csv\", index=False)\n",
    "df_test.to_csv(\"Excel/Bilanciamento/Classification_test_bilanced.csv\",  index=False)\n",
    "\n",
    "print(\"Suddivisione completata:\")\n",
    "print(f\"  - Train (Classification_train.csv): {len(df_train)} righe\")\n",
    "print(f\"  - Test  (Classification_test.csv):  {len(df_test)}  righe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Greedy Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "549dc7704016faf8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train = pd.read_csv(\"Excel/Bilanciamento/Classification_train_bilanced.csv\")\n",
    "test = pd.read_csv(\"Excel/Bilanciamento/Classification_test_bilanced.csv\")\n",
    "\n",
    "test_patients1 = test['Patient_ID']\n",
    "test = test.drop(columns=['Patient_ID'])\n",
    "train = train.drop(columns=['Patient_ID'])\n",
    "\n",
    "y_train = train['Has_Diagnostics']\n",
    "y_test = test['Has_Diagnostics']\n",
    "X_train = train.drop(columns=['Has_Diagnostics'])\n",
    "X_test = test.drop(columns=['Has_Diagnostics'])\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Definisci lo spazio di ricerca\n",
    "n_estimators_list = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "max_depth_list = [5, 10, 15, 20]\n",
    "\n",
    "best_config = None\n",
    "best_score = 0  \n",
    "\n",
    "prova_counter = 1\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        print(f\"▶️ Prova {prova_counter}: n_estimators={n_estimators}, max_depth={max_depth}\")\n",
    "\n",
    "        Prova = f\"{prova_counter} - Est:{n_estimators}_Depth:{max_depth}\"\n",
    "        \n",
    "        val_results = []\n",
    "\n",
    "        for i, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "            X_fold_train = X_train.iloc[train_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx]\n",
    "\n",
    "            clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "            clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            cm, acc, prec, rec, f1 = get_metrics(y_test, y_pred)\n",
    "            importances = clf.feature_importances_\n",
    "            importances_dict = {f'Imp_{feat}': imp for feat, imp in zip(X_train.columns, importances)}\n",
    "\n",
    "            val_results.append({\n",
    "                'Prova': Prova,\n",
    "                'KFold': i,\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1-Score': f1,\n",
    "                'Confusion_Matrix': cm.tolist(),\n",
    "                ' ': None,\n",
    "                **importances_dict\n",
    "            })\n",
    "\n",
    "        df_nested = pd.DataFrame(val_results)\n",
    "\n",
    "        # Calcola media F1\n",
    "        mean_f1 = df_nested['F1-Score'].mean()\n",
    "\n",
    "        # Aggiorna il best\n",
    "        if mean_f1 > best_score:\n",
    "            best_score = mean_f1\n",
    "            best_config = (n_estimators, max_depth)\n",
    "            print(f\"✅ Nuova configurazione migliore trovata! F1 medio = {mean_f1:.4f}\")\n",
    "\n",
    "        # Salva su Excel\n",
    "        if os.path.exists(excel_path):\n",
    "            with pd.ExcelFile(excel_path) as reader:\n",
    "                if 'Final1' in reader.sheet_names:\n",
    "                    prev_nested = pd.read_excel(reader, sheet_name='Final1')\n",
    "                    df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n",
    "\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "            df_nested.to_excel(writer, sheet_name='Final1', index=False)\n",
    "\n",
    "        print(f\"📁 Risultati prova {prova_counter} salvati.\")\n",
    "        prova_counter += 1\n",
    "\n",
    "print(f\"\\n🏁 Greedy search terminata.\")\n",
    "print(f\"⭐ Migliore configurazione: n_estimators={best_config[0]}, max_depth={best_config[1]}, con F1 medio = {best_score:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cce1d6d96acd1db5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"\\n🔎 Analisi degli errori sulla configurazione migliore...\")\n",
    "\n",
    "# Ricrea il classificatore con i parametri migliori\n",
    "n_estimators_best, max_depth_best = best_config\n",
    "clf_best = RandomForestClassifier(n_estimators=n_estimators_best, max_depth=max_depth_best, random_state=42)\n",
    "clf_best.fit(X_train, y_train)  # Allena sul full train\n",
    "\n",
    "# Predizione su X_test\n",
    "y_pred_best = clf_best.predict(X_test)\n",
    "\n",
    "# Crea DataFrame con gli errori\n",
    "df_resoults = pd.DataFrame({\n",
    "    'Patient_ID': test_patients1,\n",
    "    'True_Label': y_test,\n",
    "    'Predicted_Label': y_pred_best\n",
    "})\n",
    "df_resoults['Errore'] = df_resoults['True_Label'] != df_resoults['Predicted_Label']\n",
    "\n",
    "# Filtra solo gli errori\n",
    "df_errors_only = df_resoults[df_resoults['Errore'] == True].copy()\n",
    "\n",
    "diagnosi = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "df_errors_only = df_errors_only.merge(diagnosi, on=\"Patient_ID\", how=\"left\")\n",
    "\n",
    "df_errors_only = df_errors_only.drop(columns=['True_Label','Predicted_Label','Errore'])\n",
    "\n",
    "df_errors_only = df_errors_only.sort_values(['Code'])\n",
    "\n",
    "print(f\"❌ Trovati {len(df_errors_only)} errori su {len(df_resoults)} pazienti nel test set.\")\n",
    "\n",
    "# Salva su Excel\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "    df_errors_only.to_excel(writer, sheet_name='Error_Analysis1', index=False)\n",
    "\n",
    "print(\"Errori salvati sul foglio 'Error_Analysis'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be97c9f4f0eeac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "diagnosi_filtrato = diagnosi[diagnosi[\"Code\"].str.startswith(\"V\")]\n",
    "diagnosi_filtrato"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9510f95ca40d07f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
