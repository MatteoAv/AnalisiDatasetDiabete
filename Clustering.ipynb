{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CREAZIONE NUOVO DASET PARTE 1, A QUESTO DOVREMO AGGIUNGERE TIR, TAR, TBR ANDANDO AD ESEGUIRE IL CODICE RELATIVO AD OGNI CELLA NEL FILE AnalisiStatistica.ipynb\n",
    "############################################################################################\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) Carica il file\n",
    "df = pd.read_csv(\"Excel/Patient_info.csv\")\n",
    "\n",
    "# 2) Seleziona le colonne di interesse\n",
    "parte1 = df[['Patient_ID', 'Sex', 'Birth_year', 'Number_of_diagnostics']].copy()\n",
    "\n",
    "# 3) Mappa Sex in binario: 1 = maschio, 0 = femmina\n",
    "#    Adatta la mappatura ai valori esatti presenti nel tuo CSV (es. 'Male'/'Female', 'M'/'F', ecc.)\n",
    "parte1['Sex'] = parte1['Sex'].map({'M': 1, 'F': 0})\n",
    "\n",
    "# 4) Trasforma Number_of_diagnostics in variabile binaria Has_Diagnostics\n",
    "parte1['Has_Diagnostics'] = (parte1['Number_of_diagnostics'] >= 1).astype(int)\n",
    "\n",
    "# 5) Calcola l'età a partire dall'anno di nascita e sostituisci Birth_year\n",
    "current_year = datetime.now().year\n",
    "parte1['Age'] = current_year - parte1['Birth_year']\n",
    "\n",
    "# 6) Rimuovi le colonne originali non più necessarie\n",
    "parte1 = parte1.drop(columns=['Birth_year', 'Number_of_diagnostics'])\n",
    "\n",
    "# 7) Riorganizza le colonne finali\n",
    "parte1 = parte1[['Patient_ID', 'Sex', 'Age', 'Has_Diagnostics']]\n",
    "\n",
    "# 8) Salva su CSV\n",
    "parte1.to_csv(\"Excel/Parte1.csv\", index=False)\n",
    "\n",
    "print(\"File salvato in Excel/Parte1.csv\")\n",
    "\n"
   ],
   "id": "3a9dbe0cc4bfd97a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CREA UN DATASET PER IL CLUSTERING CHE MANTENGA LA COLONNA DELL'ID DEL PAZIENTE\n",
    "#ORIGINARIAMENTE E' STATO CREATO IL FILE Clustering.csv, ma era stata rimossa la colonna Patient_ID, QUINDI E' STATO CREATO IL NUOVO FILE Clustering2.csv CHE MANTIENE QUELLA COLONNA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Carica i due dataset\n",
    "df1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "df2 = pd.read_csv(\"Excel/parte2_v1.csv\")\n",
    "\n",
    "# Rimuove la colonna \"Complicanze\" dal secondo dataset\n",
    "df2 = df2.drop(columns=[\"Complicanze\"])\n",
    "\n",
    "# Effettua il merge tra df1 (usando Patient_ID) e df2 (usando ID)\n",
    "merged_df = pd.merge(df1, df2, left_on=\"Patient_ID\", right_on=\"ID\", how=\"inner\")\n",
    "\n",
    "merged_df = merged_df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Salva il dataframe risultante in un file CSV\n",
    "merged_df.to_csv(\"Excel/Clustering2.csv\", index=False)\n",
    "\n",
    "print(\"Merge completato. File salvato come 'Clustering2.csv'.\")\n"
   ],
   "id": "bf3568475ce119d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#RIMUOVE LA COLONNA Patient_ID DAL FILE Clustering.csv, SE USATO ORA DARA' ERRORE PERCHE' LA COLONNA GIA E' STATA RIMOSSA DAL DATASET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Carica i due dataset\n",
    "df1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "df2 = pd.read_csv(\"parte2_v1.csv\")\n",
    "\n",
    "# Rimuove la colonna \"Complicanze\" dal secondo dataset\n",
    "df2 = df2.drop(columns=[\"Complicanze\"])\n",
    "\n",
    "# Effettua il merge tra df1 (usando Patient_ID) e df2 (usando ID)\n",
    "merged_df = pd.merge(df1, df2, left_on=\"Patient_ID\", right_on=\"ID\", how=\"inner\")\n",
    "\n",
    "# Rimuove la colonna \"ID\" e \"Patient_ID\"\n",
    "merged_df = merged_df.drop(columns=[\"ID\",\"Patient_ID\"])\n",
    "\n",
    "# Salva il dataframe risultante in un file CSV\n",
    "merged_df.to_csv(\"Excel/Clustering.csv\", index=False)\n",
    "\n",
    "print(\"Merge completato. File salvato come 'Clustering.csv'.\")\n"
   ],
   "id": "87360da827d29d78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CREA UN DATASET PER IL CLUSTERING CHE MANTENGA LA COLONNA DELL'ID DEL PAZIENTE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Carica i due dataset\n",
    "df1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "df2 = pd.read_csv(\"Excel/parte2_v1.csv\")\n",
    "\n",
    "# Rimuove la colonna \"Complicanze\" dal secondo dataset\n",
    "df2 = df2.drop(columns=[\"Complicanze\"])\n",
    "\n",
    "# Effettua il merge tra df1 (usando Patient_ID) e df2 (usando ID)\n",
    "merged_df = pd.merge(df1, df2, left_on=\"Patient_ID\", right_on=\"ID\", how=\"inner\")\n",
    "\n",
    "merged_df = merged_df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Salva il dataframe risultante in un file CSV\n",
    "merged_df.to_csv(\"Excel/Clustering2.csv\", index=False)\n",
    "\n",
    "print(\"Merge completato. File salvato come 'Clustering2.csv'.\")\n"
   ],
   "id": "29f8c53ff7d80cfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CLUSTERING CON KMEANS, USANDO 4 METODI DI IMPUTAZIONE DIVERSI E USANDO DIVERSI IPERPARAMETRI DELLA FUNZIONE, TUTTI I RISULTATI OTTENUTI SONO STATI STAMPATI CON LE RELATIVE CARATTERISTICHE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, silhouette_score, adjusted_rand_score\n",
    "\n",
    "df = pd.read_csv(\"Excel/Clustering.csv\")\n",
    "\n",
    "X = df.drop(columns=[\"Has_Diagnostics\"])\n",
    "y = df[\"Has_Diagnostics\"]\n",
    "\n",
    "# Definisci i quattro imputatori\n",
    "imputers = {\n",
    "    # Sostituisce ogni valore mancante con la media aritmetica della colonna\n",
    "    \"Mean\":    SimpleImputer(strategy=\"mean\"),\n",
    "\n",
    "    # Sostituisce ogni valore mancante con la mediana (valore centrale) della colonna,\n",
    "    \"Median\":  SimpleImputer(strategy=\"median\"),\n",
    "\n",
    "    # Per ogni riga con missing, calcola i k (=5) pazienti più “simili” sulle altre feature\n",
    "    # e sostituisce i NaN con la media dei valori corrispondenti di quei vicini\n",
    "    \"KNN\":     KNNImputer(n_neighbors=5),\n",
    "\n",
    "     # Imputa in modo iterativo (MICE):\n",
    "    # 1) Inizializza i NaN (es. con la media)\n",
    "    # 2) Per ogni colonna con missing, allena un modello sulle altre variabili per prevedere i NaN\n",
    "    # 3) Ripete la procedura a catena per max_iter volte, migliorando progressivamente le stime\n",
    "    \"MICE\":    IterativeImputer(max_iter=10, random_state=42)\n",
    "}\n",
    "\n",
    "# 4) Griglia di iperparametri per KMeans\n",
    "param_grid = {\n",
    "    'n_clusters': [2],              # fisso a 2\n",
    "    'init':       ['k-means++', 'random'],\n",
    "    'n_init':     [10, 20],\n",
    "    'max_iter':   [100, 300],\n",
    "    'tol':        [1e-4, 1e-3],\n",
    "    'algorithm':  ['lloyd', 'elkan']\n",
    "}\n",
    "\n",
    "# 5) Funzione di valutazione\n",
    "def evaluate(imputer_name, imputer, params):\n",
    "    # a) Imputa i dati mancanti con il metodo scelto\n",
    "    X_imp = imputer.fit_transform(X)\n",
    "    # b) Standardizzazione\n",
    "    X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "    # c) Configura e applica KMeans\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=params['n_clusters'],\n",
    "        init=params['init'],\n",
    "        n_init=params['n_init'],\n",
    "        max_iter=params['max_iter'],\n",
    "        tol=params['tol'],\n",
    "        algorithm=params['algorithm'],\n",
    "        random_state=42\n",
    "    )\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    # --- Statistiche demografiche e diagnostici per cluster ---\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp['Cluster'] = clusters\n",
    "\n",
    "    # Fasce d'età\n",
    "    bins   = [0, 29, 49, 69, np.inf]\n",
    "    labels = ['<30', '30-49', '50-69', '70+']\n",
    "    df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    # Conta i diagnosi per cluster × age group × sesso\n",
    "    diag_counts = (\n",
    "        df_tmp\n",
    "        .groupby(['Cluster', 'AgeGroup', 'Sex'], observed=True)['Has_Diagnostics']\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .rename(columns={0: 'No_Dx', 1: 'Yes_Dx'})\n",
    "    )\n",
    "    # print(f\"\\n>> Diagnosi (Has_Diagnostics) per cluster, fasce d'età e sesso:\")\n",
    "    # print(diag_counts)\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # d) Metriche\n",
    "    sil = silhouette_score(X_scaled, clusters)\n",
    "    ari = adjusted_rand_score(y, clusters)\n",
    "    db_index = davies_bouldin_score(X_scaled, clusters)\n",
    "    ch_index = calinski_harabasz_score(X_scaled, clusters)\n",
    "    cm  = confusion_matrix(y, clusters)\n",
    "    return sil, ari, db_index, ch_index, cm\n",
    "\n",
    "# 6) Loop su imputatori e griglia\n",
    "results = []\n",
    "# essenzialmente con qeusto ciclo facciamo si che per ogni strategia di imputazione (media, mediana, KNN, MICE) vengano provate tutte le combinazioni di parametri\n",
    "for imp_name, imp in imputers.items():\n",
    "    for init in param_grid['init']:\n",
    "        for n_init in param_grid['n_init']:\n",
    "            for max_iter in param_grid['max_iter']:\n",
    "                for tol in param_grid['tol']:\n",
    "                    for algo in param_grid['algorithm']:\n",
    "                        params = {\n",
    "                            'n_clusters': 2,\n",
    "                            'init':       init,\n",
    "                            'n_init':     n_init,\n",
    "                            'max_iter':   max_iter,\n",
    "                            'tol':        tol,\n",
    "                            'algorithm':  algo\n",
    "                        }\n",
    "                        sil, ari, db_index, ch_index, cm = evaluate(imp_name, imp, params)\n",
    "                        results.append({\n",
    "                            'Imputer': imp_name,\n",
    "                            **params,\n",
    "                            'Silhouette': sil,\n",
    "                            'ARI': ari,\n",
    "                            'Davies-Bouldin Index': db_index,\n",
    "                            'Calinski-Harabasz Index': ch_index,\n",
    "                            'ConfusionMatrix': cm\n",
    "                        })\n",
    "                        print(\n",
    "                            f\"{imp_name} | init={init}, n_init={n_init}, \"\n",
    "                            f\"max_iter={max_iter}, tol={tol}, algo={algo} \"\n",
    "                            f\"-> silhouette={sil:.3f}, ARI={ari}, Davies-Bouldin Index={db_index}, Calinski-Harabasz Index={ch_index}\"\n",
    "                        )\n",
    "                        print(\"Confusion Matrix:\\n\", cm)"
   ],
   "id": "c0a3f87c30f0246f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODICE PER RESTITUIRE I CLUSTER OTTENUTI CON KMEANS CHE HANNO IL VALORE DI ARI PIU ALTO, PER OGNI RISULTATO E' STATO MOSTRATO IL GRAFICO CON DISTRIBUZIONE DI SESSO, ETA E COMPLICANZE.\n",
    "# I RISULTATI OTTENUTI SONO TUTTI UGUALI, CON STESSO ARI E STESSA MATRICE DI CONFUSIONE\n",
    "#\n",
    "# import pandas as pd\n",
    "# from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import confusion_matrix, silhouette_score, adjusted_rand_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "#\n",
    "# df = pd.read_csv(\"Excel/Clustering.csv\")\n",
    "#\n",
    "# X = df.drop(columns=[\"Has_Diagnostics\"])\n",
    "# y = df[\"Has_Diagnostics\"]\n",
    "#\n",
    "# # Definisci i quattro imputatori\n",
    "# imputers = {\n",
    "#     # Sostituisce ogni valore mancante con la media aritmetica della colonna\n",
    "#     \"Mean\":    SimpleImputer(strategy=\"mean\"),\n",
    "#\n",
    "#     # Sostituisce ogni valore mancante con la mediana (valore centrale) della colonna,\n",
    "#     \"Median\":  SimpleImputer(strategy=\"median\"),\n",
    "#\n",
    "#     # Per ogni riga con missing, calcola i k (=5) pazienti più “simili” sulle altre feature\n",
    "#     # e sostituisce i NaN con la media dei valori corrispondenti di quei vicini\n",
    "#     \"KNN\":     KNNImputer(n_neighbors=5),\n",
    "#\n",
    "#      # Imputa in modo iterativo (MICE):\n",
    "#     # 1) Inizializza i NaN (es. con la media)\n",
    "#     # 2) Per ogni colonna con missing, allena un modello sulle altre variabili per prevedere i NaN\n",
    "#     # 3) Ripete la procedura a catena per max_iter volte, migliorando progressivamente le stime\n",
    "#     \"MICE\":    IterativeImputer(max_iter=10, random_state=42)\n",
    "# }\n",
    "#\n",
    "# # 4) Griglia di iperparametri per KMeans\n",
    "# param_grid = {\n",
    "#     'n_clusters': [2],              # fisso a 2\n",
    "#     'init':       ['k-means++', 'random'],\n",
    "#     'n_init':     [10, 20],\n",
    "#     'max_iter':   [100, 300],\n",
    "#     'tol':        [1e-4, 1e-3],\n",
    "#     'algorithm':  ['lloyd', 'elkan']\n",
    "# }\n",
    "#\n",
    "# # 5) Funzione di valutazione\n",
    "# def evaluate(imputer_name, imputer, params):\n",
    "#     # a) Imputa i dati mancanti con il metodo scelto\n",
    "#     X_imp = imputer.fit_transform(X)\n",
    "#     # b) Standardizzazione\n",
    "#     X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "#     # c) Configura e applica KMeans\n",
    "#     kmeans = KMeans(\n",
    "#         n_clusters=params['n_clusters'],\n",
    "#         init=params['init'],\n",
    "#         n_init=params['n_init'],\n",
    "#         max_iter=params['max_iter'],\n",
    "#         tol=params['tol'],\n",
    "#         algorithm=params['algorithm'],\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     clusters = kmeans.fit_predict(X_scaled)\n",
    "#     # --- Statistiche demografiche e diagnostici per cluster ---\n",
    "#     df_tmp = df.copy()\n",
    "#     df_tmp['Cluster'] = clusters\n",
    "#\n",
    "#     # Fasce d'età\n",
    "#     bins   = [0, 29, 49, 69, np.inf]\n",
    "#     labels = ['<30', '30-49', '50-69', '70+']\n",
    "#     df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "#\n",
    "#     # Conta i diagnosi per cluster × age group × sesso\n",
    "#     diag_counts = (\n",
    "#         df_tmp\n",
    "#         .groupby(['Cluster', 'AgeGroup', 'Sex'], observed=True)['Has_Diagnostics']\n",
    "#         .value_counts()\n",
    "#         .unstack(fill_value=0)\n",
    "#         .rename(columns={0: 'No_Dx', 1: 'Yes_Dx'})\n",
    "#     )\n",
    "#     # print(f\"\\n>> Diagnosi (Has_Diagnostics) per cluster, fasce d'età e sesso:\")\n",
    "#     # print(diag_counts)\n",
    "#     # --------------------------------------------------------------\n",
    "#\n",
    "#\n",
    "#     # d) Metriche\n",
    "#     sil = silhouette_score(X_scaled, clusters)\n",
    "#     ari = adjusted_rand_score(y, clusters)\n",
    "#     cm  = confusion_matrix(y, clusters)\n",
    "#     return sil, ari, cm\n",
    "#\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "#\n",
    "# # Funzione per generare il grafico\n",
    "# def plot_cluster_demographics(df_tmp):\n",
    "#     # Creazione delle fasce d'età\n",
    "#     bins = [0, 29, 49, 69, np.inf]\n",
    "#     labels = ['<30', '30-49', '50-69', '>69']\n",
    "#     df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "#\n",
    "#     # Preparazione dei dati per il plot\n",
    "#     cluster_data = []\n",
    "#     for cluster in [0, 1]:\n",
    "#         cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "#\n",
    "#         age_counts = []\n",
    "#         for age_group in labels:\n",
    "#             age_df = cluster_df[cluster_df['AgeGroup'] == age_group]\n",
    "#\n",
    "#             # Maschi con complicanze\n",
    "#             male_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 1)])\n",
    "#             # Maschi senza complicanze\n",
    "#             male_no_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 0)])\n",
    "#             # Femmine con complicanze\n",
    "#             female_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 1)])\n",
    "#             # Femmine senza complicanze\n",
    "#             female_no_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 0)])\n",
    "#\n",
    "#             age_counts.append({\n",
    "#                 'Male_Dx': male_dx,\n",
    "#                 'Male_NoDx': male_no_dx,\n",
    "#                 'Female_Dx': female_dx,\n",
    "#                 'Female_NoDx': female_no_dx,\n",
    "#                 'Total': male_dx + male_no_dx + female_dx + female_no_dx\n",
    "#             })\n",
    "#\n",
    "#         cluster_data.append(age_counts)\n",
    "#\n",
    "#     # Creazione del grafico\n",
    "#     fig, ax = plt.subplots(figsize=(14, 8))\n",
    "#\n",
    "#     # Definizione dei colori\n",
    "#     colors = {\n",
    "#         'Male_Dx': '#1f77b4',      # Blu scuro\n",
    "#         'Male_NoDx': '#a5c8e4',    # Blu chiaro\n",
    "#         'Female_Dx': '#e377c2',    # Rosa scuro\n",
    "#         'Female_NoDx': '#f8b8d8'   # Rosa chiaro\n",
    "#     }\n",
    "#\n",
    "#     bar_width = 0.35\n",
    "#     x = np.arange(len(labels))\n",
    "#\n",
    "#     for i, cluster in enumerate([0, 1]):\n",
    "#         # Posizioni delle barre per ogni cluster\n",
    "#         positions = x + i * bar_width\n",
    "#\n",
    "#         # Creazione delle barre stackate per ogni gruppo\n",
    "#         bottom = np.zeros(len(labels))\n",
    "#\n",
    "#         # Lista per memorizzare le posizioni e i valori delle etichette\n",
    "#         label_positions = []\n",
    "#         label_values = []\n",
    "#\n",
    "#         for segment in ['Male_Dx', 'Male_NoDx', 'Female_Dx', 'Female_NoDx']:\n",
    "#             counts = [cluster_data[cluster][j][segment] for j in range(len(labels))]\n",
    "#             ax.bar(positions, counts, bar_width, bottom=bottom,\n",
    "#                    color=colors[segment], edgecolor='black',\n",
    "#                    label=f'Cluster {cluster} {segment}' if i == 0 else \"\")\n",
    "#\n",
    "#             # Aggiungi etichette per ogni segmento\n",
    "#             for j, count in enumerate(counts):\n",
    "#                 if count > 0:  # Mostra solo etichette per valori > 0\n",
    "#                     ax.text(positions[j], bottom[j] + count/2, str(count),\n",
    "#                             ha='center', va='center', color='black', fontsize=8)\n",
    "#\n",
    "#             bottom += counts\n",
    "#\n",
    "#         # Aggiungi etichetta con il totale per ogni cluster e fascia d'età\n",
    "#         for j in range(len(labels)):\n",
    "#             total = cluster_data[cluster][j]['Total']\n",
    "#             ax.text(positions[j], bottom[j] + 5, f'Tot: {total}',\n",
    "#                     ha='center', va='bottom', color='black', fontsize=9, fontweight='bold')\n",
    "#\n",
    "#     # Configurazione dell'asse x\n",
    "#     ax.set_xticks(x + bar_width/2)\n",
    "#     ax.set_xticklabels(labels)\n",
    "#     ax.set_xlabel('Fascia d\\'età')\n",
    "#     ax.set_ylabel('Numero di pazienti')\n",
    "#     ax.set_title('')\n",
    "#\n",
    "#     # Creazione della legenda\n",
    "#     handles = [\n",
    "#         plt.Rectangle((0,0),1,1, color='#1f77b4', label='Maschi con complicanze'),\n",
    "#         plt.Rectangle((0,0),1,1, color='#a5c8e4', label='Maschi senza complicanze'),\n",
    "#         plt.Rectangle((0,0),1,1, color='#e377c2', label='Femmine con complicanze'),\n",
    "#         plt.Rectangle((0,0),1,1, color='#f8b8d8', label='Femmine senza complicanze'),\n",
    "#     ]\n",
    "#     ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "#\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#\n",
    "#\n",
    "# results = []\n",
    "# # Modifica il loop principale per chiamare la funzione di plotting quando trova l'ARI desiderato\n",
    "# for imp_name, imp in imputers.items():\n",
    "#     for init in param_grid['init']:\n",
    "#         for n_init in param_grid['n_init']:\n",
    "#             for max_iter in param_grid['max_iter']:\n",
    "#                 for tol in param_grid['tol']:\n",
    "#                     for algo in param_grid['algorithm']:\n",
    "#                         params = {\n",
    "#                             'n_clusters': 2,\n",
    "#                             'init':       init,\n",
    "#                             'n_init':     n_init,\n",
    "#                             'max_iter':   max_iter,\n",
    "#                             'tol':        tol,\n",
    "#                             'algorithm':  algo\n",
    "#                         }\n",
    "#                         sil, ari, cm = evaluate(imp_name, imp, params)\n",
    "#                         results.append({\n",
    "#                             'Imputer': imp_name,\n",
    "#                             **params,\n",
    "#                             'Silhouette': sil,\n",
    "#                             'ARI': ari,\n",
    "#                             'ConfusionMatrix': cm\n",
    "#                         })\n",
    "#                         if ari == 0.0012036242459496649:\n",
    "#                             print(\n",
    "#                                 f\"{imp_name} | init={init}, n_init={n_init}, \"\n",
    "#                                 f\"max_iter={max_iter}, tol={tol}, algo={algo} \"\n",
    "#                                 f\"-> silhouette={sil:.3f}, ARI={ari:.3f}\"\n",
    "#                             )\n",
    "#                             print(\"Confusion Matrix:\\n\", cm)\n",
    "#\n",
    "#                             # Prepara i dati per il plotting\n",
    "#                             X_imp = imp.fit_transform(X)\n",
    "#                             X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "#                             kmeans = KMeans(\n",
    "#                                 n_clusters=params['n_clusters'],\n",
    "#                                 init=params['init'],\n",
    "#                                 n_init=params['n_init'],\n",
    "#                                 max_iter=params['max_iter'],\n",
    "#                                 tol=params['tol'],\n",
    "#                                 algorithm=params['algorithm'],\n",
    "#                                 random_state=42\n",
    "#                             )\n",
    "#                             clusters = kmeans.fit_predict(X_scaled)\n",
    "#                             df_tmp = df.copy()\n",
    "#                             df_tmp['Cluster'] = clusters\n",
    "#\n",
    "#                             # Genera il grafico\n",
    "#                             plot_cluster_demographics(df_tmp)"
   ],
   "id": "40c78acf4570c560"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#ANALISI NEL DETTAGLIO DEL SINGOLO CLUSTER MIGLIORE OTTENUTO DA KMEANS, OVVERO QUELLO INDIVIDUATO NEL CODICE PRECEDENTE CON ARI = 0.0012036242459496649\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# 1. Caricamento dei dati\n",
    "df = pd.read_csv(\"Excel/Clustering.csv\")\n",
    "X = df.drop(columns=[\"Has_Diagnostics\"])\n",
    "y = df[\"Has_Diagnostics\"]\n",
    "\n",
    "# 2. Imputazione dei valori mancanti (media)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 3. Standardizzazione\n",
    "X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# 4. KMeans clustering\n",
    "kmeans = KMeans(\n",
    "    n_clusters=2,\n",
    "    init='random',\n",
    "    n_init=10,\n",
    "    max_iter=100,\n",
    "    tol=0.001,\n",
    "    algorithm='lloyd',\n",
    "    random_state=42\n",
    ")\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 5. Aggiunta cluster al DataFrame\n",
    "df_tmp = df.copy()\n",
    "df_tmp['Cluster'] = clusters\n",
    "\n",
    "# 6. Analisi delle feature per cluster (media, std, ANOVA) Analysis of Variance per vedere se la media delle features e' significativamente diversa tra i due cluster\n",
    "feature_names = X.columns\n",
    "summary = []\n",
    "\n",
    "\n",
    "#Per ogni feature del dataset viene calcolata la media e la deviazione standard nei due cluster e il valore p del test ANOVA\n",
    "#Se p > 0.05 allora la differenza tra le media nei due cluster non e' significativa, altrimenti  e' significativa\n",
    "for i, feature in enumerate(feature_names):\n",
    "    cluster_0 = X_scaled[clusters == 0, i]\n",
    "    cluster_1 = X_scaled[clusters == 1, i]\n",
    "    stat, p = f_oneway(cluster_0, cluster_1)\n",
    "    summary.append({\n",
    "        'Feature': feature,\n",
    "        'Cluster0_Mean': np.mean(cluster_0),\n",
    "        'Cluster1_Mean': np.mean(cluster_1),\n",
    "        'Cluster0_Std': np.std(cluster_0),\n",
    "        'Cluster1_Std': np.std(cluster_1),\n",
    "        'ANOVA_p_value': p\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary = df_summary.sort_values(\"ANOVA_p_value\").reset_index(drop=True)\n",
    "df_summary.index = df_summary.index + 1  # Indici da 1 a N\n",
    "\n",
    "print(\"\\n>>> Analisi ANOVA tra cluster (feature ordinati per significatività):\")\n",
    "print(df_summary.to_string())\n",
    "\n",
    "\n",
    "# 7. Feature Importance con Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaled, clusters)\n",
    "importances = rf.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 8. Plot delle feature più importanti\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances_df, color='skyblue')\n",
    "\n",
    "plt.title('Importanza delle feature secondo Random Forest')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Funzione per il grafico demografico\n",
    "def plot_cluster_demographics(df_tmp):\n",
    "    bins = [0, 29, 49, 69, np.inf]\n",
    "    labels = ['<30', '30-49', '50-69', '>69']\n",
    "    df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    cluster_data = []\n",
    "    for cluster in [0, 1]:\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "        age_counts = []\n",
    "        for age_group in labels:\n",
    "            age_df = cluster_df[cluster_df['AgeGroup'] == age_group]\n",
    "            male_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            male_no_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            female_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            female_no_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            age_counts.append({\n",
    "                'Male_Dx': male_dx,\n",
    "                'Male_NoDx': male_no_dx,\n",
    "                'Female_Dx': female_dx,\n",
    "                'Female_NoDx': female_no_dx,\n",
    "                'Total': male_dx + male_no_dx + female_dx + female_no_dx\n",
    "            })\n",
    "        cluster_data.append(age_counts)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    colors = {\n",
    "        'Male_Dx': '#1f77b4',\n",
    "        'Male_NoDx': '#a5c8e4',\n",
    "        'Female_Dx': '#e377c2',\n",
    "        'Female_NoDx': '#f8b8d8'\n",
    "    }\n",
    "\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(labels))\n",
    "    for i, cluster in enumerate([0, 1]):\n",
    "        positions = x + i * bar_width\n",
    "        bottom = np.zeros(len(labels))\n",
    "        for segment in ['Male_Dx', 'Male_NoDx', 'Female_Dx', 'Female_NoDx']:\n",
    "            counts = [cluster_data[cluster][j][segment] for j in range(len(labels))]\n",
    "            ax.bar(positions, counts, bar_width, bottom=bottom, color=colors[segment], edgecolor='black')\n",
    "            for j, count in enumerate(counts):\n",
    "                if count > 0:\n",
    "                    ax.text(positions[j], bottom[j] + count/2, str(count), ha='center', va='center', fontsize=8)\n",
    "            bottom += counts\n",
    "        for j in range(len(labels)):\n",
    "            total = cluster_data[cluster][j]['Total']\n",
    "            ax.text(positions[j], bottom[j] + 5, f'Tot: {total}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x + bar_width / 2)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlabel('Fascia d\\'età')\n",
    "    ax.set_ylabel('Numero di pazienti')\n",
    "    ax.set_title('Distribuzione dei pazienti per cluster, fascia d\\'età, sesso e presenza di complicanze')\n",
    "\n",
    "    handles = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_Dx'], label='Maschi con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_NoDx'], label='Maschi senza complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_Dx'], label='Femmine con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_NoDx'], label='Femmine senza complicanze'),\n",
    "    ]\n",
    "    ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. Mostra il grafico demografico\n",
    "plot_cluster_demographics(df_tmp)\n",
    "\n",
    "\n",
    "# 11. Istogrammi separati per %TIR_last3m e %TARLV2_last3m\n",
    "\n",
    "\n",
    "# 2 ISTOGRAMMI\n",
    "# Colonne target\n",
    "features = ['%TIR_last3m', '%TARLV2_last3m']\n",
    "\n",
    "# Definizione fasce\n",
    "bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "for feature in features:\n",
    "    # Crea la colonna con le fasce\n",
    "    df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "    # Conta i pazienti per cluster e fascia\n",
    "    counts = df_tmp.groupby(['Cluster', f'{feature}_bin']).size().reset_index(name='Count')\n",
    "    counts.rename(columns={f'{feature}_bin': 'Fascia'}, inplace=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=counts, x='Fascia', y='Count', hue='Cluster', palette='Set2', dodge=True)\n",
    "\n",
    "    plt.title('')\n",
    "    plt.xlabel('Fasce di percentuale')\n",
    "    plt.ylabel('Numero di pazienti')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 4 ISTOGRAMMI\n",
    "# Colonne target\n",
    "features = ['%TIR_last3m', '%TARLV2_last3m']\n",
    "\n",
    "# Definizione fasce\n",
    "bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "# Crea le colonne binned per ogni feature\n",
    "for feature in features:\n",
    "    df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Per ogni combinazione di feature e cluster, plottiamo un istogramma\n",
    "for feature in features:\n",
    "    for cluster in [0, 1]:\n",
    "        # Filtra solo i dati del cluster corrente\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "\n",
    "        # Conta i pazienti nelle fasce\n",
    "        counts = cluster_df[f'{feature}_bin'].value_counts().sort_index()\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x=counts.index, y=counts.values, color='cornflowerblue', edgecolor='black')\n",
    "\n",
    "        plt.title(f'Distribuzione {feature} - Cluster {cluster}')\n",
    "        plt.xlabel('Fasce di percentuale')\n",
    "        plt.ylabel('Numero di pazienti')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "a1c_col = 'Glycated hemoglobin (A1c)_1'\n",
    "\n",
    "# Definizione dei bin per l'emoglobina glicata\n",
    "bins_a1c = [0, 5, 6, 7, 8, 9, 10, 11, 12, np.inf]\n",
    "labels_a1c = ['≤5', '5-6', '6-7', '7-8', '8-9', '9-10', '10-11', '11-12', '>12']\n",
    "\n",
    "# Crea la colonna binned\n",
    "df_tmp['A1c_bin'] = pd.cut(df_tmp[a1c_col], bins=bins_a1c, labels=labels_a1c, include_lowest=True, right=False)\n",
    "\n",
    "# Conta i pazienti per cluster e fascia\n",
    "a1c_counts = df_tmp.groupby(['Cluster', 'A1c_bin']).size().reset_index(name='Count')\n",
    "\n",
    "# Plot istogramma a due colonne\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=a1c_counts, x='A1c_bin', y='Count', hue='Cluster', palette='Set2', dodge=True)\n",
    "\n",
    "plt.title('Distribuzione dell\\'emoglobina glicata nei due cluster')\n",
    "plt.xlabel('Fasce di HbA1c (%)')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.legend(title='Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "\n",
    "\n",
    "# Calcolo delle metriche di valutazione del clustering\n",
    "sil_score = silhouette_score(X_scaled, clusters)\n",
    "dbi = davies_bouldin_score(X_scaled, clusters)\n",
    "chi = calinski_harabasz_score(X_scaled, clusters)\n",
    "ari = adjusted_rand_score(y, clusters)  # già calcolato, ma lo ricalcoliamo qui per coerenza\n",
    "\n",
    "print(\"\\n>>> Metriche di valutazione del clustering KMeans:\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {chi:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")"
   ],
   "id": "5220c797c079eaee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#KMEANS CON K=3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# 1. Caricamento dei dati\n",
    "df = pd.read_csv(\"Excel/Clustering.csv\")\n",
    "X = df.drop(columns=[\"Has_Diagnostics\"])\n",
    "y = df[\"Has_Diagnostics\"]\n",
    "\n",
    "# 2. Imputazione dei valori mancanti (media)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 3. Standardizzazione\n",
    "X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# 4. KMeans clustering\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='random',\n",
    "    n_init=10,\n",
    "    max_iter=100,\n",
    "    tol=0.001,\n",
    "    algorithm='lloyd',\n",
    "    random_state=42\n",
    ")\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 5. Aggiunta cluster al DataFrame\n",
    "df_tmp = df.copy()\n",
    "df_tmp['Cluster'] = clusters\n",
    "\n",
    "# 6. Analisi delle feature per cluster (media, std, ANOVA) Analysis of Variance per vedere se la media delle features e' significativamente diversa tra i due cluster\n",
    "feature_names = X.columns\n",
    "summary = []\n",
    "\n",
    "\n",
    "#Per ogni feature del dataset viene calcolata la media e la deviazione standard nei due cluster e il valore p del test ANOVA\n",
    "#Se p > 0.05 allora la differenza tra le media nei cluster non e' significativa, altrimenti  e' significativa\n",
    "for i, feature in enumerate(feature_names):\n",
    "    cluster_0 = X_scaled[clusters == 0, i]\n",
    "    cluster_1 = X_scaled[clusters == 1, i]\n",
    "    cluster_2 = X_scaled[clusters == 2, i]\n",
    "    stat, p = f_oneway(cluster_0, cluster_1, cluster_2)\n",
    "    summary.append({\n",
    "        'Feature': feature,\n",
    "        'Cluster0_Mean': np.mean(cluster_0),\n",
    "        'Cluster1_Mean': np.mean(cluster_1),\n",
    "        'Cluster2_Mean': np.mean(cluster_2),\n",
    "        'Cluster0_Std': np.std(cluster_0),\n",
    "        'Cluster1_Std': np.std(cluster_1),\n",
    "        'Cluster2_Std': np.std(cluster_2),\n",
    "        'ANOVA_p_value': p\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary = df_summary.sort_values(\"ANOVA_p_value\").reset_index(drop=True)\n",
    "df_summary.index = df_summary.index + 1  # Indici da 1 a N\n",
    "\n",
    "print(\"\\n>>> Analisi ANOVA tra cluster (feature ordinati per significatività):\")\n",
    "print(df_summary.to_string())\n",
    "\n",
    "\n",
    "# 7. Feature Importance con Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaled, clusters)\n",
    "importances = rf.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 8. Plot delle feature più importanti\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances_df, color='skyblue')\n",
    "\n",
    "plt.title('Importanza delle feature secondo Random Forest')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Matrice di confusione\n",
    "cm = pd.crosstab(y, clusters)\n",
    "print(\"\\n>>> Matrice di confusione:\")\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Calcolo delle metriche di valutazione del clustering\n",
    "sil_score = silhouette_score(X_scaled, clusters)\n",
    "dbi = davies_bouldin_score(X_scaled, clusters)\n",
    "chi = calinski_harabasz_score(X_scaled, clusters)\n",
    "ari = adjusted_rand_score(y, clusters)\n",
    "\n",
    "print(\"\\n>>> Metriche di valutazione del clustering GMM:\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {chi:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "\n",
    "\n",
    "def plot_cluster_demographics_3clusters(df_tmp):\n",
    "    bins = [0, 29, 49, 69, np.inf]\n",
    "    labels = ['<30', '30-49', '50-69', '>69']\n",
    "    df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    cluster_data = []\n",
    "    for cluster in range(3):  # 3 cluster\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "        age_counts = []\n",
    "        for age_group in labels:\n",
    "            age_df = cluster_df[cluster_df['AgeGroup'] == age_group]\n",
    "            male_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            male_no_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            female_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            female_no_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            age_counts.append({\n",
    "                'Male_Dx': male_dx,\n",
    "                'Male_NoDx': male_no_dx,\n",
    "                'Female_Dx': female_dx,\n",
    "                'Female_NoDx': female_no_dx,\n",
    "                'Total': male_dx + male_no_dx + female_dx + female_no_dx\n",
    "            })\n",
    "        cluster_data.append(age_counts)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    colors = {\n",
    "        'Male_Dx': '#1f77b4',       # blu scuro\n",
    "        'Male_NoDx': '#a5c8e4',     # blu chiaro\n",
    "        'Female_Dx': '#e377c2',     # rosa scuro\n",
    "        'Female_NoDx': '#f8b8d8'    # rosa chiaro\n",
    "    }\n",
    "\n",
    "    bar_width = 0.18  # più stretto per 4 cluster\n",
    "    x = np.arange(len(labels))  # posizioni delle fasce d'età\n",
    "\n",
    "    for i, cluster in enumerate(range(3)):\n",
    "        positions = x + i * bar_width\n",
    "        bottom = np.zeros(len(labels))\n",
    "        for segment in ['Male_Dx', 'Male_NoDx', 'Female_Dx', 'Female_NoDx']:\n",
    "            counts = [cluster_data[cluster][j][segment] for j in range(len(labels))]\n",
    "            ax.bar(positions, counts, bar_width, bottom=bottom, color=colors[segment], edgecolor='black')\n",
    "            # Testo con numero conteggiato sulle barre (solo se >0)\n",
    "            for j, count in enumerate(counts):\n",
    "                if count > 0:\n",
    "                    ax.text(positions[j], bottom[j] + count/2, str(count), ha='center', va='center', fontsize=7)\n",
    "            bottom += counts\n",
    "        # Totali sopra ogni barra cluster/fascia\n",
    "        for j in range(len(labels)):\n",
    "            total = cluster_data[cluster][j]['Total']\n",
    "            ax.text(positions[j], bottom[j] + 2, f'Tot: {total}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x + 1.5 * bar_width)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlabel('Fascia d\\'età')\n",
    "    ax.set_ylabel('Numero di pazienti')\n",
    "    ax.set_title('Distribuzione pazienti per cluster, fascia d\\'età, sesso e complicanze')\n",
    "\n",
    "    # Legenda\n",
    "    handles = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_Dx'], label='Maschi con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_NoDx'], label='Maschi senza complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_Dx'], label='Femmine con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_NoDx'], label='Femmine senza complicanze'),\n",
    "    ]\n",
    "    ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Esegui la funzione con il dataframe aggiornato\n",
    "plot_cluster_demographics_3clusters(df_tmp)\n",
    "\n",
    "\n",
    "\n",
    "# Feature da analizzare\n",
    "feature = '%TBRLV2_last3m'\n",
    "\n",
    "# Definizione fasce per %TBRLV2_last3m\n",
    "bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "# Crea la colonna con le fasce\n",
    "df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Conta i pazienti per cluster e fascia\n",
    "counts = df_tmp.groupby(['Cluster', f'{feature}_bin']).size().reset_index(name='Count')\n",
    "counts.rename(columns={f'{feature}_bin': 'Fascia'}, inplace=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=counts, x='Fascia', y='Count', hue='Cluster', palette='Set2', dodge=True)\n",
    "\n",
    "plt.title('Distribuzione pazienti per cluster e fasce di %TBRLV2_last3m')\n",
    "plt.xlabel('Fasce di %TBRLV2_last3m')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.legend(title='Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "9ab39f5260216ef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#KMEANS CON K=4\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# 1. Caricamento dei dati\n",
    "df = pd.read_csv(\"Excel/Clustering.csv\")\n",
    "X = df.drop(columns=[\"Has_Diagnostics\"])\n",
    "y = df[\"Has_Diagnostics\"]\n",
    "\n",
    "# 2. Imputazione dei valori mancanti (media)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 3. Standardizzazione\n",
    "X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# 4. KMeans clustering\n",
    "kmeans = KMeans(\n",
    "    n_clusters=4,\n",
    "    init='random',\n",
    "    n_init=10,\n",
    "    max_iter=100,\n",
    "    tol=0.001,\n",
    "    algorithm='lloyd',\n",
    "    random_state=42\n",
    ")\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 5. Aggiunta cluster al DataFrame\n",
    "df_tmp = df.copy()\n",
    "df_tmp['Cluster'] = clusters\n",
    "\n",
    "# 6. Analisi delle feature per cluster (media, std, ANOVA) Analysis of Variance per vedere se la media delle features e' significativamente diversa tra i due cluster\n",
    "feature_names = X.columns\n",
    "summary = []\n",
    "\n",
    "\n",
    "#Per ogni feature del dataset viene calcolata la media e la deviazione standard nei due cluster e il valore p del test ANOVA\n",
    "#Se p > 0.05 allora la differenza tra le media nei cluster non e' significativa, altrimenti  e' significativa\n",
    "for i, feature in enumerate(feature_names):\n",
    "    cluster_0 = X_scaled[clusters == 0, i]\n",
    "    cluster_1 = X_scaled[clusters == 1, i]\n",
    "    cluster_2 = X_scaled[clusters == 2, i]\n",
    "    cluster_3 = X_scaled[clusters == 3, i]\n",
    "    stat, p = f_oneway(cluster_0, cluster_1, cluster_2, cluster_3)\n",
    "    summary.append({\n",
    "        'Feature': feature,\n",
    "        'Cluster0_Mean': np.mean(cluster_0),\n",
    "        'Cluster1_Mean': np.mean(cluster_1),\n",
    "        'Cluster2_Mean': np.mean(cluster_2),\n",
    "        'Cluster3_Mean': np.mean(cluster_3),\n",
    "        'Cluster0_Std': np.std(cluster_0),\n",
    "        'Cluster1_Std': np.std(cluster_1),\n",
    "        'Cluster2_Std': np.std(cluster_2),\n",
    "        'Cluster3_Std': np.std(cluster_3),\n",
    "        'ANOVA_p_value': p\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary = df_summary.sort_values(\"ANOVA_p_value\").reset_index(drop=True)\n",
    "df_summary.index = df_summary.index + 1  # Indici da 1 a N\n",
    "\n",
    "print(\"\\n>>> Analisi ANOVA tra cluster (feature ordinati per significatività):\")\n",
    "print(df_summary.to_string())\n",
    "\n",
    "\n",
    "# 7. Feature Importance con Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaled, clusters)\n",
    "importances = rf.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 8. Plot delle feature più importanti\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances_df, color='skyblue')\n",
    "\n",
    "plt.title('Importanza delle feature secondo Random Forest')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Matrice di confusione\n",
    "cm = pd.crosstab(y, clusters)\n",
    "print(\"\\n>>> Matrice di confusione:\")\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Calcolo delle metriche di valutazione del clustering\n",
    "sil_score = silhouette_score(X_scaled, clusters)\n",
    "dbi = davies_bouldin_score(X_scaled, clusters)\n",
    "chi = calinski_harabasz_score(X_scaled, clusters)\n",
    "ari = adjusted_rand_score(y, clusters)\n",
    "\n",
    "print(\"\\n>>> Metriche di valutazione del clustering GMM:\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {chi:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "\n",
    "\n",
    "def plot_cluster_demographics_4clusters(df_tmp):\n",
    "    bins = [0, 29, 49, 69, np.inf]\n",
    "    labels = ['<30', '30-49', '50-69', '>69']\n",
    "    df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    cluster_data = []\n",
    "    for cluster in range(4):  # 4 cluster\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "        age_counts = []\n",
    "        for age_group in labels:\n",
    "            age_df = cluster_df[cluster_df['AgeGroup'] == age_group]\n",
    "            male_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            male_no_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            female_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            female_no_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            age_counts.append({\n",
    "                'Male_Dx': male_dx,\n",
    "                'Male_NoDx': male_no_dx,\n",
    "                'Female_Dx': female_dx,\n",
    "                'Female_NoDx': female_no_dx,\n",
    "                'Total': male_dx + male_no_dx + female_dx + female_no_dx\n",
    "            })\n",
    "        cluster_data.append(age_counts)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    colors = {\n",
    "        'Male_Dx': '#1f77b4',       # blu scuro\n",
    "        'Male_NoDx': '#a5c8e4',     # blu chiaro\n",
    "        'Female_Dx': '#e377c2',     # rosa scuro\n",
    "        'Female_NoDx': '#f8b8d8'    # rosa chiaro\n",
    "    }\n",
    "\n",
    "    bar_width = 0.18  # più stretto per 4 cluster\n",
    "    x = np.arange(len(labels))  # posizioni delle fasce d'età\n",
    "\n",
    "    for i, cluster in enumerate(range(4)):\n",
    "        positions = x + i * bar_width\n",
    "        bottom = np.zeros(len(labels))\n",
    "        for segment in ['Male_Dx', 'Male_NoDx', 'Female_Dx', 'Female_NoDx']:\n",
    "            counts = [cluster_data[cluster][j][segment] for j in range(len(labels))]\n",
    "            ax.bar(positions, counts, bar_width, bottom=bottom, color=colors[segment], edgecolor='black')\n",
    "            # Testo con numero conteggiato sulle barre (solo se >0)\n",
    "            for j, count in enumerate(counts):\n",
    "                if count > 0:\n",
    "                    ax.text(positions[j], bottom[j] + count/2, str(count), ha='center', va='center', fontsize=7)\n",
    "            bottom += counts\n",
    "        # Totali sopra ogni barra cluster/fascia\n",
    "        for j in range(len(labels)):\n",
    "            total = cluster_data[cluster][j]['Total']\n",
    "            ax.text(positions[j], bottom[j] + 2, f'Tot: {total}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x + 1.5 * bar_width)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlabel('Fascia d\\'età')\n",
    "    ax.set_ylabel('Numero di pazienti')\n",
    "    ax.set_title('Distribuzione pazienti per cluster, fascia d\\'età, sesso e complicanze')\n",
    "\n",
    "    # Legenda\n",
    "    handles = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_Dx'], label='Maschi con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_NoDx'], label='Maschi senza complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_Dx'], label='Femmine con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_NoDx'], label='Femmine senza complicanze'),\n",
    "    ]\n",
    "    ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Esegui la funzione con il dataframe aggiornato\n",
    "plot_cluster_demographics_4clusters(df_tmp)\n",
    "\n",
    "\n",
    "\n",
    "# Feature da analizzare\n",
    "feature = '%TBRLV2_last3m'\n",
    "\n",
    "# Definizione fasce per %TBRLV2_last3m\n",
    "bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "# Crea la colonna con le fasce\n",
    "df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Conta i pazienti per cluster e fascia\n",
    "counts = df_tmp.groupby(['Cluster', f'{feature}_bin']).size().reset_index(name='Count')\n",
    "counts.rename(columns={f'{feature}_bin': 'Fascia'}, inplace=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=counts, x='Fascia', y='Count', hue='Cluster', palette='Set2', dodge=True)\n",
    "\n",
    "plt.title('Distribuzione pazienti per cluster e fasce di %TBRLV2_last3m')\n",
    "plt.xlabel('Fasce di %TBRLV2_last3m')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.legend(title='Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "5e2646f95724ffd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CLUSTERING USANDO KMEANS CON K=2, PERO USANDO COME INFORMAZIONI DA DARE ALL'ALGORITMO SOLO LE INFORMAZIONI RELATIVE AL GLUCOSIO:\n",
    "    # '%TIR_last3m',\n",
    "    # '%TARLV1_last3m',\n",
    "    # '%TARLV2_last3m',\n",
    "    # '%TBRLV1_last3m',\n",
    "    # '%TBRLV2_last3m',\n",
    "    # 'Glycated hemoglobin (A1c)_1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# 1. Caricamento dei dati\n",
    "df = pd.read_csv(\"Excel/Clustering.csv\")\n",
    "y = df[\"Has_Diagnostics\"]\n",
    "\n",
    "# Selezione feature per il clustering\n",
    "selected_features = [\n",
    "    '%TIR_last3m',\n",
    "    '%TARLV1_last3m',\n",
    "    '%TARLV2_last3m',\n",
    "    '%TBRLV1_last3m',\n",
    "    '%TBRLV2_last3m',\n",
    "    'Glycated hemoglobin (A1c)_1'\n",
    "]\n",
    "X = df[selected_features]\n",
    "\n",
    "\n",
    "# 2. Imputazione dei valori mancanti (media)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 3. Standardizzazione\n",
    "X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# 4. KMeans clustering\n",
    "kmeans = KMeans(\n",
    "    n_clusters=2,\n",
    "    init='random',\n",
    "    n_init=10,\n",
    "    max_iter=100,\n",
    "    tol=0.001,\n",
    "    algorithm='lloyd',\n",
    "    random_state=42\n",
    ")\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 5. Aggiunta cluster al DataFrame\n",
    "df_tmp = df.copy()\n",
    "df_tmp['Cluster'] = clusters\n",
    "\n",
    "# 6. Analisi delle feature per cluster (media, std, ANOVA) Analysis of Variance per vedere se la media delle features e' significativamente diversa tra i due cluster\n",
    "feature_names = X.columns\n",
    "summary = []\n",
    "\n",
    "\n",
    "#Per ogni feature del dataset viene calcolata la media e la deviazione standard nei due cluster e il valore p del test ANOVA\n",
    "#Se p > 0.05 allora la differenza tra le media nei due cluster non e' significativa, altrimenti  e' significativa\n",
    "for i, feature in enumerate(feature_names):\n",
    "    cluster_0 = X_scaled[clusters == 0, i]\n",
    "    cluster_1 = X_scaled[clusters == 1, i]\n",
    "    stat, p = f_oneway(cluster_0, cluster_1)\n",
    "    summary.append({\n",
    "        'Feature': feature,\n",
    "        'Cluster0_Mean': np.mean(cluster_0),\n",
    "        'Cluster1_Mean': np.mean(cluster_1),\n",
    "        'Cluster0_Std': np.std(cluster_0),\n",
    "        'Cluster1_Std': np.std(cluster_1),\n",
    "        'ANOVA_p_value': p\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary = df_summary.sort_values(\"ANOVA_p_value\").reset_index(drop=True)\n",
    "df_summary.index = df_summary.index + 1  # Indici da 1 a N\n",
    "\n",
    "print(\"\\n>>> Analisi ANOVA tra cluster (feature ordinati per significatività):\")\n",
    "print(df_summary.to_string())\n",
    "\n",
    "\n",
    "# 7. Feature Importance con Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaled, clusters)\n",
    "importances = rf.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 8. Plot delle feature più importanti\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances_df, color='skyblue')\n",
    "\n",
    "plt.title('Importanza delle feature secondo Random Forest')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Funzione per il grafico demografico\n",
    "def plot_cluster_demographics(df_tmp):\n",
    "    bins = [0, 29, 49, 69, np.inf]\n",
    "    labels = ['<30', '30-49', '50-69', '>69']\n",
    "    df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    cluster_data = []\n",
    "    for cluster in [0, 1]:\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "        age_counts = []\n",
    "        for age_group in labels:\n",
    "            age_df = cluster_df[cluster_df['AgeGroup'] == age_group]\n",
    "            male_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            male_no_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            female_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            female_no_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            age_counts.append({\n",
    "                'Male_Dx': male_dx,\n",
    "                'Male_NoDx': male_no_dx,\n",
    "                'Female_Dx': female_dx,\n",
    "                'Female_NoDx': female_no_dx,\n",
    "                'Total': male_dx + male_no_dx + female_dx + female_no_dx\n",
    "            })\n",
    "        cluster_data.append(age_counts)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    colors = {\n",
    "        'Male_Dx': '#1f77b4',\n",
    "        'Male_NoDx': '#a5c8e4',\n",
    "        'Female_Dx': '#e377c2',\n",
    "        'Female_NoDx': '#f8b8d8'\n",
    "    }\n",
    "\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(labels))\n",
    "    for i, cluster in enumerate([0, 1]):\n",
    "        positions = x + i * bar_width\n",
    "        bottom = np.zeros(len(labels))\n",
    "        for segment in ['Male_Dx', 'Male_NoDx', 'Female_Dx', 'Female_NoDx']:\n",
    "            counts = [cluster_data[cluster][j][segment] for j in range(len(labels))]\n",
    "            ax.bar(positions, counts, bar_width, bottom=bottom, color=colors[segment], edgecolor='black')\n",
    "            for j, count in enumerate(counts):\n",
    "                if count > 0:\n",
    "                    ax.text(positions[j], bottom[j] + count/2, str(count), ha='center', va='center', fontsize=8)\n",
    "            bottom += counts\n",
    "        for j in range(len(labels)):\n",
    "            total = cluster_data[cluster][j]['Total']\n",
    "            ax.text(positions[j], bottom[j] + 5, f'Tot: {total}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x + bar_width / 2)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlabel('Fascia d\\'età')\n",
    "    ax.set_ylabel('Numero di pazienti')\n",
    "    ax.set_title('Distribuzione dei pazienti per cluster, fascia d\\'età, sesso e presenza di complicanze')\n",
    "\n",
    "    handles = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_Dx'], label='Maschi con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_NoDx'], label='Maschi senza complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_Dx'], label='Femmine con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_NoDx'], label='Femmine senza complicanze'),\n",
    "    ]\n",
    "    ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. Mostra il grafico demografico\n",
    "plot_cluster_demographics(df_tmp)\n",
    "\n",
    "\n",
    "# 11. Istogrammi separati per %TIR_last3m e %TARLV2_last3m\n",
    "\n",
    "\n",
    "# 2 ISTOGRAMMI\n",
    "# Colonne target\n",
    "features = ['%TIR_last3m', '%TARLV2_last3m']\n",
    "\n",
    "# Definizione fasce\n",
    "bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "for feature in features:\n",
    "    # Crea la colonna con le fasce\n",
    "    df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "    # Conta i pazienti per cluster e fascia\n",
    "    counts = df_tmp.groupby(['Cluster', f'{feature}_bin']).size().reset_index(name='Count')\n",
    "    counts.rename(columns={f'{feature}_bin': 'Fascia'}, inplace=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=counts, x='Fascia', y='Count', hue='Cluster', palette='Set2', dodge=True)\n",
    "\n",
    "    plt.title('')\n",
    "    plt.xlabel('Fasce di percentuale')\n",
    "    plt.ylabel('Numero di pazienti')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 4 ISTOGRAMMI\n",
    "# Colonne target\n",
    "features = ['%TIR_last3m', '%TARLV2_last3m']\n",
    "\n",
    "# Definizione fasce\n",
    "bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "# Crea le colonne binned per ogni feature\n",
    "for feature in features:\n",
    "    df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Per ogni combinazione di feature e cluster, plottiamo un istogramma\n",
    "for feature in features:\n",
    "    for cluster in [0, 1]:\n",
    "        # Filtra solo i dati del cluster corrente\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "\n",
    "        # Conta i pazienti nelle fasce\n",
    "        counts = cluster_df[f'{feature}_bin'].value_counts().sort_index()\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x=counts.index, y=counts.values, color='cornflowerblue', edgecolor='black')\n",
    "\n",
    "        plt.title(f'Distribuzione {feature} - Cluster {cluster}')\n",
    "        plt.xlabel('Fasce di percentuale')\n",
    "        plt.ylabel('Numero di pazienti')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "a1c_col = 'Glycated hemoglobin (A1c)_1'\n",
    "\n",
    "# Definizione dei bin per l'emoglobina glicata\n",
    "bins_a1c = [0, 5, 6, 7, 8, 9, 10, 11, 12, np.inf]\n",
    "labels_a1c = ['≤5', '5-6', '6-7', '7-8', '8-9', '9-10', '10-11', '11-12', '>12']\n",
    "\n",
    "# Crea la colonna binned\n",
    "df_tmp['A1c_bin'] = pd.cut(df_tmp[a1c_col], bins=bins_a1c, labels=labels_a1c, include_lowest=True, right=False)\n",
    "\n",
    "# Conta i pazienti per cluster e fascia\n",
    "a1c_counts = df_tmp.groupby(['Cluster', 'A1c_bin']).size().reset_index(name='Count')\n",
    "\n",
    "# Plot istogramma a due colonne\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=a1c_counts, x='A1c_bin', y='Count', hue='Cluster', palette='Set2', dodge=True)\n",
    "\n",
    "plt.title('Distribuzione dell\\'emoglobina glicata nei due cluster')\n",
    "plt.xlabel('Fasce di HbA1c (%)')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.legend(title='Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, confusion_matrix\n",
    "\n",
    "# Calcolo delle metriche di valutazione del clustering\n",
    "sil_score = silhouette_score(X_scaled, clusters)\n",
    "dbi = davies_bouldin_score(X_scaled, clusters)\n",
    "chi = calinski_harabasz_score(X_scaled, clusters)\n",
    "ari = adjusted_rand_score(y, clusters)  # già calcolato, ma lo ricalcoliamo qui per coerenza\n",
    "\n",
    "print(\"\\n>>> Metriche di valutazione del clustering KMeans:\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {chi:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y, clusters)\n",
    "print(\"\\n>>> Matrice di confusione (etichetta: Has_Diagnostics vs Cluster_GMM):\")\n",
    "print(cm)"
   ],
   "id": "e9d459fcbe322322"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CLUSTERING CON GAUSSIAN MIXTURE MODELS, USANDO COME METODO DI IMPUTAZIONE LA MEDIA, CHE NEL KMEANS E' RISULTATO IL METODO MIGLIORE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# 1. Caricamento dei dati\n",
    "df = pd.read_csv(\"Excel/Clustering.csv\")\n",
    "X = df.drop(columns=[\"Has_Diagnostics\"])\n",
    "y = df[\"Has_Diagnostics\"]\n",
    "\n",
    "# 2. Imputazione dei valori mancanti (media)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 3. Standardizzazione\n",
    "X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# 4. GMM clustering\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm.fit(X_scaled)\n",
    "clusters = gmm.predict(X_scaled)\n",
    "probs = gmm.predict_proba(X_scaled)  # probabilità di appartenenza\n",
    "\n",
    "# 5. Aggiunta cluster e probabilità al DataFrame\n",
    "df_tmp = df.copy()\n",
    "df_tmp['Cluster_GMM'] = clusters\n",
    "df_tmp['GMM_Prob0'] = probs[:, 0]\n",
    "df_tmp['GMM_Prob1'] = probs[:, 1]\n",
    "\n",
    "# 6. Analisi ANOVA tra i due cluster\n",
    "feature_names = X.columns\n",
    "summary = []\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    cluster_0 = X_scaled[clusters == 0, i]\n",
    "    cluster_1 = X_scaled[clusters == 1, i]\n",
    "    stat, p = f_oneway(cluster_0, cluster_1)\n",
    "    summary.append({\n",
    "        'Feature': feature,\n",
    "        'Cluster0_Mean': np.mean(cluster_0),\n",
    "        'Cluster1_Mean': np.mean(cluster_1),\n",
    "        'Cluster0_Std': np.std(cluster_0),\n",
    "        'Cluster1_Std': np.std(cluster_1),\n",
    "        'ANOVA_p_value': p\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary).sort_values(\"ANOVA_p_value\").reset_index(drop=True)\n",
    "df_summary.index += 1\n",
    "\n",
    "print(\"\\n>>> Analisi ANOVA tra cluster GMM:\")\n",
    "print(df_summary.to_string())\n",
    "\n",
    "# 7. Feature Importance con Random Forest (etichetta = cluster GMM)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaled, clusters)\n",
    "importances = rf.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 8. Plot delle feature più importanti\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances_df, color='skyblue')\n",
    "plt.title('Importanza delle feature secondo Random Forest (Cluster GMM)')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Istogramma delle probabilità di appartenenza\n",
    "#PRIMA DIVISI\n",
    "# Ottieni i colori da Set2\n",
    "colors = sns.color_palette(\"Set2\", 2)\n",
    "\n",
    "# Istogrammi separati con colori coerenti\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "sns.histplot(df_tmp['GMM_Prob0'], bins=20, kde=True, color=colors[0], ax=axs[0])\n",
    "axs[0].set_title('Probabilità di appartenenza al Cluster 0')\n",
    "axs[0].set_xlabel('GMM_Prob0')\n",
    "axs[0].set_ylabel('Numero di pazienti')\n",
    "\n",
    "sns.histplot(df_tmp['GMM_Prob1'], bins=20, kde=True, color=colors[1], ax=axs[1])\n",
    "axs[1].set_title('Probabilità di appartenenza al Cluster 1')\n",
    "axs[1].set_xlabel('GMM_Prob1')\n",
    "axs[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#POI SOVRAPPOSTI\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_tmp[['GMM_Prob0', 'GMM_Prob1']], bins=20, kde=True, palette='Set2')\n",
    "plt.title('Distribuzione delle probabilità di appartenenza ai cluster GMM')\n",
    "plt.xlabel('Probabilità')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 10. Pazienti \"borderline\" (probabilità compresa tra 0.4 e 0.6)\n",
    "df_tmp['Borderline'] = ((df_tmp['GMM_Prob0'] >= 0.4) & (df_tmp['GMM_Prob0'] <= 0.6)) | \\\n",
    "                       ((df_tmp['GMM_Prob1'] >= 0.4) & (df_tmp['GMM_Prob1'] <= 0.6))\n",
    "n_borderline = df_tmp['Borderline'].sum()\n",
    "print(f\"\\n>>> Pazienti borderline (probabilità ∈ [0.4, 0.6]): {n_borderline} su {len(df_tmp)}\")\n",
    "\n",
    "# (facoltativo) Mostra le prime righe dei pazienti borderline\n",
    "print(\"\\n>>> Esempi di pazienti borderline:\")\n",
    "# Mostrare tutte le colonne senza spezzare a capo\n",
    "pd.set_option('display.max_columns', None)  # mostra tutte le colonne\n",
    "pd.set_option('display.width', 2000)        # larghezza massima per la stampa (molto larga)\n",
    "pd.set_option('display.max_colwidth', None) # non troncare le colonne\n",
    "\n",
    "# Poi stampa\n",
    "print(df_tmp[df_tmp['Borderline']].head().to_string(index=False))\n",
    "df_tmp[df_tmp['Borderline']].to_csv(\"Excel/borderline_patients.csv\", index=False)\n",
    "\n",
    "\n",
    "# --- Da qui in poi puoi riutilizzare le tue stesse funzioni di analisi, come:\n",
    "# - il grafico demografico `plot_cluster_demographics(df_tmp.rename(columns={\"Cluster_GMM\": \"Cluster\"}))`\n",
    "# - istogrammi su TIR, TARLV2 e HbA1c usando 'Cluster_GMM' al posto di 'Cluster'\n",
    "\n",
    "\n",
    "\n",
    "# --- ISTOGRAMMI PER TIR, TARLV2 e HbA1c rispetto ai cluster GMM ---\n",
    "\n",
    "# Definizione delle colonne e dei bin\n",
    "tir_tar_features = ['%TIR_last3m', '%TARLV2_last3m']\n",
    "bins_perc = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels_perc = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "# Creazione dei bin per %TIR e %TAR\n",
    "for feature in tir_tar_features:\n",
    "    df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins_perc, labels=labels_perc, include_lowest=True)\n",
    "\n",
    "    # Conta i pazienti per cluster GMM e fascia\n",
    "    counts = df_tmp.groupby(['Cluster_GMM', f'{feature}_bin']).size().reset_index(name='Count')\n",
    "    counts.rename(columns={f'{feature}_bin': 'Fascia'}, inplace=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=counts, x='Fascia', y='Count', hue='Cluster_GMM', palette='Set2', dodge=True)\n",
    "    plt.title(f'Distribuzione {feature} nei due cluster GMM')\n",
    "    plt.xlabel('Fasce di percentuale')\n",
    "    plt.ylabel('Numero di pazienti')\n",
    "    plt.legend(title='Cluster GMM')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- HbA1c ---\n",
    "a1c_col = 'Glycated hemoglobin (A1c)_1'\n",
    "bins_a1c = [0, 5, 6, 7, 8, 9, 10, 11, 12, np.inf]\n",
    "labels_a1c = ['≤5', '5-6', '6-7', '7-8', '8-9', '9-10', '10-11', '11-12', '>12']\n",
    "\n",
    "df_tmp['A1c_bin'] = pd.cut(df_tmp[a1c_col], bins=bins_a1c, labels=labels_a1c, include_lowest=True, right=False)\n",
    "\n",
    "# Conta i pazienti per cluster GMM e fascia\n",
    "a1c_counts = df_tmp.groupby(['Cluster_GMM', 'A1c_bin']).size().reset_index(name='Count')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=a1c_counts, x='A1c_bin', y='Count', hue='Cluster_GMM', palette='Set2', dodge=True)\n",
    "plt.title('Distribuzione dell\\'emoglobina glicata nei due cluster GMM')\n",
    "plt.xlabel('Fasce di HbA1c (%)')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.legend(title='Cluster GMM')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Verifica se serve invertire i cluster: calcolo le medie della variabile Has_Diagnostics nei due cluster\n",
    "print(\"\\n>>> Media di Has_Diagnostics per cluster GMM:\")\n",
    "print(df_tmp.groupby('Cluster_GMM')['Has_Diagnostics'].mean())\n",
    "\n",
    "# Matrice di confusione originale\n",
    "cm = confusion_matrix(y, clusters)\n",
    "print(\"\\n>>> Matrice di confusione (etichetta: Has_Diagnostics vs Cluster_GMM):\")\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Calcolo delle metriche di valutazione del clustering\n",
    "sil_score = silhouette_score(X_scaled, clusters)\n",
    "dbi = davies_bouldin_score(X_scaled, clusters)\n",
    "chi = calinski_harabasz_score(X_scaled, clusters)\n",
    "ari = adjusted_rand_score(y, clusters)  # già calcolato, ma lo ricalcoliamo qui per coerenza\n",
    "\n",
    "print(\"\\n>>> Metriche di valutazione del clustering GMM:\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {chi:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "\n",
    "\n"
   ],
   "id": "da18dd1e5ced442a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CREAZIONE DI 2 DATASET: CON E SENZA COMPLICANZA\n",
    "#A PARTIRE DAL DATASET Clustering2.csv I PAZIENTI SONO STATI DIVISI IN 2 DATASET, CON E SENZA COMPLICANZE (RISPETTIVAMENTE Has_Diagnostics=1 e Has_Diagnostics=2)\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Legge il file originale\n",
    "df = pd.read_csv(\"Excel/Clustering.csv\")\n",
    "\n",
    "# 2. Filtra le righe con e senza complicanze\n",
    "df_complicanze = df[df[\"Has_Diagnostics\"] == 1].copy()\n",
    "df_no_complicanze = df[df[\"Has_Diagnostics\"] == 0].copy()\n",
    "\n",
    "# 3. Rimuove la colonna 'Has_Diagnostics' se presente\n",
    "df_complicanze.drop(columns=[\"Has_Diagnostics\"], inplace=True, errors='ignore')\n",
    "df_no_complicanze.drop(columns=[\"Has_Diagnostics\"], inplace=True, errors='ignore')\n",
    "\n",
    "# 4. Salva i due nuovi file CSV nella stessa cartella\n",
    "df_complicanze.to_csv(\"Excel/Complicanze.csv\", index=False)\n",
    "df_no_complicanze.to_csv(\"Excel/NoComplicanze.csv\", index=False)\n",
    "\n",
    "print(\"File Complicanze.csv e NoComplicanze.csv creati e puliti con successo.\")\n"
   ],
   "id": "afac2a3b9245eae5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CLUSTERING CON KMEANS SUI DUE DATASET CON E SENZA COMPLICANZE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.stats import f_oneway\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Funzione per il plot delle distribuzioni sesso e fasce età per ogni cluster\n",
    "def plot_cluster_demographics(df_tmp, n_clusters):\n",
    "    bins = [0, 29, 49, 69, np.inf]\n",
    "    labels = ['<30', '30-49', '50-69', '>69']\n",
    "    df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    cluster_data = []\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "        age_counts = []\n",
    "        for age_group in labels:\n",
    "            age_df = cluster_df[cluster_df['AgeGroup'] == age_group]\n",
    "            male_count = len(age_df[age_df['Sex'] == 1])\n",
    "            female_count = len(age_df[age_df['Sex'] == 0])\n",
    "            total = male_count + female_count\n",
    "            age_counts.append({'Male': male_count, 'Female': female_count, 'Total': total})\n",
    "        cluster_data.append(age_counts)\n",
    "\n",
    "    x = np.arange(len(labels))  # posizioni fasce d'età\n",
    "    bar_width = 0.8 / n_clusters\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    colors = {'Male': '#1f77b4', 'Female': '#e377c2'}\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        male_counts = [cluster_data[i][j]['Male'] for j in range(len(labels))]\n",
    "        female_counts = [cluster_data[i][j]['Female'] for j in range(len(labels))]\n",
    "        positions = x - 0.4 + i * bar_width + bar_width / 2\n",
    "\n",
    "        bars_male = ax.bar(positions, male_counts, bar_width, color=colors['Male'], edgecolor='black', label=f'Maschi Cluster {i}')\n",
    "        bars_female = ax.bar(positions, female_counts, bar_width, bottom=male_counts, color=colors['Female'], edgecolor='black', label=f'Femmine Cluster {i}')\n",
    "\n",
    "        for bar, count in zip(bars_male, male_counts):\n",
    "            if count > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, str(count), ha='center', va='center', fontsize=8, color='black')\n",
    "        for bar, count, bottom in zip(bars_female, female_counts, male_counts):\n",
    "            if count > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bottom + bar.get_height()/2, str(count), ha='center', va='center', fontsize=8, color='black')\n",
    "\n",
    "        for pos, total in zip(positions, [cluster_data[i][j]['Total'] for j in range(len(labels))]):\n",
    "            if total > 0:\n",
    "                ax.text(pos, total + 1, f'Tot: {total}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlabel('Fascia d\\'età')\n",
    "    ax.set_ylabel('Numero di pazienti')\n",
    "    ax.set_title(f'Distribuzione per cluster, fascia d\\'età e sesso (k={n_clusters})')\n",
    "\n",
    "    # Legenda con solo maschi e femmine\n",
    "    colors = {'Male': '#1f77b4', 'Female': '#e377c2'}\n",
    "    handles = [\n",
    "        Patch(color=colors['Male'], label='Maschi'),\n",
    "        Patch(color=colors['Female'], label='Femmine')\n",
    "    ]\n",
    "    ax.legend(handles=handles, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Nuova funzione per il plot degli istogrammi % TIR, TARLV2, Emoglobina glicata\n",
    "def plot_percentage_histograms(df, clusters, n_clusters):\n",
    "    # Variabili da plottare\n",
    "    variables = ['%TIR_last3m', '%TARLV2_last3m', 'Glycated hemoglobin (A1c)_1']\n",
    "\n",
    "    # Definizione bin personalizzati\n",
    "    default_bins = np.arange(0, 110, 10)\n",
    "    default_labels = [f'{default_bins[i]}-{default_bins[i+1]}%' for i in range(len(default_bins)-1)]\n",
    "\n",
    "    a1c_bins = [0, 5, 6, 7, 8, 9, 10, 11, 12, np.inf]\n",
    "    a1c_labels = [f'{a1c_bins[i]}-{a1c_bins[i+1]}' if a1c_bins[i+1] != np.inf else f'>{a1c_bins[i]}' for i in range(len(a1c_bins)-1)]\n",
    "\n",
    "    for var in variables:\n",
    "        if var == 'Glycated hemoglobin (A1c)_1':\n",
    "            bins = a1c_bins\n",
    "            bin_labels = a1c_labels\n",
    "            xlabel = 'Fasce di HbA1c (%)'\n",
    "        else:\n",
    "            bins = default_bins\n",
    "            bin_labels = default_labels\n",
    "            xlabel = f'Fasce di {var} (%)'\n",
    "\n",
    "        plt.figure(figsize=(12,6))\n",
    "        x = np.arange(len(bin_labels))\n",
    "        bar_width = 0.8 / n_clusters\n",
    "        colors = plt.cm.tab10.colors\n",
    "\n",
    "        # Per ogni cluster, conto pazienti per bin\n",
    "        counts_per_cluster = []\n",
    "        for c in range(n_clusters):\n",
    "            cluster_vals = df.loc[clusters == c, var]\n",
    "            cats = pd.cut(cluster_vals, bins=bins, labels=bin_labels, include_lowest=True, right=False)\n",
    "            counts = cats.value_counts().reindex(bin_labels, fill_value=0)\n",
    "            counts_per_cluster.append(counts.values)\n",
    "\n",
    "        for c in range(n_clusters):\n",
    "            positions = x - 0.4 + c * bar_width + bar_width/2\n",
    "            plt.bar(positions, counts_per_cluster[c], width=bar_width, color=colors[c], edgecolor='black', label=f'Cluster {c}')\n",
    "            for pos, count in zip(positions, counts_per_cluster[c]):\n",
    "                if count > 0:\n",
    "                    plt.text(pos, count/2, str(count), ha='center', va='center', fontsize=8)\n",
    "\n",
    "        plt.xticks(x, bin_labels, rotation=45)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel('Numero di pazienti')\n",
    "        plt.title(f'Distribuzione di {var} per cluster (k={n_clusters})')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- Main ---\n",
    "\n",
    "# 1. Caricamento dati\n",
    "df = pd.read_csv(\"Excel/Complicanze.csv\")\n",
    "#df = pd.read_csv(\"Excel/NoComplicanze.csv\")\n",
    "\n",
    "# 2. Separazione colonne demografiche (le reinseriremo dopo per i plot)\n",
    "demographic_cols = ['Age', 'Sex']\n",
    "demographic_df = df[demographic_cols]\n",
    "\n",
    "# 3. Dati per clustering (tutte le colonne tranne 'Age' e 'Sex')\n",
    "X = df.drop(columns=demographic_cols)\n",
    "\n",
    "# 4. Imputazione valori mancanti con media\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 5. Standardizzazione\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imp)\n",
    "\n",
    "# 6. Clustering per k=2,3,4\n",
    "for k in [2, 3, 4]:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # DataFrame temporaneo per plot + cluster\n",
    "    df_tmp = demographic_df.copy()\n",
    "    df_tmp['Cluster'] = clusters\n",
    "\n",
    "    # Plot distribuzioni demografiche\n",
    "    plot_cluster_demographics(df_tmp, n_clusters=k)\n",
    "\n",
    "    # Plot istogrammi percentuali TIR, TARLV2, Emoglobina glicata\n",
    "    plot_percentage_histograms(df, clusters, n_clusters=k)\n",
    "\n",
    "    # Metriche di validazione clustering\n",
    "    sil = silhouette_score(X_scaled, clusters)\n",
    "    db = davies_bouldin_score(X_scaled, clusters)\n",
    "    ch = calinski_harabasz_score(X_scaled, clusters)\n",
    "\n",
    "    print(f\"Metriche clustering per k={k}:\")\n",
    "    print(f\" - Silhouette Score: {sil:.4f}\")\n",
    "    print(f\" - Davies-Bouldin Index: {db:.4f}\")\n",
    "    print(f\" - Calinski-Harabasz Index: {ch:.4f}\")\n",
    "\n",
    "    # Analisi ANOVA per ogni feature, confronto tra cluster\n",
    "    summary = []\n",
    "    feature_names = X.columns\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        groups = [X_scaled[clusters == cluster, i] for cluster in range(k)]\n",
    "        stat, p = f_oneway(*groups)\n",
    "        means = [np.mean(g) for g in groups]\n",
    "        stds = [np.std(g) for g in groups]\n",
    "\n",
    "        entry = {'Feature': feature, 'ANOVA_p_value': p}\n",
    "        for c in range(k):\n",
    "            entry[f'Cluster{c}_Mean'] = means[c]\n",
    "            entry[f'Cluster{c}_Std'] = stds[c]\n",
    "        summary.append(entry)\n",
    "\n",
    "    df_summary = pd.DataFrame(summary).sort_values(\"ANOVA_p_value\").reset_index(drop=True)\n",
    "    df_summary.index += 1\n",
    "\n",
    "    print(f\"\\n>>> Analisi ANOVA tra cluster k={k}:\")\n",
    "    print(df_summary.to_string())\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n"
   ],
   "id": "c39d9b2a9fde5443"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CREA UN DATASET PER IL CLUSTERING CHE MANTENGA LA COLONNA DELL'ID DEL PAZIENTE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Carica i due dataset\n",
    "df1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "df2 = pd.read_csv(\"Excel/parte2_v1.csv\")\n",
    "\n",
    "# Rimuove la colonna \"Complicanze\" dal secondo dataset\n",
    "df2 = df2.drop(columns=[\"Complicanze\"])\n",
    "\n",
    "# Effettua il merge tra df1 (usando Patient_ID) e df2 (usando ID)\n",
    "merged_df = pd.merge(df1, df2, left_on=\"Patient_ID\", right_on=\"ID\", how=\"inner\")\n",
    "\n",
    "merged_df = merged_df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Salva il dataframe risultante in un file CSV\n",
    "merged_df.to_csv(\"Excel/Clustering2.csv\", index=False)\n",
    "\n",
    "print(\"Merge completato. File salvato come 'Clustering2.csv'.\")\n"
   ],
   "id": "a10ae39170465ffd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CREA UN DATASET PER IL CLUSTERING CHE MANTENGA LA COLONNA DELL'ID DEL PAZIENTE IN MODO DA POTERLA TOGLIERE PER FARE CLUSTERING MA POTERLA USARE PER DETERMINARE QUALE PAZIENTE E' STATO INSERITO IN QUALE GRUPPO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Carica i due dataset\n",
    "df1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "df2 = pd.read_csv(\"Excel/parte2_v1.csv\")\n",
    "\n",
    "# Rimuove la colonna \"Complicanze\" dal secondo dataset\n",
    "df2 = df2.drop(columns=[\"Complicanze\"])\n",
    "\n",
    "# Effettua il merge tra df1 (usando Patient_ID) e df2 (usando ID)\n",
    "merged_df = pd.merge(df1, df2, left_on=\"Patient_ID\", right_on=\"ID\", how=\"inner\")\n",
    "\n",
    "merged_df = merged_df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Salva il dataframe risultante in un file CSV\n",
    "merged_df.to_csv(\"Excel/Clustering2.csv\", index=False)\n",
    "\n",
    "print(\"Merge completato. File salvato come 'Clustering2.csv'.\")\n"
   ],
   "id": "3a5d50937eadb940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#ANALISI DI KMEANS SUL NUOVO DATASET CLUSTERING2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# 1. Caricamento dei dati\n",
    "df = pd.read_csv(\"Excel/Clustering2.csv\")\n",
    "X = df.drop(columns=[\"Has_Diagnostics\",\"Patient_ID\"])\n",
    "y = df[\"Has_Diagnostics\"]\n",
    "\n",
    "# 2. Imputazione dei valori mancanti (media)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 3. Standardizzazione\n",
    "X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# 4. KMeans clustering\n",
    "kmeans = KMeans(\n",
    "    n_clusters=2,\n",
    "    init='random',\n",
    "    n_init=10,\n",
    "    max_iter=100,\n",
    "    tol=0.001,\n",
    "    algorithm='lloyd',\n",
    "    random_state=42\n",
    ")\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 5. Aggiunta cluster al DataFrame\n",
    "df_tmp = df.copy()\n",
    "df_tmp['Cluster'] = clusters\n",
    "\n",
    "# 6. Analisi delle feature per cluster (media, std, ANOVA) Analysis of Variance per vedere se la media delle features e' significativamente diversa tra i due cluster\n",
    "feature_names = X.columns\n",
    "summary = []\n",
    "\n",
    "\n",
    "#Per ogni feature del dataset viene calcolata la media e la deviazione standard nei due cluster e il valore p del test ANOVA\n",
    "#Se p > 0.05 allora la differenza tra le media nei due cluster non e' significativa, altrimenti  e' significativa\n",
    "for i, feature in enumerate(feature_names):\n",
    "    cluster_0 = X_scaled[clusters == 0, i]\n",
    "    cluster_1 = X_scaled[clusters == 1, i]\n",
    "    stat, p = f_oneway(cluster_0, cluster_1)\n",
    "    summary.append({\n",
    "        'Feature': feature,\n",
    "        'Cluster0_Mean': np.mean(cluster_0),\n",
    "        'Cluster1_Mean': np.mean(cluster_1),\n",
    "        'Cluster0_Std': np.std(cluster_0),\n",
    "        'Cluster1_Std': np.std(cluster_1),\n",
    "        'ANOVA_p_value': p\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary = df_summary.sort_values(\"ANOVA_p_value\").reset_index(drop=True)\n",
    "df_summary.index = df_summary.index + 1  # Indici da 1 a N\n",
    "\n",
    "print(\"\\n>>> Analisi ANOVA tra cluster (feature ordinati per significatività):\")\n",
    "print(df_summary.to_string())\n",
    "\n",
    "\n",
    "# 7. Feature Importance con Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaled, clusters)\n",
    "importances = rf.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 8. Plot delle feature più importanti\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances_df, color='skyblue')\n",
    "\n",
    "plt.title('Importanza delle feature secondo Random Forest')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Funzione per il grafico demografico\n",
    "def plot_cluster_demographics(df_tmp):\n",
    "    bins = [0, 29, 49, 69, np.inf]\n",
    "    labels = ['<30', '30-49', '50-69', '>69']\n",
    "    df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    cluster_data = []\n",
    "    for cluster in [0, 1]:\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "        age_counts = []\n",
    "        for age_group in labels:\n",
    "            age_df = cluster_df[cluster_df['AgeGroup'] == age_group]\n",
    "            male_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            male_no_dx = len(age_df[(age_df['Sex'] == 1) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            female_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 1)])\n",
    "            female_no_dx = len(age_df[(age_df['Sex'] == 0) & (age_df['Has_Diagnostics'] == 0)])\n",
    "            age_counts.append({\n",
    "                'Male_Dx': male_dx,\n",
    "                'Male_NoDx': male_no_dx,\n",
    "                'Female_Dx': female_dx,\n",
    "                'Female_NoDx': female_no_dx,\n",
    "                'Total': male_dx + male_no_dx + female_dx + female_no_dx\n",
    "            })\n",
    "        cluster_data.append(age_counts)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    colors = {\n",
    "        'Male_Dx': '#1f77b4',\n",
    "        'Male_NoDx': '#a5c8e4',\n",
    "        'Female_Dx': '#e377c2',\n",
    "        'Female_NoDx': '#f8b8d8'\n",
    "    }\n",
    "\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(labels))\n",
    "    for i, cluster in enumerate([0, 1]):\n",
    "        positions = x + i * bar_width\n",
    "        bottom = np.zeros(len(labels))\n",
    "        for segment in ['Male_Dx', 'Male_NoDx', 'Female_Dx', 'Female_NoDx']:\n",
    "            counts = [cluster_data[cluster][j][segment] for j in range(len(labels))]\n",
    "            ax.bar(positions, counts, bar_width, bottom=bottom, color=colors[segment], edgecolor='black')\n",
    "            for j, count in enumerate(counts):\n",
    "                if count > 0:\n",
    "                    ax.text(positions[j], bottom[j] + count/2, str(count), ha='center', va='center', fontsize=8)\n",
    "            bottom += counts\n",
    "        for j in range(len(labels)):\n",
    "            total = cluster_data[cluster][j]['Total']\n",
    "            ax.text(positions[j], bottom[j] + 5, f'Tot: {total}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x + bar_width / 2)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlabel('Fascia d\\'età')\n",
    "    ax.set_ylabel('Numero di pazienti')\n",
    "    ax.set_title('Distribuzione dei pazienti per cluster, fascia d\\'età, sesso e presenza di complicanze')\n",
    "\n",
    "    handles = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_Dx'], label='Maschi con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Male_NoDx'], label='Maschi senza complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_Dx'], label='Femmine con complicanze'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=colors['Female_NoDx'], label='Femmine senza complicanze'),\n",
    "    ]\n",
    "    ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. Mostra il grafico demografico\n",
    "plot_cluster_demographics(df_tmp)\n",
    "\n",
    "\n",
    "# 11. Istogrammi separati per %TIR_last3m e %TARLV2_last3m\n",
    "\n",
    "\n",
    "# 2 ISTOGRAMMI\n",
    "# Colonne target\n",
    "features = ['%TIR_last3m', '%TARLV2_last3m']\n",
    "\n",
    "# Definizione fasce\n",
    "bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "for feature in features:\n",
    "    # Crea la colonna con le fasce\n",
    "    df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "    # Conta i pazienti per cluster e fascia\n",
    "    counts = df_tmp.groupby(['Cluster', f'{feature}_bin']).size().reset_index(name='Count')\n",
    "    counts.rename(columns={f'{feature}_bin': 'Fascia'}, inplace=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=counts, x='Fascia', y='Count', hue='Cluster', palette='Set2', dodge=True)\n",
    "\n",
    "    plt.title('')\n",
    "    plt.xlabel('Fasce di percentuale')\n",
    "    plt.ylabel('Numero di pazienti')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 4 ISTOGRAMMI\n",
    "# Colonne target\n",
    "features = ['%TIR_last3m', '%TARLV2_last3m']\n",
    "\n",
    "# Definizione fasce\n",
    "bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "# Crea le colonne binned per ogni feature\n",
    "for feature in features:\n",
    "    df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Per ogni combinazione di feature e cluster, plottiamo un istogramma\n",
    "for feature in features:\n",
    "    for cluster in [0, 1]:\n",
    "        # Filtra solo i dati del cluster corrente\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "\n",
    "        # Conta i pazienti nelle fasce\n",
    "        counts = cluster_df[f'{feature}_bin'].value_counts().sort_index()\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x=counts.index, y=counts.values, color='cornflowerblue', edgecolor='black')\n",
    "\n",
    "        plt.title(f'Distribuzione {feature} - Cluster {cluster}')\n",
    "        plt.xlabel('Fasce di percentuale')\n",
    "        plt.ylabel('Numero di pazienti')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "a1c_col = 'Glycated hemoglobin (A1c)_1'\n",
    "\n",
    "# Definizione dei bin per l'emoglobina glicata\n",
    "bins_a1c = [0, 5, 6, 7, 8, 9, 10, 11, 12, np.inf]\n",
    "labels_a1c = ['≤5', '5-6', '6-7', '7-8', '8-9', '9-10', '10-11', '11-12', '>12']\n",
    "\n",
    "# Crea la colonna binned\n",
    "df_tmp['A1c_bin'] = pd.cut(df_tmp[a1c_col], bins=bins_a1c, labels=labels_a1c, include_lowest=True, right=False)\n",
    "\n",
    "# Conta i pazienti per cluster e fascia\n",
    "a1c_counts = df_tmp.groupby(['Cluster', 'A1c_bin']).size().reset_index(name='Count')\n",
    "\n",
    "# Plot istogramma a due colonne\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=a1c_counts, x='A1c_bin', y='Count', hue='Cluster', palette='Set2', dodge=True)\n",
    "\n",
    "plt.title('Distribuzione dell\\'emoglobina glicata nei due cluster')\n",
    "plt.xlabel('Fasce di HbA1c (%)')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.legend(title='Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "\n",
    "\n",
    "# Calcolo delle metriche di valutazione del clustering\n",
    "sil_score = silhouette_score(X_scaled, clusters)\n",
    "dbi = davies_bouldin_score(X_scaled, clusters)\n",
    "chi = calinski_harabasz_score(X_scaled, clusters)\n",
    "ari = adjusted_rand_score(y, clusters)  # già calcolato, ma lo ricalcoliamo qui per coerenza\n",
    "\n",
    "print(\"\\n>>> Metriche di valutazione del clustering KMeans:\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {chi:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "\n",
    "#ANALISI IN DETTAGLIO DELLE COMPLICANZE\n",
    "\n",
    "# 12. Analisi delle complicanze per cluster usando Diagnostics.csv\n",
    "# Carica il dataset delle complicanze\n",
    "diagnostics_df = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "# Associa il cluster a ciascun paziente creando un nuovo dataset\n",
    "patient_clusters = df_tmp[['Patient_ID', 'Cluster']]\n",
    "\n",
    "# Unisci le informazioni di cluster alle complicanze\n",
    "diag_with_cluster = diagnostics_df.merge(patient_clusters, on=\"Patient_ID\", how=\"inner\")\n",
    "\n",
    "# Conta le complicanze per Description e Cluster\n",
    "complication_counts = diag_with_cluster.groupby(['Description', 'Cluster']).size().reset_index(name='Count')\n",
    "#OTTENIAMO UNA COSA DEL GENERE:\n",
    "#Description              Cluster    Count\n",
    "# Neuropathy               0          12\n",
    "# Neuropathy               1          28\n",
    "\n",
    "# Calcola il totale delle complicanze e seleziona le 15 piu frequenti\n",
    "complication_totals = complication_counts.groupby('Description')['Count'].sum().reset_index()\n",
    "top_complications = complication_totals.sort_values('Count', ascending=False).head(15)['Description']\n",
    "\n",
    "# Filtro per le prime 15 complicanze più frequenti\n",
    "complication_counts_filtered = complication_counts[complication_counts['Description'].isin(top_complications)]\n",
    "\n",
    "# Plot: Complicanze più frequenti nei due cluster\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=complication_counts_filtered, x='Count', y='Description', hue='Cluster', palette='Set2')\n",
    "plt.title('Distribuzione delle complicanze nei due cluster')\n",
    "plt.xlabel('Numero di pazienti')\n",
    "plt.ylabel('Complicanza')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Tabella di contingenza\n",
    "contingency_table = pd.crosstab(\n",
    "    diag_with_cluster[diag_with_cluster['Description'].isin(top_complications)]['Description'],\n",
    "    diag_with_cluster[diag_with_cluster['Description'].isin(top_complications)]['Cluster']\n",
    ")\n",
    "print(contingency_table)\n",
    "\n",
    "\n",
    "# Test del chi-quadro\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-quadro: chi2 = {chi2:.2f}, p-value = {p:.4f}\")\n",
    "\n"
   ],
   "id": "a11de734de6c970a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#ANALISI DI GMM SUL NUOVO DATASET CLUSTERING2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# 1. Caricamento dei dati\n",
    "df = pd.read_csv(\"Excel/Clustering2.csv\")\n",
    "X = df.drop(columns=[\"Has_Diagnostics\",\"Patient_ID\"])\n",
    "y = df[\"Has_Diagnostics\"]\n",
    "\n",
    "# 2. Imputazione dei valori mancanti (media)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 3. Standardizzazione\n",
    "X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# 4. GMM clustering\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm.fit(X_scaled)\n",
    "clusters = gmm.predict(X_scaled)\n",
    "probs = gmm.predict_proba(X_scaled)  # probabilità di appartenenza\n",
    "\n",
    "# 5. Aggiunta cluster e probabilità al DataFrame\n",
    "df_tmp = df.copy()\n",
    "df_tmp['Cluster_GMM'] = clusters\n",
    "df_tmp['GMM_Prob0'] = probs[:, 0]\n",
    "df_tmp['GMM_Prob1'] = probs[:, 1]\n",
    "\n",
    "# 6. Analisi ANOVA tra i due cluster\n",
    "feature_names = X.columns\n",
    "summary = []\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    cluster_0 = X_scaled[clusters == 0, i]\n",
    "    cluster_1 = X_scaled[clusters == 1, i]\n",
    "    stat, p = f_oneway(cluster_0, cluster_1)\n",
    "    summary.append({\n",
    "        'Feature': feature,\n",
    "        'Cluster0_Mean': np.mean(cluster_0),\n",
    "        'Cluster1_Mean': np.mean(cluster_1),\n",
    "        'Cluster0_Std': np.std(cluster_0),\n",
    "        'Cluster1_Std': np.std(cluster_1),\n",
    "        'ANOVA_p_value': p\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary).sort_values(\"ANOVA_p_value\").reset_index(drop=True)\n",
    "df_summary.index += 1\n",
    "\n",
    "print(\"\\n>>> Analisi ANOVA tra cluster GMM:\")\n",
    "print(df_summary.to_string())\n",
    "\n",
    "# 7. Feature Importance con Random Forest (etichetta = cluster GMM)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaled, clusters)\n",
    "importances = rf.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 8. Plot delle feature più importanti\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances_df, color='skyblue')\n",
    "plt.title('Importanza delle feature secondo Random Forest (Cluster GMM)')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Istogramma delle probabilità di appartenenza\n",
    "#PRIMA DIVISI\n",
    "# Ottieni i colori da Set2\n",
    "colors = sns.color_palette(\"Set2\", 2)\n",
    "\n",
    "# Istogrammi separati con colori coerenti\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "sns.histplot(df_tmp['GMM_Prob0'], bins=20, kde=True, color=colors[0], ax=axs[0])\n",
    "axs[0].set_title('Probabilità di appartenenza al Cluster 0')\n",
    "axs[0].set_xlabel('GMM_Prob0')\n",
    "axs[0].set_ylabel('Numero di pazienti')\n",
    "\n",
    "sns.histplot(df_tmp['GMM_Prob1'], bins=20, kde=True, color=colors[1], ax=axs[1])\n",
    "axs[1].set_title('Probabilità di appartenenza al Cluster 1')\n",
    "axs[1].set_xlabel('GMM_Prob1')\n",
    "axs[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#POI SOVRAPPOSTI\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_tmp[['GMM_Prob0', 'GMM_Prob1']], bins=20, kde=True, palette='Set2')\n",
    "plt.title('Distribuzione delle probabilità di appartenenza ai cluster GMM')\n",
    "plt.xlabel('Probabilità')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 10. Pazienti \"borderline\" (probabilità compresa tra 0.4 e 0.6)\n",
    "df_tmp['Borderline'] = ((df_tmp['GMM_Prob0'] >= 0.4) & (df_tmp['GMM_Prob0'] <= 0.6)) | \\\n",
    "                       ((df_tmp['GMM_Prob1'] >= 0.4) & (df_tmp['GMM_Prob1'] <= 0.6))\n",
    "n_borderline = df_tmp['Borderline'].sum()\n",
    "print(f\"\\n>>> Pazienti borderline (probabilità ∈ [0.4, 0.6]): {n_borderline} su {len(df_tmp)}\")\n",
    "\n",
    "# (facoltativo) Mostra le prime righe dei pazienti borderline\n",
    "print(\"\\n>>> Esempi di pazienti borderline:\")\n",
    "# Mostrare tutte le colonne senza spezzare a capo\n",
    "pd.set_option('display.max_columns', None)  # mostra tutte le colonne\n",
    "pd.set_option('display.width', 2000)        # larghezza massima per la stampa (molto larga)\n",
    "pd.set_option('display.max_colwidth', None) # non troncare le colonne\n",
    "\n",
    "# Poi stampa\n",
    "print(df_tmp[df_tmp['Borderline']].head().to_string(index=False))\n",
    "df_tmp[df_tmp['Borderline']].to_csv(\"Excel/borderline_patients.csv\", index=False)\n",
    "\n",
    "\n",
    "# --- Da qui in poi puoi riutilizzare le tue stesse funzioni di analisi, come:\n",
    "# - il grafico demografico `plot_cluster_demographics(df_tmp.rename(columns={\"Cluster_GMM\": \"Cluster\"}))`\n",
    "# - istogrammi su TIR, TARLV2 e HbA1c usando 'Cluster_GMM' al posto di 'Cluster'\n",
    "\n",
    "\n",
    "\n",
    "# --- ISTOGRAMMI PER TIR, TARLV2 e HbA1c rispetto ai cluster GMM ---\n",
    "\n",
    "# Definizione delle colonne e dei bin\n",
    "tir_tar_features = ['%TIR_last3m', '%TARLV2_last3m']\n",
    "bins_perc = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels_perc = ['0', '1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "\n",
    "# Creazione dei bin per %TIR e %TAR\n",
    "for feature in tir_tar_features:\n",
    "    df_tmp[f'{feature}_bin'] = pd.cut(df_tmp[feature], bins=bins_perc, labels=labels_perc, include_lowest=True)\n",
    "\n",
    "    # Conta i pazienti per cluster GMM e fascia\n",
    "    counts = df_tmp.groupby(['Cluster_GMM', f'{feature}_bin']).size().reset_index(name='Count')\n",
    "    counts.rename(columns={f'{feature}_bin': 'Fascia'}, inplace=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=counts, x='Fascia', y='Count', hue='Cluster_GMM', palette='Set2', dodge=True)\n",
    "    plt.title(f'Distribuzione {feature} nei due cluster GMM')\n",
    "    plt.xlabel('Fasce di percentuale')\n",
    "    plt.ylabel('Numero di pazienti')\n",
    "    plt.legend(title='Cluster GMM')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- HbA1c ---\n",
    "a1c_col = 'Glycated hemoglobin (A1c)_1'\n",
    "bins_a1c = [0, 5, 6, 7, 8, 9, 10, 11, 12, np.inf]\n",
    "labels_a1c = ['≤5', '5-6', '6-7', '7-8', '8-9', '9-10', '10-11', '11-12', '>12']\n",
    "\n",
    "df_tmp['A1c_bin'] = pd.cut(df_tmp[a1c_col], bins=bins_a1c, labels=labels_a1c, include_lowest=True, right=False)\n",
    "\n",
    "# Conta i pazienti per cluster GMM e fascia\n",
    "a1c_counts = df_tmp.groupby(['Cluster_GMM', 'A1c_bin']).size().reset_index(name='Count')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=a1c_counts, x='A1c_bin', y='Count', hue='Cluster_GMM', palette='Set2', dodge=True)\n",
    "plt.title('Distribuzione dell\\'emoglobina glicata nei due cluster GMM')\n",
    "plt.xlabel('Fasce di HbA1c (%)')\n",
    "plt.ylabel('Numero di pazienti')\n",
    "plt.legend(title='Cluster GMM')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Verifica se serve invertire i cluster: calcolo le medie della variabile Has_Diagnostics nei due cluster\n",
    "print(\"\\n>>> Media di Has_Diagnostics per cluster GMM:\")\n",
    "print(df_tmp.groupby('Cluster_GMM')['Has_Diagnostics'].mean())\n",
    "\n",
    "# Matrice di confusione originale\n",
    "cm = confusion_matrix(y, clusters)\n",
    "print(\"\\n>>> Matrice di confusione (etichetta: Has_Diagnostics vs Cluster_GMM):\")\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Calcolo delle metriche di valutazione del clustering\n",
    "sil_score = silhouette_score(X_scaled, clusters)\n",
    "dbi = davies_bouldin_score(X_scaled, clusters)\n",
    "chi = calinski_harabasz_score(X_scaled, clusters)\n",
    "ari = adjusted_rand_score(y, clusters)  # già calcolato, ma lo ricalcoliamo qui per coerenza\n",
    "\n",
    "print(\"\\n>>> Metriche di valutazione del clustering GMM:\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {chi:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- ANALISI IN DETTAGLIO DELLE COMPLICANZE CON I CLUSTER GMM ---\n",
    "\n",
    "# Carica il dataset delle complicanze\n",
    "diagnostics_df = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "# Associa il cluster GMM a ciascun paziente\n",
    "patient_clusters = df_tmp[['Patient_ID', 'Cluster_GMM']]\n",
    "\n",
    "# Unisci le informazioni di cluster alle complicanze\n",
    "diag_with_cluster = diagnostics_df.merge(patient_clusters, on=\"Patient_ID\", how=\"inner\")\n",
    "\n",
    "# Conta le complicanze per Description e Cluster_GMM\n",
    "complication_counts = diag_with_cluster.groupby(['Description', 'Cluster_GMM']).size().reset_index(name='Count')\n",
    "\n",
    "# Calcola il totale delle complicanze e seleziona le 15 più frequenti\n",
    "complication_totals = complication_counts.groupby('Description')['Count'].sum().reset_index()\n",
    "top_complications = complication_totals.sort_values('Count', ascending=False).head(15)['Description']\n",
    "\n",
    "# Filtro per le prime 15 complicanze più frequenti\n",
    "complication_counts_filtered = complication_counts[complication_counts['Description'].isin(top_complications)]\n",
    "\n",
    "# Plot: complicanze più frequenti nei due cluster GMM\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=complication_counts_filtered, x='Count', y='Description', hue='Cluster_GMM', palette='Set2')\n",
    "plt.title('Distribuzione delle complicanze nei due cluster GMM')\n",
    "plt.xlabel('Numero di pazienti')\n",
    "plt.ylabel('Complicanza')\n",
    "plt.legend(title='Cluster GMM')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Tabella di contingenza per il test chi-quadro\n",
    "contingency_table = pd.crosstab(\n",
    "    diag_with_cluster[diag_with_cluster['Description'].isin(top_complications)]['Description'],\n",
    "    diag_with_cluster[diag_with_cluster['Description'].isin(top_complications)]['Cluster_GMM']\n",
    ")\n",
    "\n",
    "print(\"\\nTabella di contingenza complicanze cluster GMM:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Test del chi-quadro per verificare associazione\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"\\nChi-quadro: chi2 = {chi2:.2f}, p-value = {p:.4f}\")\n",
    "\n",
    "\n"
   ],
   "id": "d87b2ad9c69c18e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CREAZIONE DI 2 DATASET: CON E SENZA COMPLICANZA, UGUALI AI DUE PRECEDENTI MA SENZA COLONNA Patient_ID\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Legge il file originale\n",
    "df = pd.read_csv(\"Excel/Clustering2.csv\")\n",
    "\n",
    "# 2. Filtra le righe con e senza complicanze\n",
    "df_complicanze = df[df[\"Has_Diagnostics\"] == 1].copy()\n",
    "df_no_complicanze = df[df[\"Has_Diagnostics\"] == 0].copy()\n",
    "\n",
    "# 3. Rimuove la colonna 'Has_Diagnostics' se presente\n",
    "df_complicanze.drop(columns=[\"Has_Diagnostics\"], inplace=True, errors='ignore')\n",
    "df_no_complicanze.drop(columns=[\"Has_Diagnostics\"], inplace=True, errors='ignore')\n",
    "\n",
    "# 4. Salva il nuovo file CSV nella stessa cartella\n",
    "df_complicanze.to_csv(\"Excel/Complicanze2.csv\", index=False)\n",
    "df_no_complicanze.to_csv(\"Excel/NoComplicanze2.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"File Complicanze2.csv creato e pulito con successo.\")\n",
    "print(\"File NoComplicanze2.csv creato e pulito con successo.\")\n"
   ],
   "id": "b18c1754e13db72c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CLUSTERING CON KMEANS SU DATASET CON E SENZA COMPLICANZE\n",
    "#NOTA CHE IL CODICE E' LO STESSO PER ENTRAMBI, BASTA COMMENTARE E DECOMMENTARE A RIGA 130 E 131 PER SECGLIERE SU QUALE DATASET APPLICARLO, MA IL TEST CHI-QUADRO SE APPLICATO SU NoComplicanze2.csv FALLIRA' ED IL CODICE RESTITUIRA' ERRORE, PERCHE' LA TABELLA DI CONTINGENZA CHE SI OTTIENE E' VUOTA, VISTO CHE NON CI SONO COMPLICANZE, INVECE IL RESTO DEL CODICE MOSTRA I GRAFICI PER QUEL DATASET.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.stats import f_oneway\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Funzione per il plot delle distribuzioni sesso e fasce età per ogni cluster\n",
    "def plot_cluster_demographics(df_tmp, n_clusters):\n",
    "    bins = [0, 29, 49, 69, np.inf]\n",
    "    labels = ['<30', '30-49', '50-69', '>69']\n",
    "    df_tmp['AgeGroup'] = pd.cut(df_tmp['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    cluster_data = []\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_df = df_tmp[df_tmp['Cluster'] == cluster]\n",
    "        age_counts = []\n",
    "        for age_group in labels:\n",
    "            age_df = cluster_df[cluster_df['AgeGroup'] == age_group]\n",
    "            male_count = len(age_df[age_df['Sex'] == 1])\n",
    "            female_count = len(age_df[age_df['Sex'] == 0])\n",
    "            total = male_count + female_count\n",
    "            age_counts.append({'Male': male_count, 'Female': female_count, 'Total': total})\n",
    "        cluster_data.append(age_counts)\n",
    "\n",
    "    x = np.arange(len(labels))  # posizioni fasce d'età\n",
    "    bar_width = 0.8 / n_clusters\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    colors = {'Male': '#1f77b4', 'Female': '#e377c2'}\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        male_counts = [cluster_data[i][j]['Male'] for j in range(len(labels))]\n",
    "        female_counts = [cluster_data[i][j]['Female'] for j in range(len(labels))]\n",
    "        positions = x - 0.4 + i * bar_width + bar_width / 2\n",
    "\n",
    "        bars_male = ax.bar(positions, male_counts, bar_width, color=colors['Male'], edgecolor='black', label=f'Maschi Cluster {i}')\n",
    "        bars_female = ax.bar(positions, female_counts, bar_width, bottom=male_counts, color=colors['Female'], edgecolor='black', label=f'Femmine Cluster {i}')\n",
    "\n",
    "        for bar, count in zip(bars_male, male_counts):\n",
    "            if count > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, str(count), ha='center', va='center', fontsize=8, color='black')\n",
    "        for bar, count, bottom in zip(bars_female, female_counts, male_counts):\n",
    "            if count > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bottom + bar.get_height()/2, str(count), ha='center', va='center', fontsize=8, color='black')\n",
    "\n",
    "        for pos, total in zip(positions, [cluster_data[i][j]['Total'] for j in range(len(labels))]):\n",
    "            if total > 0:\n",
    "                ax.text(pos, total + 1, f'Tot: {total}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlabel('Fascia d\\'età')\n",
    "    ax.set_ylabel('Numero di pazienti')\n",
    "    ax.set_title(f'Distribuzione per cluster, fascia d\\'età e sesso (k={n_clusters})')\n",
    "\n",
    "    # Legenda con solo maschi e femmine\n",
    "    colors = {'Male': '#1f77b4', 'Female': '#e377c2'}\n",
    "    handles = [\n",
    "        Patch(color=colors['Male'], label='Maschi'),\n",
    "        Patch(color=colors['Female'], label='Femmine')\n",
    "    ]\n",
    "    ax.legend(handles=handles, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Nuova funzione per il plot degli istogrammi % TIR, TARLV2, Emoglobina glicata\n",
    "def plot_percentage_histograms(df, clusters, n_clusters):\n",
    "    # Variabili da plottare\n",
    "    variables = ['%TIR_last3m', '%TARLV2_last3m', 'Glycated hemoglobin (A1c)_1']\n",
    "\n",
    "    # Definizione bin personalizzati\n",
    "    default_bins = np.arange(0, 110, 10)\n",
    "    default_labels = [f'{default_bins[i]}-{default_bins[i+1]}%' for i in range(len(default_bins)-1)]\n",
    "\n",
    "    a1c_bins = [0, 5, 6, 7, 8, 9, 10, 11, 12, np.inf]\n",
    "    a1c_labels = [f'{a1c_bins[i]}-{a1c_bins[i+1]}' if a1c_bins[i+1] != np.inf else f'>{a1c_bins[i]}' for i in range(len(a1c_bins)-1)]\n",
    "\n",
    "    for var in variables:\n",
    "        if var == 'Glycated hemoglobin (A1c)_1':\n",
    "            bins = a1c_bins\n",
    "            bin_labels = a1c_labels\n",
    "            xlabel = 'Fasce di HbA1c (%)'\n",
    "        else:\n",
    "            bins = default_bins\n",
    "            bin_labels = default_labels\n",
    "            xlabel = f'Fasce di {var} (%)'\n",
    "\n",
    "        plt.figure(figsize=(12,6))\n",
    "        x = np.arange(len(bin_labels))\n",
    "        bar_width = 0.8 / n_clusters\n",
    "        colors = plt.cm.tab10.colors\n",
    "\n",
    "        # Per ogni cluster, conto pazienti per bin\n",
    "        counts_per_cluster = []\n",
    "        for c in range(n_clusters):\n",
    "            cluster_vals = df.loc[clusters == c, var]\n",
    "            cats = pd.cut(cluster_vals, bins=bins, labels=bin_labels, include_lowest=True, right=False)\n",
    "            counts = cats.value_counts().reindex(bin_labels, fill_value=0)\n",
    "            counts_per_cluster.append(counts.values)\n",
    "\n",
    "        for c in range(n_clusters):\n",
    "            positions = x - 0.4 + c * bar_width + bar_width/2\n",
    "            plt.bar(positions, counts_per_cluster[c], width=bar_width, color=colors[c], edgecolor='black', label=f'Cluster {c}')\n",
    "            for pos, count in zip(positions, counts_per_cluster[c]):\n",
    "                if count > 0:\n",
    "                    plt.text(pos, count/2, str(count), ha='center', va='center', fontsize=8)\n",
    "\n",
    "        plt.xticks(x, bin_labels, rotation=45)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel('Numero di pazienti')\n",
    "        plt.title(f'Distribuzione di {var} per cluster (k={n_clusters})')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- Main ---\n",
    "\n",
    "# 1. Caricamento dati\n",
    "df = pd.read_csv(\"Excel/Complicanze2.csv\")\n",
    "# df = pd.read_csv(\"Excel/NoComplicanze2.csv\")\n",
    "\n",
    "# 2. Separazione colonne demografiche (le reinseriremo dopo per i plot)\n",
    "demographic_cols = ['Age', 'Sex', 'Patient_ID']\n",
    "demographic_df = df[demographic_cols]\n",
    "\n",
    "# 3. Dati per clustering (tutte le colonne tranne 'Age' e 'Sex')\n",
    "X = df.drop(columns=demographic_cols)\n",
    "\n",
    "# 4. Imputazione valori mancanti con media\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# 5. Standardizzazione\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imp)\n",
    "\n",
    "# 6. Clustering per k=2\n",
    "k = 2\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# DataFrame temporaneo per plot + cluster\n",
    "df_tmp = demographic_df.copy()\n",
    "df_tmp['Cluster'] = clusters\n",
    "\n",
    "# Plot distribuzioni demografiche\n",
    "plot_cluster_demographics(df_tmp, n_clusters=k)\n",
    "\n",
    "# Plot istogrammi percentuali TIR, TARLV2, Emoglobina glicata\n",
    "plot_percentage_histograms(df, clusters, n_clusters=k)\n",
    "\n",
    "# --- ANALISI IN DETTAGLIO DELLE COMPLICANZE ---\n",
    "\n",
    "# 7. Carica il dataset delle complicanze (diagnostiche)\n",
    "diagnostics_df = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "# Controlla anche qui la presenza di Patient_ID\n",
    "if 'Patient_ID' not in diagnostics_df.columns:\n",
    "    raise ValueError(\"Il dataset Diagnostics.csv deve contenere la colonna 'Patient_ID'.\")\n",
    "\n",
    "# 8. Associa il cluster a ciascun paziente\n",
    "patient_clusters = df_tmp[['Patient_ID', 'Cluster']]\n",
    "\n",
    "# 9. Unisci le informazioni di cluster alle complicanze diagnostiche\n",
    "diag_with_cluster = diagnostics_df.merge(patient_clusters, on=\"Patient_ID\", how=\"inner\")\n",
    "\n",
    "# 10. Conta le complicanze per Description e Cluster\n",
    "complication_counts = diag_with_cluster.groupby(['Description', 'Cluster']).size().reset_index(name='Count')\n",
    "\n",
    "# 11. Calcola il totale delle complicanze e seleziona le 15 più frequenti\n",
    "complication_totals = complication_counts.groupby('Description')['Count'].sum().reset_index()\n",
    "top_complications = complication_totals.sort_values('Count', ascending=False).head(15)['Description']\n",
    "\n",
    "# 12. Filtra per le prime 15 complicanze più frequenti\n",
    "complication_counts_filtered = complication_counts[complication_counts['Description'].isin(top_complications)]\n",
    "\n",
    "# 13. Plot complicanze più frequenti nei due cluster\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=complication_counts_filtered, x='Count', y='Description', hue='Cluster', palette='Set2')\n",
    "plt.title('Distribuzione delle complicanze nei due cluster')\n",
    "plt.xlabel('Numero di pazienti')\n",
    "plt.ylabel('Complicanza')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 14. Test chi-quadro di associazione\n",
    "\n",
    "# Tabella di contingenza\n",
    "contingency_table = pd.crosstab(\n",
    "    diag_with_cluster[diag_with_cluster['Description'].isin(top_complications)]['Description'],\n",
    "    diag_with_cluster[diag_with_cluster['Description'].isin(top_complications)]['Cluster']\n",
    ")\n",
    "print(\"Tabella di contingenza:\\n\", contingency_table)\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-quadro: chi2 = {chi2:.2f}, p-value = {p:.4f}\")"
   ],
   "id": "512f5706007744c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#CONFRONTO TRA I DUE GRUPPI OTTENUTI CONTROLLANDO KMEAN E GMM, (K=1,G=1) E (K=0,G=0), CALCOLO DEL TEST ANOVA E DEL TEST DI TUKEY PER IL GRUPPO A, PER IL GRUPPO B E PER LA SOMMA DI A & B\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# ==============================================\n",
    "# 1. CARICAMENTO E PREPROCESSING DEI DATI\n",
    "# ==============================================\n",
    "# - Carica i dataset da file CSV\n",
    "# - Rimuove colonne non numeriche (ID paziente)\n",
    "# - Gestisce valori mancanti con imputazione della media\n",
    "# - Applica standardizzazione (scala le features)\n",
    "\n",
    "\n",
    "df_all = pd.read_csv(\"Excel/Clustering2.csv\")\n",
    "df_diag = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "X_all = df_all.drop(columns=[\"Patient_ID\", \"Has_Diagnostics\"])\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_all_imp = imputer.fit_transform(X_all)\n",
    "scaler = StandardScaler()\n",
    "X_all_scaled = scaler.fit_transform(X_all_imp)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 2. CLUSTERING\n",
    "# ==============================================\n",
    "# - Applica due algoritmi di clustering:\n",
    "#   * K-Means con 2 cluster\n",
    "#   * Gaussian Mixture Model (GMM) con 2 componenti\n",
    "# - Aggiunge le etichette di cluster al dataframe originale\n",
    "\n",
    "\n",
    "df_all[\"Cluster_KMeans\"] = KMeans(\n",
    "    n_clusters=2,\n",
    "    init='random',\n",
    "    n_init=10,\n",
    "    max_iter=100,\n",
    "    tol=0.001,\n",
    "    algorithm='lloyd',\n",
    "    random_state=42\n",
    ").fit_predict(X_all_scaled)\n",
    "\n",
    "df_all[\"Cluster_GMM\"] = GaussianMixture(\n",
    "    n_components=2,\n",
    "    random_state=42\n",
    ").fit_predict(X_all_scaled)\n",
    "\n",
    "# ==============================================\n",
    "# 3. ANALISI DEI GRUPPI COMBINATI\n",
    "# ==============================================\n",
    "# - Combina i risultati dei due algoritmi di clustering\n",
    "# - Identifica i 4 gruppi combinati (KMeans x GMM)\n",
    "# - Seleziona i due gruppi principali per l'analisi successiva\n",
    "# - Suddivide ulteriormente in pazienti con/senza complicanze\n",
    "\n",
    "\n",
    "group_sizes = (\n",
    "    df_all\n",
    "    .groupby([\"Cluster_KMeans\", \"Cluster_GMM\"])[\"Patient_ID\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"Size\")\n",
    "    .sort_values(\"Size\", ascending=False)\n",
    ")\n",
    "group_order  = list(zip(group_sizes[\"Cluster_KMeans\"], group_sizes[\"Cluster_GMM\"]))\n",
    "group_labels = [f\"K={k}, G={g}\" for k, g in group_order]\n",
    "\n",
    "# --- Selezione dei due gruppi di interesse (K=1,G=1) e (K=0,G=0) ---\n",
    "group_a = df_all[(df_all[\"Cluster_KMeans\"] == 0) & (df_all[\"Cluster_GMM\"] == 0)]\n",
    "group_b = df_all[(df_all[\"Cluster_KMeans\"] == 1) & (df_all[\"Cluster_GMM\"] == 1)]\n",
    "\n",
    "# --- All’interno di ogni gruppo, separo chi ha complicanze da chi non ne ha ---\n",
    "group_a_with    = group_a[group_a[\"Has_Diagnostics\"] == 1]   # K=0,G=0: pazienti CON complicanze\n",
    "group_a_without = group_a[group_a[\"Has_Diagnostics\"] == 0]   # K=0,G=0: pazienti SENZA complicanze\n",
    "group_b_with    = group_b[group_b[\"Has_Diagnostics\"] == 1]   # K=1,G=1: pazienti CON complicanze\n",
    "group_b_without = group_b[group_b[\"Has_Diagnostics\"] == 0]   # K=1,G=1: pazienti SENZA complicanze\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 4. ANALISI DELLE COMPLICANZE DIABETICHE\n",
    "# ==============================================\n",
    "# - Definisce parole chiave per identificare complicanze diabetiche\n",
    "# - Conta pazienti con complicanze diabetiche vs non-diabetiche\n",
    "#   in ciascun gruppo selezionato\n",
    "\n",
    "\n",
    "# --- Parole chiave per identificare complicanze diabetiche (case-insensitive) ---\n",
    "keywords_diab = [\n",
    "    \"diabetes\",\n",
    "    \"diabetic\",\n",
    "    \"neuropathy\",\n",
    "    \"retinopathy\",\n",
    "    \"retinal\",\n",
    "    \"ocular\",\n",
    "    \"hyperglyceridemia\",\n",
    "    \"glycemic\"\n",
    "]\n",
    "\n",
    "def has_diabetic_comp(patient_id: str, diag_df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Ritorna True se almeno una Description per quel Patient_ID\n",
    "    contiene una parola chiave di 'keywords_diab', altrimenti False.\n",
    "    \"\"\"\n",
    "    subset = diag_df[diag_df[\"Patient_ID\"] == patient_id]\n",
    "    if subset.empty:\n",
    "        return False\n",
    "    for desc in subset[\"Description\"].astype(str):\n",
    "        d = desc.lower()\n",
    "        for kw in keywords_diab:\n",
    "            if kw in d:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# --- Conto in ciascun gruppo quanti sono diabetici vs non-diabetici ---\n",
    "count_A_diab = 0\n",
    "count_A_nondiab = 0\n",
    "\n",
    "for pid in group_a_with[\"Patient_ID\"].astype(str):\n",
    "    if has_diabetic_comp(pid, df_diag):\n",
    "        count_A_diab += 1\n",
    "    else:\n",
    "        count_A_nondiab += 1\n",
    "\n",
    "count_B_diab = 0\n",
    "count_B_nondiab = 0\n",
    "\n",
    "for pid in group_b_with[\"Patient_ID\"].astype(str):\n",
    "    if has_diabetic_comp(pid, df_diag):\n",
    "        count_B_diab += 1\n",
    "    else:\n",
    "        count_B_nondiab += 1\n",
    "\n",
    "# --- Stampo i risultati ---\n",
    "print(\"=== Gruppo A (K=0, G=0) ===\")\n",
    "print(f\"  • Con complicanze diabetiche:     {count_A_diab}\")\n",
    "print(f\"  • Con complicanze non-diabetiche: {count_A_nondiab}\")\n",
    "print(f\"  • Senza complicanze:              {len(group_a_without)}\")\n",
    "\n",
    "print(\"\\n=== Gruppo B (K=1, G=1) ===\")\n",
    "print(f\"  • Con complicanze diabetiche:     {count_B_diab}\")\n",
    "print(f\"  • Con complicanze non-diabetiche: {count_B_nondiab}\")\n",
    "print(f\"  • Senza complicanze:              {len(group_b_without)}\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 5. ANALISI STATISTICA (ANOVA E TUKEY)\n",
    "# ==============================================\n",
    "# - Esegue test ANOVA per confrontare le medie tra gruppi\n",
    "# - Applica test post-hoc di Tukey per confronti multipli\n",
    "# - Lo fa separatamente per:\n",
    "#   * Ciascun gruppo combinato (A e B)\n",
    "#   * Tutti i pazienti aggregati\n",
    "\n",
    "\n",
    "\n",
    "def run_anova_and_tukey(df_group, group_name):\n",
    "    print(f\"\\n=== Test su Gruppo {group_name} ===\")\n",
    "\n",
    "    # Colonne numeriche da testare (escludiamo quelle non quantitative)\n",
    "    cols = [col for col in df_group.columns if col not in [\"Patient_ID\", \"Has_Diagnostics\", \"Cluster_KMeans\", \"Cluster_GMM\", \"Class\"]]\n",
    "\n",
    "    results = []\n",
    "    for col in cols:\n",
    "        group1 = df_group[df_group[\"Class\"] == \"diabetic\"][col]\n",
    "        group2 = df_group[df_group[\"Class\"] == \"non_diabetic\"][col]\n",
    "        group3 = df_group[df_group[\"Class\"] == \"healthy\"][col]\n",
    "\n",
    "        # Skip se uno dei gruppi ha solo NaN\n",
    "        if group1.isnull().all() or group2.isnull().all() or group3.isnull().all():\n",
    "            continue\n",
    "\n",
    "        # Test ANOVA\n",
    "        stat, pval = f_oneway(group1.dropna(), group2.dropna(), group3.dropna())\n",
    "        results.append((col, stat, pval))\n",
    "\n",
    "    # Ordina per p-value crescente\n",
    "    results.sort(key=lambda x: x[2])\n",
    "\n",
    "    print(\"=== Risultati ANOVA ===\")\n",
    "    for col, stat, pval in results:\n",
    "        print(f\"{col:<25} | F = {stat:8.3f} | p = {pval:.4e}\")\n",
    "\n",
    "    # Test Tukey HSD\n",
    "    cols_to_test = [\n",
    "        col for col in df_group.columns\n",
    "        if col not in [\"Patient_ID\", \"Has_Diagnostics\", \"Cluster_KMeans\", \"Cluster_GMM\", \"Class\"]\n",
    "        and pd.api.types.is_numeric_dtype(df_group[col])\n",
    "    ]\n",
    "\n",
    "    tukey_results = []\n",
    "    for col in cols_to_test:\n",
    "        data = df_group[[col, \"Class\"]].dropna()\n",
    "        if data[\"Class\"].nunique() < 2:\n",
    "            continue\n",
    "        try:\n",
    "            tukey = pairwise_tukeyhsd(endog=data[col], groups=data[\"Class\"], alpha=0.05)\n",
    "            df_tukey = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])\n",
    "            df_tukey.insert(0, \"Variable\", col)\n",
    "            tukey_results.append(df_tukey)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if tukey_results:\n",
    "        df_tukey_all = pd.concat(tukey_results, ignore_index=True)\n",
    "        print(\"\\n=== Risultati Tukey HSD ===\")\n",
    "        print(df_tukey_all.to_string(index=False))\n",
    "    else:\n",
    "        print(\"Nessun risultato Tukey disponibile\")\n",
    "\n",
    "\n",
    "# Etichetto i pazienti del gruppo A\n",
    "df_labeled_A = pd.concat([group_a_with, group_a_without])\n",
    "df_labeled_A = df_labeled_A.copy()\n",
    "df_labeled_A[\"Class\"] = df_labeled_A[\"Patient_ID\"].astype(str).apply(\n",
    "    lambda pid: \"diabetic\" if has_diabetic_comp(pid, df_diag) else \"non_diabetic\"\n",
    ")\n",
    "df_labeled_A.loc[df_labeled_A[\"Has_Diagnostics\"] == 0, \"Class\"] = \"healthy\"\n",
    "\n",
    "# Etichetto i pazienti del gruppo B\n",
    "df_labeled_B = pd.concat([group_b_with, group_b_without])\n",
    "df_labeled_B = df_labeled_B.copy()\n",
    "df_labeled_B[\"Class\"] = df_labeled_B[\"Patient_ID\"].astype(str).apply(\n",
    "    lambda pid: \"diabetic\" if has_diabetic_comp(pid, df_diag) else \"non_diabetic\"\n",
    ")\n",
    "df_labeled_B.loc[df_labeled_B[\"Has_Diagnostics\"] == 0, \"Class\"] = \"healthy\"\n",
    "\n",
    "# Eseguo ANOVA + Tukey su ciascun gruppo\n",
    "run_anova_and_tukey(df_labeled_A, \"A (K=0, G=0)\")\n",
    "run_anova_and_tukey(df_labeled_B, \"B (K=1, G=1)\")\n",
    "\n",
    "# Rieseguiamo i test ANOVA e Tukey ma sul totale delle 3 categorie dei due gruppi, quindi diabetic A + diabetic B, non_diabetic A + non_diabetic B e healthy A + healthy B\n",
    "df_diabetic = pd.concat([group_a_with, group_b_with])\n",
    "df_diabetic = df_diabetic[df_diabetic[\"Patient_ID\"].astype(str).isin(\n",
    "    [pid for pid in df_diag[\"Patient_ID\"].astype(str) if has_diabetic_comp(pid, df_diag)]\n",
    ")]\n",
    "df_diabetic = df_diabetic.assign(Class=\"diabetic\")\n",
    "\n",
    "df_nondiabetic = pd.concat([group_a_with, group_b_with])\n",
    "df_nondiabetic = df_nondiabetic[~df_nondiabetic[\"Patient_ID\"].isin(df_diabetic[\"Patient_ID\"])]\n",
    "df_nondiabetic = df_nondiabetic.assign(Class=\"non_diabetic\")\n",
    "\n",
    "df_healthy = pd.concat([group_a_without, group_b_without]).assign(Class=\"healthy\")\n",
    "\n",
    "# Unisci tutto\n",
    "df_labeled = pd.concat([df_diabetic, df_nondiabetic, df_healthy])\n",
    "\n",
    "# Colonne numeriche da testare (escludiamo le non quantitative)\n",
    "cols = [col for col in df_labeled.columns if col not in [\"Patient_ID\", \"Has_Diagnostics\", \"Cluster_KMeans\", \"Cluster_GMM\", \"Class\"]]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Per ogni variabile, applica ANOVA a 3 gruppi: diabetic, non_diabetic, healthy\n",
    "for col in cols:\n",
    "    group1 = df_labeled[df_labeled[\"Class\"] == \"diabetic\"][col]\n",
    "    group2 = df_labeled[df_labeled[\"Class\"] == \"non_diabetic\"][col]\n",
    "    group3 = df_labeled[df_labeled[\"Class\"] == \"healthy\"][col]\n",
    "\n",
    "    # Escludi variabili con NaN totali in uno dei gruppi\n",
    "    if group1.isnull().all() or group2.isnull().all() or group3.isnull().all():\n",
    "        continue\n",
    "\n",
    "    stat, pval = f_oneway(group1.dropna(), group2.dropna(), group3.dropna())\n",
    "    results.append((col, stat, pval))\n",
    "\n",
    "# Ordina i risultati per significatività (p-value crescente)\n",
    "results.sort(key=lambda x: x[2])\n",
    "\n",
    "# Stampa tutti i risultati\n",
    "print(\"=== Risultati ANOVA tra gruppi (diabetic vs non_diabetic vs healthy) ===\")\n",
    "for col, stat, pval in results:\n",
    "    print(f\"{col:<25} | F = {stat:8.3f} | p = {pval:.4e}\")\n",
    "\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Prendo tutte le colonne numeriche da confrontare\n",
    "cols_to_test = [\n",
    "    col for col in df_labeled.columns\n",
    "    if col not in [\"Patient_ID\", \"Has_Diagnostics\", \"Cluster_KMeans\", \"Cluster_GMM\", \"Class\"]\n",
    "    and pd.api.types.is_numeric_dtype(df_labeled[col])\n",
    "]\n",
    "\n",
    "# Applico il test di Tukey HSD per ogni variabile\n",
    "tukey_results = []\n",
    "\n",
    "for col in cols_to_test:\n",
    "    data = df_labeled[[col, \"Class\"]].dropna()\n",
    "    if data[\"Class\"].nunique() < 2:\n",
    "        continue\n",
    "    try:\n",
    "        tukey = pairwise_tukeyhsd(endog=data[col], groups=data[\"Class\"], alpha=0.05)\n",
    "        df_tukey = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])\n",
    "        df_tukey.insert(0, \"Variable\", col)\n",
    "        tukey_results.append(df_tukey)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Concateno tutti i risultati in un unico dataframe\n",
    "df_tukey_all = pd.concat(tukey_results, ignore_index=True)\n",
    "print(\"=== Risultati Tukey tra gruppi (diabetic vs non_diabetic vs healthy) ===\")\n",
    "print(df_tukey_all.to_string(index=False))\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 6. VISUALIZZAZIONI\n",
    "# ==============================================\n",
    "# - Genera vari plot per analizzare:\n",
    "#   6.1 Distribuzione per sesso e presenza complicanze\n",
    "#   6.2 Distribuzione %TIR (Time In Range)\n",
    "#   6.3 Distribuzione %TAR (Time Above Range)\n",
    "#   6.4 Top complicanze per gruppo\n",
    "#   6.5 Distribuzione per fasce d'età\n",
    "\n",
    "\n",
    "# ---  6.1 Grafico sesso/complicanze ---\n",
    "group_counts = []\n",
    "\n",
    "for (k_label, g_label) in group_order:\n",
    "    subset = df_all[(df_all[\"Cluster_KMeans\"] == k_label) & (df_all[\"Cluster_GMM\"] == g_label)]\n",
    "    M_with = int(((subset[\"Sex\"] == 1) & (subset[\"Has_Diagnostics\"] == 1)).sum())\n",
    "    M_without = int(((subset[\"Sex\"] == 1) & (subset[\"Has_Diagnostics\"] == 0)).sum())\n",
    "    F_with = int(((subset[\"Sex\"] == 0) & (subset[\"Has_Diagnostics\"] == 1)).sum())\n",
    "    F_without = int(((subset[\"Sex\"] == 0) & (subset[\"Has_Diagnostics\"] == 0)).sum())\n",
    "\n",
    "    group_counts.append({\n",
    "        'M_with': M_with,\n",
    "        'M_without': M_without,\n",
    "        'F_with': F_with,\n",
    "        'F_without': F_without\n",
    "    })\n",
    "\n",
    "ind = np.arange(len(group_order))\n",
    "width = 0.6\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = {'M_without': 'lightblue', 'M_with': 'blue', 'F_without': 'pink', 'F_with': 'deeppink'}\n",
    "\n",
    "bottom_m = np.zeros(len(group_order))\n",
    "bottom_f = np.zeros(len(group_order))\n",
    "\n",
    "m_without_vals = [g['M_without'] for g in group_counts]\n",
    "m_with_vals    = [g['M_with'] for g in group_counts]\n",
    "f_without_vals = [g['F_without'] for g in group_counts]\n",
    "f_with_vals    = [g['F_with'] for g in group_counts]\n",
    "\n",
    "ax.bar(ind, m_without_vals, width, color=colors['M_without'], label='Maschi senza complicanze')\n",
    "bottom_m = np.array(m_without_vals)\n",
    "\n",
    "ax.bar(ind, m_with_vals, width, bottom=bottom_m, color=colors['M_with'], label='Maschi con complicanze')\n",
    "bottom_m += np.array(m_with_vals)\n",
    "\n",
    "ax.bar(ind, f_without_vals, width, bottom=bottom_m, color=colors['F_without'], label='Femmine senza complicanze')\n",
    "bottom_f = bottom_m + np.array(f_without_vals)\n",
    "\n",
    "ax.bar(ind, f_with_vals, width, bottom=bottom_f, color=colors['F_with'], label='Femmine con complicanze')\n",
    "\n",
    "for i in range(len(group_order)):\n",
    "    total = m_without_vals[i] + m_with_vals[i] + f_without_vals[i] + f_with_vals[i]\n",
    "    if total > 0:\n",
    "        ax.text(ind[i], total + 1, str(total), ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_title(\"Distribuzione Maschi/Femmine con/senza complicanze per i 4 Gruppi\")\n",
    "ax.set_xlabel(\"Gruppo (Cluster KMeans, Cluster GMM)\")\n",
    "ax.set_ylabel(\"Numero di pazienti\")\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(group_labels, rotation=15)\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.set_ylim(0, max(group_sizes[\"Size\"]) * 1.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 6.1 Istogramma sesso/complicanze per i 4 cluster singoli (KMeans e GMM) ---\n",
    "\n",
    "cluster_defs = [\n",
    "    (\"KMeans 0\", df_all[df_all[\"Cluster_KMeans\"] == 0]),\n",
    "    (\"KMeans 1\", df_all[df_all[\"Cluster_KMeans\"] == 1]),\n",
    "    (\"GMM 0\", df_all[df_all[\"Cluster_GMM\"] == 0]),\n",
    "    (\"GMM 1\", df_all[df_all[\"Cluster_GMM\"] == 1])\n",
    "]\n",
    "\n",
    "\n",
    "cluster_labels = [label for label, _ in cluster_defs]\n",
    "cluster_counts = []\n",
    "\n",
    "for label, subset in cluster_defs:\n",
    "    M_with = int(((subset[\"Sex\"] == 1) & (subset[\"Has_Diagnostics\"] == 1)).sum())\n",
    "    M_without = int(((subset[\"Sex\"] == 1) & (subset[\"Has_Diagnostics\"] == 0)).sum())\n",
    "    F_with = int(((subset[\"Sex\"] == 0) & (subset[\"Has_Diagnostics\"] == 1)).sum())\n",
    "    F_without = int(((subset[\"Sex\"] == 0) & (subset[\"Has_Diagnostics\"] == 0)).sum())\n",
    "\n",
    "    cluster_counts.append({\n",
    "        'M_with': M_with,\n",
    "        'M_without': M_without,\n",
    "        'F_with': F_with,\n",
    "        'F_without': F_without\n",
    "    })\n",
    "\n",
    "# Grafico\n",
    "ind = np.arange(len(cluster_labels))\n",
    "width = 0.6\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = {'M_without': 'lightblue', 'M_with': 'blue', 'F_without': 'pink', 'F_with': 'deeppink'}\n",
    "\n",
    "m_without_vals = [g['M_without'] for g in cluster_counts]\n",
    "m_with_vals    = [g['M_with'] for g in cluster_counts]\n",
    "f_without_vals = [g['F_without'] for g in cluster_counts]\n",
    "f_with_vals    = [g['F_with'] for g in cluster_counts]\n",
    "\n",
    "bottom_m = np.zeros(len(cluster_labels))\n",
    "ax.bar(ind, m_without_vals, width, color=colors['M_without'], label='Maschi senza complicanze')\n",
    "bottom_m += m_without_vals\n",
    "\n",
    "ax.bar(ind, m_with_vals, width, bottom=bottom_m, color=colors['M_with'], label='Maschi con complicanze')\n",
    "bottom_m += m_with_vals\n",
    "\n",
    "ax.bar(ind, f_without_vals, width, bottom=bottom_m, color=colors['F_without'], label='Femmine senza complicanze')\n",
    "bottom_f = bottom_m + f_without_vals\n",
    "\n",
    "ax.bar(ind, f_with_vals, width, bottom=bottom_f, color=colors['F_with'], label='Femmine con complicanze')\n",
    "\n",
    "# Totali sopra le colonne\n",
    "for i in range(len(cluster_labels)):\n",
    "    total = m_without_vals[i] + m_with_vals[i] + f_without_vals[i] + f_with_vals[i]\n",
    "    if total > 0:\n",
    "        ax.text(ind[i], total + 1, str(total), ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_title(\"Distribuzione Maschi/Femmine con/senza complicanze nei 4 Cluster singoli\")\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Numero di pazienti\")\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(cluster_labels, rotation=15)\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.set_ylim(0, max([m + f + mw + fw for m, mw, f, fw in zip(m_without_vals, m_with_vals, f_without_vals, f_with_vals)]) * 1.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 7. Istogramma %TIR per i 4 Cluster singoli (KMeans e GMM) ---\n",
    "tir_bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "tir_labels = [\n",
    "    \"0%\", \"1–10%\", \"11–20%\", \"21–30%\", \"31–40%\", \"41–50%\",\n",
    "    \"51–60%\", \"61–70%\", \"71–80%\", \"81–90%\", \"91–100%\"\n",
    "]\n",
    "\n",
    "tir_counts_by_cluster = {}\n",
    "for label, subset in cluster_defs:\n",
    "    tir_vals = subset[\"%TIR_last3m\"].dropna()\n",
    "    tir_bins_idx = pd.cut(tir_vals, bins=tir_bins, labels=tir_labels, include_lowest=True, right=False)\n",
    "    tir_counts = tir_bins_idx.value_counts().reindex(tir_labels, fill_value=0)\n",
    "    tir_counts_by_cluster[label] = tir_counts.tolist()\n",
    "\n",
    "# Plot\n",
    "n_bins = len(tir_labels)\n",
    "bar_width = 0.2\n",
    "x = np.arange(n_bins)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#1f77b4', '#aec7e8', '#d62728', '#ff9896']\n",
    "\n",
    "for idx, (label, _) in enumerate(cluster_defs):\n",
    "    counts = tir_counts_by_cluster[label]\n",
    "    bar_x = x + idx * bar_width - (bar_width * (len(cluster_defs) - 1) / 2)\n",
    "    ax.bar(bar_x, counts, width=bar_width, label=label, color=colors[idx])\n",
    "\n",
    "ax.set_title(\"Distribuzione %TIR_last3m per ciascun Cluster (KMeans e GMM)\")\n",
    "ax.set_xlabel(\"Fasce %TIR\")\n",
    "ax.set_ylabel(\"Numero di pazienti\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tir_labels)\n",
    "ax.legend(title=\"Cluster\", fontsize=9)\n",
    "ax.set_ylim(0, max(sum(zip(*tir_counts_by_cluster.values()), ())) * 1.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 6.2 Istogramma %TIR per i 4 gruppi combinati ---\n",
    "tir_counts_by_group = {label: [0]*len(tir_labels) for label in group_labels}\n",
    "\n",
    "for (k_label, g_label), label in zip(group_order, group_labels):\n",
    "    subset = df_all[(df_all[\"Cluster_KMeans\"] == k_label) & (df_all[\"Cluster_GMM\"] == g_label)]\n",
    "    tir_vals = subset[\"%TIR_last3m\"].dropna()\n",
    "    tir_bins_idx = pd.cut(tir_vals, bins=tir_bins, labels=tir_labels, include_lowest=True, right=False)\n",
    "    tir_counts = tir_bins_idx.value_counts().reindex(tir_labels, fill_value=0)\n",
    "    tir_counts_by_group[label] = tir_counts.tolist()\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.2\n",
    "x = np.arange(n_bins)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for idx, label in enumerate(group_labels):\n",
    "    counts = tir_counts_by_group[label]\n",
    "    bar_x = x + idx * bar_width - (bar_width * (len(group_labels) - 1) / 2)\n",
    "    ax.bar(bar_x, counts, width=bar_width, label=label, color=colors[idx])\n",
    "\n",
    "ax.set_title(\"Distribuzione %TIR_last3m nei 4 gruppi (KMeans-GMM)\")\n",
    "ax.set_xlabel(\"Fasce %TIR\")\n",
    "ax.set_ylabel(\"Numero di pazienti\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tir_labels)\n",
    "ax.legend(title=\"Gruppo\", fontsize=9)\n",
    "ax.set_ylim(0, max(sum(zip(*tir_counts_by_group.values()), ())) * 1.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 6.3 Istogramma %TAR per i 4 Cluster singoli (KMeans e GMM) ---\n",
    "tir_bins = [-0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "tir_labels = [\n",
    "    \"0%\", \"1–10%\", \"11–20%\", \"21–30%\", \"31–40%\", \"41–50%\",\n",
    "    \"51–60%\", \"61–70%\", \"71–80%\", \"81–90%\", \"91–100%\"\n",
    "]\n",
    "\n",
    "tir_counts_by_cluster = {}\n",
    "for label, subset in cluster_defs:\n",
    "    tir_vals = subset[\"%TARLV2_last3m\"].dropna()\n",
    "    tir_bins_idx = pd.cut(tir_vals, bins=tir_bins, labels=tir_labels, include_lowest=True, right=False)\n",
    "    tir_counts = tir_bins_idx.value_counts().reindex(tir_labels, fill_value=0)\n",
    "    tir_counts_by_cluster[label] = tir_counts.tolist()\n",
    "\n",
    "# Plot\n",
    "n_bins = len(tir_labels)\n",
    "bar_width = 0.2\n",
    "x = np.arange(n_bins)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#1f77b4', '#aec7e8', '#d62728', '#ff9896']\n",
    "\n",
    "for idx, (label, _) in enumerate(cluster_defs):\n",
    "    counts = tir_counts_by_cluster[label]\n",
    "    bar_x = x + idx * bar_width - (bar_width * (len(cluster_defs) - 1) / 2)\n",
    "    ax.bar(bar_x, counts, width=bar_width, label=label, color=colors[idx])\n",
    "\n",
    "ax.set_title(\"Distribuzione %TARLV2_last3m per ciascun Cluster (KMeans e GMM)\")\n",
    "ax.set_xlabel(\"Fasce %TARLV2\")\n",
    "ax.set_ylabel(\"Numero di pazienti\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tir_labels)\n",
    "ax.legend(title=\"Cluster\", fontsize=9)\n",
    "ax.set_ylim(0, max(sum(zip(*tir_counts_by_cluster.values()), ())) * 1.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 6.3 Istogramma %TAR per i 4 gruppi combinati ---\n",
    "tir_counts_by_group = {label: [0]*len(tir_labels) for label in group_labels}\n",
    "\n",
    "for (k_label, g_label), label in zip(group_order, group_labels):\n",
    "    subset = df_all[(df_all[\"Cluster_KMeans\"] == k_label) & (df_all[\"Cluster_GMM\"] == g_label)]\n",
    "    tir_vals = subset[\"%TARLV2_last3m\"].dropna()\n",
    "    tir_bins_idx = pd.cut(tir_vals, bins=tir_bins, labels=tir_labels, include_lowest=True, right=False)\n",
    "    tir_counts = tir_bins_idx.value_counts().reindex(tir_labels, fill_value=0)\n",
    "    tir_counts_by_group[label] = tir_counts.tolist()\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.2\n",
    "x = np.arange(n_bins)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for idx, label in enumerate(group_labels):\n",
    "    counts = tir_counts_by_group[label]\n",
    "    bar_x = x + idx * bar_width - (bar_width * (len(group_labels) - 1) / 2)\n",
    "    ax.bar(bar_x, counts, width=bar_width, label=label, color=colors[idx])\n",
    "\n",
    "ax.set_title(\"Distribuzione %TARLV2_last3m nei 4 gruppi (KMeans-GMM)\")\n",
    "ax.set_xlabel(\"Fasce %TARLV2\")\n",
    "ax.set_ylabel(\"Numero di pazienti\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tir_labels)\n",
    "ax.legend(title=\"Gruppo\", fontsize=9)\n",
    "ax.set_ylim(0, max(sum(zip(*tir_counts_by_group.values()), ())) * 1.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 6.4 Analisi complicanze per i gruppi K=0,G=0 e K=1,G=1 ---\n",
    "\n",
    "# Seleziona i due gruppi\n",
    "group_a = df_all[(df_all[\"Cluster_KMeans\"] == 0) & (df_all[\"Cluster_GMM\"] == 0)]\n",
    "group_b = df_all[(df_all[\"Cluster_KMeans\"] == 1) & (df_all[\"Cluster_GMM\"] == 1)]\n",
    "\n",
    "# Filtra i record delle complicanze per ciascun gruppo\n",
    "group_a_diag = df_diag[df_diag[\"Patient_ID\"].isin(group_a[\"Patient_ID\"])]\n",
    "group_b_diag = df_diag[df_diag[\"Patient_ID\"].isin(group_b[\"Patient_ID\"])]\n",
    "\n",
    "# Conta le complicanze per ciascun gruppo\n",
    "top_a = group_a_diag[\"Description\"].value_counts().nlargest(15)\n",
    "top_b = group_b_diag[\"Description\"].value_counts().nlargest(15)\n",
    "\n",
    "# Plot gruppo A\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_a[::-1].plot(kind=\"barh\", color=\"steelblue\")\n",
    "plt.title(\"Top 15 complicanze nel gruppo K=0, G=0\")\n",
    "plt.xlabel(\"Numero di pazienti\")\n",
    "plt.ylabel(\"Complicanza\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot gruppo B\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_b[::-1].plot(kind=\"barh\", color=\"salmon\")\n",
    "plt.title(\"Top 15 complicanze nel gruppo K=1, G=1\")\n",
    "plt.xlabel(\"Numero di pazienti\")\n",
    "plt.ylabel(\"Complicanza\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 6.5 Istogramma fasce di età per i gruppi K=0,G=0 e K=1,G=1 ---\n",
    "\n",
    "# Usa 'group_a' e 'group_b' già definiti in /8. Analisi complicanze/\n",
    "# group_a = df_all[(Cluster_KMeans==0)&(Cluster_GMM==0)]\n",
    "# group_b = df_all[(Cluster_KMeans==1)&(Cluster_GMM==1)]\n",
    "\n",
    "# Definisci le fasce di età\n",
    "age_bins = [0, 30, 40, 50, 60, 70, np.inf]\n",
    "age_labels = [\"<30\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \">=70\"]\n",
    "\n",
    "\n",
    "# Assegna la fascia di età a ciascun paziente\n",
    "df_all[\"Age_bin\"] = pd.cut(df_all[\"Age\"], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Conta quanti pazienti per fascia nei due gruppi\n",
    "counts_a = df_all[\n",
    "    (df_all[\"Cluster_KMeans\"] == 0) & (df_all[\"Cluster_GMM\"] == 0)\n",
    "][\"Age_bin\"].value_counts().reindex(age_labels, fill_value=0)\n",
    "\n",
    "counts_b = df_all[\n",
    "    (df_all[\"Cluster_KMeans\"] == 1) & (df_all[\"Cluster_GMM\"] == 1)\n",
    "][\"Age_bin\"].value_counts().reindex(age_labels, fill_value=0)\n",
    "\n",
    "# Prepara i dati per il plot\n",
    "x = np.arange(len(age_labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bars_a = ax.bar(x - width/2, counts_a, width, label=\"K=0, G=0\", color=\"#1f77b4\")\n",
    "bars_b = ax.bar(x + width/2, counts_b, width, label=\"K=1, G=1\", color=\"#ff7f0e\")\n",
    "\n",
    "# Etichette sopra ogni barra\n",
    "for bar in bars_a:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.5, str(int(height)),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "for bar in bars_b:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.5, str(int(height)),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "ax.set_title(\"Distribuzione per fascia di età nei gruppi K=0,G=0 vs K=1,G=1\")\n",
    "ax.set_xlabel(\"Fasce di età\")\n",
    "ax.set_ylabel(\"Numero di pazienti\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(age_labels)\n",
    "ax.legend(title=\"Gruppo\")\n",
    "ax.set_ylim(0, max(counts_a.max(), counts_b.max()) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "id": "b79cdf6e6cdc788c"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
