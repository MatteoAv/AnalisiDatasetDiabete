{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e2e3211ada0d75b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inizializzazione variabili"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a8d365ed61cd250"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Percorso dove salvare l'excel\n",
    "excel_path = \"Excel/RandomForest_Results.xlsx\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T07:06:25.130247200Z",
     "start_time": "2025-06-06T07:06:25.112060Z"
    }
   },
   "id": "99f98a9cbe832858",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Genera il dataset\n",
    "parte1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "parte2 = pd.read_csv(\"Excel/parte2_v1.csv\")\n",
    "\n",
    "# Viene rinominata la feature ID in Patiene_ID per perettere il merge\n",
    "parte2 = parte2.rename(columns={\"ID\": \"Patient_ID\"})\n",
    "\n",
    "# Viene fatto un inner‐merge su Patient_ID:\n",
    "df = parte1.merge(parte2, on=\"Patient_ID\", how=\"inner\")\n",
    "df = df.drop(columns=[\"Patient_ID\"])\n",
    "\n",
    "# Dividi X e Y\n",
    "Y = df['Has_Diagnostics']\n",
    "X = df.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:10:55.670453400Z",
     "start_time": "2025-06-01T20:10:55.641142Z"
    }
   },
   "id": "961a27ebcaa5cf07",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Funzione per metriche\n",
    "def get_metrics(y_true, y_predicted):\n",
    "    matrix = confusion_matrix(y_true, y_predicted)\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "    precision = precision_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    f1score = f1_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    return matrix, accuracy, precision, recall, f1score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T07:03:06.159569500Z",
     "start_time": "2025-06-06T07:03:06.141181Z"
    }
   },
   "id": "dc528a44abdd7487",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Riempimento valori null con la media"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e69b2625af8123b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_media = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "# Dividi X e Y\n",
    "del X, Y\n",
    "Y = df_media['Has_Diagnostics']\n",
    "X = df_media.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:10:58.461796200Z",
     "start_time": "2025-06-01T20:10:58.441053600Z"
    }
   },
   "id": "943341e0ee44265",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Riempimento valori null con la mediana"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9ba982d8c798d5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_mediana = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "# Dividi X e Y\n",
    "del X, Y\n",
    "Y = df_mediana['Has_Diagnostics']\n",
    "X = df_mediana.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T10:53:49.180123900Z",
     "start_time": "2025-06-01T10:53:49.144436Z"
    }
   },
   "id": "f84a073b3b1e6f40",
   "execution_count": 193
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Riempimento valori null con KNNImputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e42483d3e8f309c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Rimuovi momentaneamente X e Y\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Crea un imputatore KNN\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Applica l'imputazione solo sulle colonne numeriche\n",
    "df_imputed_array = imputer.fit_transform(df_temp.select_dtypes(include='number'))\n",
    "\n",
    "# Ricrea il DataFrame con le stesse colonne\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=df_temp.select_dtypes(include='number').columns)\n",
    "\n",
    "# Se ci sono colonne non numeriche, le aggiungiamo di nuovo (senza modificarle)\n",
    "for col in df_temp.columns:\n",
    "    if col not in df_imputed.columns:\n",
    "        df_imputed[col] = df_temp[col]\n",
    "\n",
    "# Dividi X e Y\n",
    "del X, Y\n",
    "Y = df_imputed['Has_Diagnostics']\n",
    "X = df_imputed.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:13:55.632533100Z",
     "start_time": "2025-06-01T20:13:55.586953100Z"
    }
   },
   "id": "6338be6e58bcceaa",
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Riempimento valori null con IterativeImputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "333c5326ae40e2e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Rimuovi momentaneamente X e Y\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Crea un imputatore iterativo\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "\n",
    "# Applica l'imputazione solo sulle colonne numeriche\n",
    "df_imputed_array = imputer.fit_transform(df_temp.select_dtypes(include='number'))\n",
    "\n",
    "# Ricrea il DataFrame con le stesse colonne\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=df_temp.select_dtypes(include='number').columns)\n",
    "\n",
    "# Se ci sono colonne non numeriche, le aggiungiamo di nuovo (senza modificarle)\n",
    "for col in df_temp.columns:\n",
    "    if col not in df_imputed.columns:\n",
    "        df_imputed[col] = df_temp[col]\n",
    "\n",
    "# Dividi X e Y\n",
    "del X, Y\n",
    "Y = df_imputed['Has_Diagnostics']\n",
    "X = df_imputed.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T11:09:45.152065800Z",
     "start_time": "2025-06-01T11:09:44.500334700Z"
    }
   },
   "id": "3c1316516dc3cdc5",
   "execution_count": 220
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Prova = 24\n",
    "n_estimators = 300\n",
    "max_depth = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:15:27.799040300Z",
     "start_time": "2025-06-01T20:15:27.759520100Z"
    }
   },
   "id": "123dd65ead26b47d",
   "execution_count": 111
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metodo dell 80/20"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c46d0b5a824caafa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDAZIONE 80/20 ===\n",
      "✅ Risultati 80/20 salvati su Excel.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALIDAZIONE 80/20 ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm, acc, prec, rec, f1 = get_metrics(y_test, y_pred)\n",
    "\n",
    "#print(f\"Accuracy: {acc:.4f}\")\n",
    "#print(f\"Precision: {prec:.4f}\")\n",
    "#print(f\"Recall: {rec:.4f}\")\n",
    "#print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "#disp.plot(cmap=plt.cm.Blues)\n",
    "#plt.title(\"Confusion Matrix 80/20\")\n",
    "#plt.show()\n",
    "\n",
    "# Calcola importanza delle feature\n",
    "importances = clf.feature_importances_\n",
    "features = X.columns\n",
    "importances_dict = {f'Imp_{feat}': [imp] for feat, imp in zip(features, importances)}\n",
    "\n",
    "# Prepara il DataFrame da salvare\n",
    "df_split = pd.DataFrame({\n",
    "    'Prova': [Prova],\n",
    "    'n_estimators': [n_estimators],\n",
    "    'max_depth': [max_depth],\n",
    "    'Accuracy': [acc],\n",
    "    'Precision': [prec],\n",
    "    'Recall': [rec],\n",
    "    'F1-Score': [f1],\n",
    "    'Confusion_Matrix': [cm.tolist()],\n",
    "    ' ': [None]\n",
    "} | importances_dict)  # Unione dizionari\n",
    "\n",
    "# Aggiungi a eventuali dati già presenti\n",
    "if os.path.exists(excel_path):\n",
    "    with pd.ExcelFile(excel_path) as reader:\n",
    "        if 'Split_80_20_Test' in reader.sheet_names:\n",
    "            prev_data = pd.read_excel(reader, sheet_name='Split_80_20_Test')\n",
    "            df_split = pd.concat([prev_data, df_split], ignore_index=True)\n",
    "\n",
    "# Scrivi sul file\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    df_split.to_excel(writer, sheet_name='Split_80_20_Test', index=False)\n",
    "\n",
    "print(\"✅ Risultati 80/20 salvati su Excel.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:15:32.533976200Z",
     "start_time": "2025-06-01T20:15:29.522535600Z"
    }
   },
   "id": "8ddee73f823b18d3",
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Fold cross recognition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e4bc7b7d117a911"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDAZIONE K-FOLD ===\n",
      "✅ Risultati K-Fold salvati su Excel.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALIDAZIONE K-FOLD ===\")\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X, Y), start=1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    cm, acc, prec, rec, f1 = get_metrics(y_test, y_pred)\n",
    "\n",
    "#    print(f\"\\n--- Fold {i} ---\")\n",
    "#    print(f\"Accuracy: {acc:.4f}\")\n",
    "#    print(f\"Precision: {prec:.4f}\")\n",
    "#    print(f\"Recall: {rec:.4f}\")\n",
    "#    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "#    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "#    disp.plot(cmap=plt.cm.Blues)\n",
    "#    plt.title(f\"Confusion Matrix Fold {i}\")\n",
    "#    plt.show()\n",
    "\n",
    "    # Calcola importanza delle feature\n",
    "    importances = clf.feature_importances_\n",
    "    features = X.columns\n",
    "    importances_dict = {f'Imp_{feat}': imp for feat, imp in zip(features, importances)}\n",
    "\n",
    "    # Salva tutti i dati in un'unica riga\n",
    "    fold_results.append({\n",
    "        'Prova': Prova,\n",
    "        'KFold' : i,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion_Matrix': cm.tolist(),\n",
    "        ' ': None,  # cella vuota per separare le metriche dalle importances\n",
    "        **importances_dict\n",
    "    })\n",
    "\n",
    "df_kfold = pd.DataFrame(fold_results)\n",
    "\n",
    "# Aggiungi a eventuali dati già presenti\n",
    "if os.path.exists(excel_path):\n",
    "    with pd.ExcelFile(excel_path) as reader:\n",
    "        if 'KFold_CV_Test' in reader.sheet_names:\n",
    "            prev_kfold = pd.read_excel(reader, sheet_name='KFold_CV_Test')\n",
    "            df_kfold = pd.concat([prev_kfold, df_kfold], ignore_index=True)\n",
    "\n",
    "# Scrivi sul file\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    df_kfold.to_excel(writer, sheet_name='KFold_CV_Test', index=False)\n",
    "\n",
    "print(\"✅ Risultati K-Fold salvati su Excel.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:15:39.601906200Z",
     "start_time": "2025-06-01T20:15:32.517992500Z"
    }
   },
   "id": "f2336578911cf3fc",
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN-FOLD / TEST-HOLDOUT validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55e30b4b94d16471"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDAZIONE TRAIN-FOLD / TEST-HOLDOUT ===\n",
      "✅ Risultati nested fold salvati su Excel.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALIDAZIONE TRAIN-FOLD / TEST-HOLDOUT ===\")\n",
    "val_results = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train = X_train.iloc[train_idx]\n",
    "    y_fold_train = y_train.iloc[train_idx]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "    # Si testa sempre sullo stesso 20%\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    cm, acc, prec, rec, f1 = get_metrics(y_test, y_pred)\n",
    "\n",
    "#    print(f\"\\n--- Fold {i} (train su fold, test fisso) ---\")\n",
    "#    print(f\"Accuracy: {acc:.4f}\")\n",
    "#    print(f\"Precision: {prec:.4f}\")\n",
    "#    print(f\"Recall: {rec:.4f}\")\n",
    "#    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "#    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "#    disp.plot(cmap=plt.cm.Blues)\n",
    "#    plt.title(f\"Confusion Matrix Fold {i}\")\n",
    "#    plt.show()\n",
    "\n",
    "    # Feature importances\n",
    "    importances = clf.feature_importances_\n",
    "    importances_dict = {f'Imp_{feat}': imp for feat, imp in zip(X.columns, importances)}\n",
    "\n",
    "    # Salva i risultati del fold\n",
    "    val_results.append({\n",
    "        'Prova': Prova,\n",
    "        'KFold' : i,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion_Matrix': cm.tolist(),\n",
    "        ' ': None,  # cella vuota per separare le metriche dalle importances\n",
    "        **importances_dict\n",
    "    })\n",
    "\n",
    "# Crea il DataFrame\n",
    "df_nested = pd.DataFrame(val_results)\n",
    "\n",
    "# Se esiste già, aggiungi i dati al foglio\n",
    "if os.path.exists(excel_path):\n",
    "    with pd.ExcelFile(excel_path) as reader:\n",
    "        if 'TrainFold_TestFixed_Test' in reader.sheet_names:\n",
    "            prev_nested = pd.read_excel(reader, sheet_name='TrainFold_TestFixed_Test')\n",
    "            df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n",
    "            \n",
    "# Salva su Excel\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    df_nested.to_excel(writer, sheet_name='TrainFold_TestFixed_Test', index=False)\n",
    "\n",
    "print(\"✅ Risultati nested fold salvati su Excel.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:15:45.743231600Z",
     "start_time": "2025-06-01T20:15:39.605578300Z"
    }
   },
   "id": "58aa474531f64e1e",
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creazione dataset test per complicanze"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "144f3d6e4cfb0b8d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero pazienti senza diagnosi: 225\n",
      "Suddivisione completata:\n",
      "  - Train (Classification_train.csv): 581 righe\n",
      "  - Test  (Classification_test.csv):  142  righe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Vengono letti i due file\n",
    "parte1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "parte2 = pd.read_csv(\"Excel/parte2_v1.csv\")\n",
    "\n",
    "# 1.1) Viene rinominata la feature ID in Patiene_ID per perettere il merge\n",
    "parte2 = parte2.rename(columns={\"ID\": \"Patient_ID\"})\n",
    "\n",
    "# 1.2) Viene fatto un inner‐merge su Patient_ID:\n",
    "clust = parte1.merge(parte2, on=\"Patient_ID\", how=\"inner\")\n",
    "clust = clust.fillna(clust.mean(numeric_only=True))\n",
    "\n",
    "# 2) Viene letto Diagnostics.csv per ricavare i codici associati a ciascun paziente\n",
    "diag = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "# 2.1) Per ogni paziente, viene prenso il Code con il maggior numero di occorrenze\n",
    "diag_unique = (\n",
    "    diag\n",
    "    .groupby(\"Patient_ID\")[\"Code\"]\n",
    "    .agg(lambda codes: codes.value_counts().idxmax())\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Code\": \"CodeStrat\"})\n",
    ")\n",
    "\n",
    "# 2.2) Se hai codici troppo rari vengono raggruppati sotto \"RARE\" per evitare errori di stratify\n",
    "code_counts = diag_unique[\"CodeStrat\"].value_counts()\n",
    "rare_codes = set(code_counts[code_counts < 2].index)\n",
    "diag_unique[\"CodeStrat\"] = diag_unique[\"CodeStrat\"].apply(\n",
    "    lambda c: \"RARE\" if c in rare_codes else c\n",
    ")\n",
    "\n",
    "# 3) Suddividi i soli pazienti “diagnosticati” 80/20, stratificando su CodeStrat\n",
    "diag_train_patients, diag_test_patients = train_test_split(\n",
    "    diag_unique[\"Patient_ID\"],\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=diag_unique[\"CodeStrat\"]\n",
    ")\n",
    "\n",
    "# 4) Trova i pazienti in clust che NON compaiono in diag_unique (ossia “senza diagnosi”)\n",
    "all_clust_patients = set(clust[\"Patient_ID\"].unique())\n",
    "diag_patients = set(diag_unique[\"Patient_ID\"].unique())\n",
    "no_diag_patients = list(all_clust_patients - diag_patients)\n",
    "print(f\"Numero pazienti senza diagnosi: {len(no_diag_patients)}\")\n",
    "\n",
    "# 5) Dividi i pazienti “senza diagnosi” (%) 80/20 in modo casuale\n",
    "no_diag_train, no_diag_test = train_test_split(\n",
    "    no_diag_patients,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) Costruisci la lista finale di test (diagnosticati nel 20% + senza diagnosi nel 20%)\n",
    "test_patients  = set(diag_test_patients)  | set(no_diag_test)\n",
    "train_patients = set(diag_train_patients) | set(no_diag_train)\n",
    "\n",
    "# Verifica che non ci siano sovrapposizioni\n",
    "assert train_patients.isdisjoint(test_patients), \"Errore: un paziente è in entrambi i set!\"\n",
    "\n",
    "# 7) Filtra clust in base a Patient_ID: \n",
    "#    - se Patient_ID ∈ test_patients → va in df_test\n",
    "#    - altrimenti va in df_train\n",
    "df_train = clust[~clust[\"Patient_ID\"].isin(test_patients)].copy()\n",
    "df_test  = clust[ clust[\"Patient_ID\"].isin(test_patients)].copy()\n",
    "\n",
    "# 8) Rimuovi di nuovo la colonna Patient_ID, come richiesto, prima di salvare\n",
    "df_train_final = df_train.drop(columns=[\"Patient_ID\", \"Complicanze\"])\n",
    "df_test_final  = df_test.drop(columns=[\"Complicanze\"])\n",
    "\n",
    "# 9) Salva i due file risultanti\n",
    "df_train_final.to_csv(\"Excel/Classification_train.csv\", index=False)\n",
    "df_test_final.to_csv(\"Excel/Classification_test.csv\",  index=False)\n",
    "\n",
    "print(\"Suddivisione completata:\")\n",
    "print(f\"  - Train (Classification_train.csv): {len(df_train_final)} righe\")\n",
    "print(f\"  - Test  (Classification_test.csv):  {len(df_test_final)}  righe\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T07:26:24.770907300Z",
     "start_time": "2025-06-06T07:26:24.454129400Z"
    }
   },
   "id": "9923dd9978c0932b",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Greedy Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d758d80e4ab35381"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Prova 1: n_estimators=50, max_depth=5\n",
      "✅ Nuova configurazione migliore trovata! F1 medio = 0.5979\n",
      "📁 Risultati prova 1 salvati.\n",
      "▶️ Prova 2: n_estimators=50, max_depth=10\n",
      "✅ Nuova configurazione migliore trovata! F1 medio = 0.6102\n",
      "📁 Risultati prova 2 salvati.\n",
      "▶️ Prova 3: n_estimators=50, max_depth=15\n",
      "📁 Risultati prova 3 salvati.\n",
      "▶️ Prova 4: n_estimators=50, max_depth=20\n",
      "✅ Nuova configurazione migliore trovata! F1 medio = 0.6144\n",
      "📁 Risultati prova 4 salvati.\n",
      "▶️ Prova 5: n_estimators=50, max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabry\\AppData\\Local\\Temp\\ipykernel_28156\\2121736144.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Risultati prova 5 salvati.\n",
      "▶️ Prova 6: n_estimators=100, max_depth=5\n",
      "📁 Risultati prova 6 salvati.\n",
      "▶️ Prova 7: n_estimators=100, max_depth=10\n",
      "📁 Risultati prova 7 salvati.\n",
      "▶️ Prova 8: n_estimators=100, max_depth=15\n",
      "✅ Nuova configurazione migliore trovata! F1 medio = 0.6190\n",
      "📁 Risultati prova 8 salvati.\n",
      "▶️ Prova 9: n_estimators=100, max_depth=20\n",
      "📁 Risultati prova 9 salvati.\n",
      "▶️ Prova 10: n_estimators=100, max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabry\\AppData\\Local\\Temp\\ipykernel_28156\\2121736144.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Risultati prova 10 salvati.\n",
      "▶️ Prova 11: n_estimators=150, max_depth=5\n",
      "📁 Risultati prova 11 salvati.\n",
      "▶️ Prova 12: n_estimators=150, max_depth=10\n",
      "📁 Risultati prova 12 salvati.\n",
      "▶️ Prova 13: n_estimators=150, max_depth=15\n",
      "📁 Risultati prova 13 salvati.\n",
      "▶️ Prova 14: n_estimators=150, max_depth=20\n",
      "📁 Risultati prova 14 salvati.\n",
      "▶️ Prova 15: n_estimators=150, max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabry\\AppData\\Local\\Temp\\ipykernel_28156\\2121736144.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Risultati prova 15 salvati.\n",
      "▶️ Prova 16: n_estimators=200, max_depth=5\n",
      "📁 Risultati prova 16 salvati.\n",
      "▶️ Prova 17: n_estimators=200, max_depth=10\n",
      "📁 Risultati prova 17 salvati.\n",
      "▶️ Prova 18: n_estimators=200, max_depth=15\n",
      "📁 Risultati prova 18 salvati.\n",
      "▶️ Prova 19: n_estimators=200, max_depth=20\n",
      "📁 Risultati prova 19 salvati.\n",
      "▶️ Prova 20: n_estimators=200, max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabry\\AppData\\Local\\Temp\\ipykernel_28156\\2121736144.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Risultati prova 20 salvati.\n",
      "▶️ Prova 21: n_estimators=250, max_depth=5\n",
      "📁 Risultati prova 21 salvati.\n",
      "▶️ Prova 22: n_estimators=250, max_depth=10\n",
      "📁 Risultati prova 22 salvati.\n",
      "▶️ Prova 23: n_estimators=250, max_depth=15\n",
      "📁 Risultati prova 23 salvati.\n",
      "▶️ Prova 24: n_estimators=250, max_depth=20\n",
      "📁 Risultati prova 24 salvati.\n",
      "▶️ Prova 25: n_estimators=250, max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabry\\AppData\\Local\\Temp\\ipykernel_28156\\2121736144.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Risultati prova 25 salvati.\n",
      "▶️ Prova 26: n_estimators=300, max_depth=5\n",
      "📁 Risultati prova 26 salvati.\n",
      "▶️ Prova 27: n_estimators=300, max_depth=10\n",
      "📁 Risultati prova 27 salvati.\n",
      "▶️ Prova 28: n_estimators=300, max_depth=15\n",
      "📁 Risultati prova 28 salvati.\n",
      "▶️ Prova 29: n_estimators=300, max_depth=20\n",
      "📁 Risultati prova 29 salvati.\n",
      "▶️ Prova 30: n_estimators=300, max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabry\\AppData\\Local\\Temp\\ipykernel_28156\\2121736144.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Risultati prova 30 salvati.\n",
      "▶️ Prova 31: n_estimators=350, max_depth=5\n",
      "📁 Risultati prova 31 salvati.\n",
      "▶️ Prova 32: n_estimators=350, max_depth=10\n",
      "📁 Risultati prova 32 salvati.\n",
      "▶️ Prova 33: n_estimators=350, max_depth=15\n",
      "📁 Risultati prova 33 salvati.\n",
      "▶️ Prova 34: n_estimators=350, max_depth=20\n",
      "📁 Risultati prova 34 salvati.\n",
      "▶️ Prova 35: n_estimators=350, max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabry\\AppData\\Local\\Temp\\ipykernel_28156\\2121736144.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Risultati prova 35 salvati.\n",
      "▶️ Prova 36: n_estimators=400, max_depth=5\n",
      "📁 Risultati prova 36 salvati.\n",
      "▶️ Prova 37: n_estimators=400, max_depth=10\n",
      "📁 Risultati prova 37 salvati.\n",
      "▶️ Prova 38: n_estimators=400, max_depth=15\n",
      "📁 Risultati prova 38 salvati.\n",
      "▶️ Prova 39: n_estimators=400, max_depth=20\n",
      "📁 Risultati prova 39 salvati.\n",
      "▶️ Prova 40: n_estimators=400, max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabry\\AppData\\Local\\Temp\\ipykernel_28156\\2121736144.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Risultati prova 40 salvati.\n",
      "\n",
      "🏁 Greedy search terminata.\n",
      "⭐ Migliore configurazione: n_estimators=100, max_depth=15, con F1 medio = 0.6190\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"Excel/Classification_train.csv\")\n",
    "test = pd.read_csv(\"Excel/Classification_test.csv\")\n",
    "\n",
    "test_patients1 = test['Patient_ID']\n",
    "test = test.drop(columns=['Patient_ID'])\n",
    "\n",
    "y_train = train['Has_Diagnostics']\n",
    "y_test = test['Has_Diagnostics']\n",
    "X_train = train.drop(columns=['Has_Diagnostics'])\n",
    "X_test = test.drop(columns=['Has_Diagnostics'])\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Definisci lo spazio di ricerca\n",
    "n_estimators_list = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "max_depth_list = [5, 10, 15, 20]\n",
    "\n",
    "best_config = None\n",
    "best_score = 0  \n",
    "\n",
    "prova_counter = 1\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        print(f\"▶️ Prova {prova_counter}: n_estimators={n_estimators}, max_depth={max_depth}\")\n",
    "\n",
    "        Prova = f\"{prova_counter} - Est:{n_estimators}_Depth:{max_depth}\"\n",
    "        \n",
    "        val_results = []\n",
    "\n",
    "        for i, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "            X_fold_train = X_train.iloc[train_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx]\n",
    "\n",
    "            clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "            clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            cm, acc, prec, rec, f1 = get_metrics(y_test, y_pred)\n",
    "            importances = clf.feature_importances_\n",
    "            importances_dict = {f'Imp_{feat}': imp for feat, imp in zip(X_train.columns, importances)}\n",
    "\n",
    "            val_results.append({\n",
    "                'Prova': Prova,\n",
    "                'KFold': i,\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1-Score': f1,\n",
    "                'Confusion_Matrix': cm.tolist(),\n",
    "                ' ': None,\n",
    "                **importances_dict\n",
    "            })\n",
    "\n",
    "        df_nested = pd.DataFrame(val_results)\n",
    "\n",
    "        # Calcola media F1\n",
    "        mean_f1 = df_nested['F1-Score'].mean()\n",
    "\n",
    "        # Aggiorna il best\n",
    "        if mean_f1 > best_score:\n",
    "            best_score = mean_f1\n",
    "            best_config = (n_estimators, max_depth)\n",
    "            print(f\"✅ Nuova configurazione migliore trovata! F1 medio = {mean_f1:.4f}\")\n",
    "\n",
    "        # Salva su Excel\n",
    "        if os.path.exists(excel_path):\n",
    "            with pd.ExcelFile(excel_path) as reader:\n",
    "                if 'Final' in reader.sheet_names:\n",
    "                    prev_nested = pd.read_excel(reader, sheet_name='Final')\n",
    "                    df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n",
    "\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "            df_nested.to_excel(writer, sheet_name='Final', index=False)\n",
    "\n",
    "        print(f\"📁 Risultati prova {prova_counter} salvati.\")\n",
    "        prova_counter += 1\n",
    "\n",
    "print(f\"\\n🏁 Greedy search terminata.\")\n",
    "print(f\"⭐ Migliore configurazione: n_estimators={best_config[0]}, max_depth={best_config[1]}, con F1 medio = {best_score:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T07:37:05.340816600Z",
     "start_time": "2025-06-06T07:33:08.372525300Z"
    }
   },
   "id": "5acf103d1dc4fc2c",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Analisi degli errori sulla configurazione migliore...\n",
      "❌ Trovati 27 errori su 142 pazienti nel test set.\n",
      "Errori salvati sul foglio 'Error_Analysis'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔎 Analisi degli errori sulla configurazione migliore...\")\n",
    "\n",
    "# Ricrea il classificatore con i parametri migliori\n",
    "n_estimators_best, max_depth_best = best_config\n",
    "clf_best = RandomForestClassifier(n_estimators=n_estimators_best, max_depth=max_depth_best, random_state=42)\n",
    "clf_best.fit(X_train, y_train)  # Allena sul full train\n",
    "\n",
    "# Predizione su X_test\n",
    "y_pred_best = clf_best.predict(X_test)\n",
    "\n",
    "# Crea DataFrame con gli errori\n",
    "df_resoults = pd.DataFrame({\n",
    "    'Patient_ID': test_patients1,\n",
    "    'True_Label': y_test,\n",
    "    'Predicted_Label': y_pred_best\n",
    "})\n",
    "df_resoults['Errore'] = df_resoults['True_Label'] != df_resoults['Predicted_Label']\n",
    "\n",
    "# Filtra solo gli errori\n",
    "df_errors_only = df_resoults[df_resoults['Errore'] == True].copy()\n",
    "\n",
    "diagnosi = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "df_errors_only = df_errors_only.merge(diagnosi, on=\"Patient_ID\", how=\"inner\")\n",
    "\n",
    "df_errors_only = df_errors_only.drop(columns=['True_Label', 'Predicted_Label', 'Errore'])\n",
    "\n",
    "df_errors_only = df_errors_only.sort_values(['Code'])\n",
    "\n",
    "print(f\"❌ Trovati {len(df_errors_only)} errori su {len(df_resoults)} pazienti nel test set.\")\n",
    "\n",
    "# Salva su Excel\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "    df_errors_only.to_excel(writer, sheet_name='Error_Analysis', index=False)\n",
    "\n",
    "print(\"Errori salvati sul foglio 'Error_Analysis'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T08:07:44.816311900Z",
     "start_time": "2025-06-06T08:07:42.000564600Z"
    }
   },
   "id": "7df18c5376cbc918",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Patient_ID    Code                                        Description\n23    LIB193276  V58.61          Long-term (current) use of anticoagulants\n25    LIB193277   V24.2                       Routine postpartum follow-up\n26    LIB193277  V67.00  Follow-up examination, following surgery, unsp...\n33    LIB193278  V49.70     Unspecified level lower limb amputation status\n56    LIB193302   V25.2                                      Sterilization\n...         ...     ...                                                ...\n1659  LIB194091  V72.32  Encounter for Papanicolaou cervical smear to c...\n1677  LIB194095  V62.82                         Bereavement, uncomplicated\n1737  LIB194133   V14.0          Personal history of allergy to penicillin\n1738  LIB194133   V15.0            Allergy, other than to medicinal agents\n1739  LIB194133  V43.64                              Hip joint replacement\n\n[74 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_ID</th>\n      <th>Code</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23</th>\n      <td>LIB193276</td>\n      <td>V58.61</td>\n      <td>Long-term (current) use of anticoagulants</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>LIB193277</td>\n      <td>V24.2</td>\n      <td>Routine postpartum follow-up</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>LIB193277</td>\n      <td>V67.00</td>\n      <td>Follow-up examination, following surgery, unsp...</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>LIB193278</td>\n      <td>V49.70</td>\n      <td>Unspecified level lower limb amputation status</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>LIB193302</td>\n      <td>V25.2</td>\n      <td>Sterilization</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1659</th>\n      <td>LIB194091</td>\n      <td>V72.32</td>\n      <td>Encounter for Papanicolaou cervical smear to c...</td>\n    </tr>\n    <tr>\n      <th>1677</th>\n      <td>LIB194095</td>\n      <td>V62.82</td>\n      <td>Bereavement, uncomplicated</td>\n    </tr>\n    <tr>\n      <th>1737</th>\n      <td>LIB194133</td>\n      <td>V14.0</td>\n      <td>Personal history of allergy to penicillin</td>\n    </tr>\n    <tr>\n      <th>1738</th>\n      <td>LIB194133</td>\n      <td>V15.0</td>\n      <td>Allergy, other than to medicinal agents</td>\n    </tr>\n    <tr>\n      <th>1739</th>\n      <td>LIB194133</td>\n      <td>V43.64</td>\n      <td>Hip joint replacement</td>\n    </tr>\n  </tbody>\n</table>\n<p>74 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosi_filtrato = diagnosi[diagnosi[\"Code\"].str.startswith(\"V\")]\n",
    "diagnosi_filtrato"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T08:18:24.548589900Z",
     "start_time": "2025-06-06T08:18:24.534134300Z"
    }
   },
   "id": "da16024ff231f7b6",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1a4a63ac7ba6462f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
