{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e2e3211ada0d75b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inizializzazione variabili"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a8d365ed61cd250"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T17:33:41.583823Z",
     "start_time": "2025-06-01T17:33:40.752519200Z"
    }
   },
   "id": "99f98a9cbe832858",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Percorso dove salvare l'excel\n",
    "excel_path = \"Excel/RandomForest_Results.xlsx\"\n",
    "\n",
    "# Genera il dataset\n",
    "parte1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "parte2 = pd.read_csv(\"Excel/parte2_v4.csv\")\n",
    "\n",
    "# Viene rinominata la feature ID in Patiene_ID per perettere il merge\n",
    "parte2 = parte2.rename(columns={\"ID\": \"Patient_ID\"})\n",
    "\n",
    "# Viene fatto un inner‐merge su Patient_ID:\n",
    "df = parte1.merge(parte2, on=\"Patient_ID\", how=\"inner\")\n",
    "df = df.drop(columns=[\"Patient_ID\"])\n",
    "\n",
    "# Dividi X e Y\n",
    "Y = df['Has_Diagnostics']\n",
    "X = df.drop(columns=['Has_Diagnostics'])\n",
    "\n",
    "# Funzione per metriche\n",
    "def get_metrics(y_true, y_predicted):\n",
    "    matrix = confusion_matrix(y_true, y_predicted)\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "    precision = precision_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    f1score = f1_score(y_true, y_predicted, average='weighted', zero_division=0)\n",
    "    return matrix, accuracy, precision, recall, f1score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:10:55.670453400Z",
     "start_time": "2025-06-01T20:10:55.641142Z"
    }
   },
   "id": "961a27ebcaa5cf07",
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Riempimento valori null con la media"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e69b2625af8123b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_media = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "# Dividi X e Y\n",
    "del X, Y\n",
    "Y = df_media['Has_Diagnostics']\n",
    "X = df_media.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:10:58.461796200Z",
     "start_time": "2025-06-01T20:10:58.441053600Z"
    }
   },
   "id": "943341e0ee44265",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Riempimento valori null con la mediana"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9ba982d8c798d5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_mediana = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "# Dividi X e Y\n",
    "del X, Y\n",
    "Y = df_mediana['Has_Diagnostics']\n",
    "X = df_mediana.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T10:53:49.180123900Z",
     "start_time": "2025-06-01T10:53:49.144436Z"
    }
   },
   "id": "f84a073b3b1e6f40",
   "execution_count": 193
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Riempimento valori null con KNNImputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e42483d3e8f309c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Rimuovi momentaneamente X e Y\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Crea un imputatore KNN\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Applica l'imputazione solo sulle colonne numeriche\n",
    "df_imputed_array = imputer.fit_transform(df_temp.select_dtypes(include='number'))\n",
    "\n",
    "# Ricrea il DataFrame con le stesse colonne\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=df_temp.select_dtypes(include='number').columns)\n",
    "\n",
    "# Se ci sono colonne non numeriche, le aggiungiamo di nuovo (senza modificarle)\n",
    "for col in df_temp.columns:\n",
    "    if col not in df_imputed.columns:\n",
    "        df_imputed[col] = df_temp[col]\n",
    "\n",
    "# Dividi X e Y\n",
    "del X, Y\n",
    "Y = df_imputed['Has_Diagnostics']\n",
    "X = df_imputed.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:13:55.632533100Z",
     "start_time": "2025-06-01T20:13:55.586953100Z"
    }
   },
   "id": "6338be6e58bcceaa",
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Riempimento valori null con IterativeImputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "333c5326ae40e2e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Rimuovi momentaneamente X e Y\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Crea un imputatore iterativo\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "\n",
    "# Applica l'imputazione solo sulle colonne numeriche\n",
    "df_imputed_array = imputer.fit_transform(df_temp.select_dtypes(include='number'))\n",
    "\n",
    "# Ricrea il DataFrame con le stesse colonne\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=df_temp.select_dtypes(include='number').columns)\n",
    "\n",
    "# Se ci sono colonne non numeriche, le aggiungiamo di nuovo (senza modificarle)\n",
    "for col in df_temp.columns:\n",
    "    if col not in df_imputed.columns:\n",
    "        df_imputed[col] = df_temp[col]\n",
    "\n",
    "# Dividi X e Y\n",
    "del X, Y\n",
    "Y = df_imputed['Has_Diagnostics']\n",
    "X = df_imputed.drop(columns=['Has_Diagnostics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T11:09:45.152065800Z",
     "start_time": "2025-06-01T11:09:44.500334700Z"
    }
   },
   "id": "3c1316516dc3cdc5",
   "execution_count": 220
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Prova = 24\n",
    "n_estimators = 300\n",
    "max_depth = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:15:27.799040300Z",
     "start_time": "2025-06-01T20:15:27.759520100Z"
    }
   },
   "id": "123dd65ead26b47d",
   "execution_count": 111
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metodo dell 80/20"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c46d0b5a824caafa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDAZIONE 80/20 ===\n",
      "✅ Risultati 80/20 salvati su Excel.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALIDAZIONE 80/20 ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm, acc, prec, rec, f1 = get_metrics(y_test, y_pred)\n",
    "\n",
    "#print(f\"Accuracy: {acc:.4f}\")\n",
    "#print(f\"Precision: {prec:.4f}\")\n",
    "#print(f\"Recall: {rec:.4f}\")\n",
    "#print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "#disp.plot(cmap=plt.cm.Blues)\n",
    "#plt.title(\"Confusion Matrix 80/20\")\n",
    "#plt.show()\n",
    "\n",
    "# Calcola importanza delle feature\n",
    "importances = clf.feature_importances_\n",
    "features = X.columns\n",
    "importances_dict = {f'Imp_{feat}': [imp] for feat, imp in zip(features, importances)}\n",
    "\n",
    "# Prepara il DataFrame da salvare\n",
    "df_split = pd.DataFrame({\n",
    "    'Prova': [Prova],\n",
    "    'n_estimators': [n_estimators],\n",
    "    'max_depth': [max_depth],\n",
    "    'Accuracy': [acc],\n",
    "    'Precision': [prec],\n",
    "    'Recall': [rec],\n",
    "    'F1-Score': [f1],\n",
    "    'Confusion_Matrix': [cm.tolist()],\n",
    "    ' ': [None]\n",
    "} | importances_dict)  # Unione dizionari\n",
    "\n",
    "# Aggiungi a eventuali dati già presenti\n",
    "if os.path.exists(excel_path):\n",
    "    with pd.ExcelFile(excel_path) as reader:\n",
    "        if 'Split_80_20_Test' in reader.sheet_names:\n",
    "            prev_data = pd.read_excel(reader, sheet_name='Split_80_20_Test')\n",
    "            df_split = pd.concat([prev_data, df_split], ignore_index=True)\n",
    "\n",
    "# Scrivi sul file\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    df_split.to_excel(writer, sheet_name='Split_80_20_Test', index=False)\n",
    "\n",
    "print(\"✅ Risultati 80/20 salvati su Excel.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:15:32.533976200Z",
     "start_time": "2025-06-01T20:15:29.522535600Z"
    }
   },
   "id": "8ddee73f823b18d3",
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Fold cross recognition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e4bc7b7d117a911"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDAZIONE K-FOLD ===\n",
      "✅ Risultati K-Fold salvati su Excel.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALIDAZIONE K-FOLD ===\")\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X, Y), start=1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    cm, acc, prec, rec, f1 = get_metrics(y_test, y_pred)\n",
    "\n",
    "#    print(f\"\\n--- Fold {i} ---\")\n",
    "#    print(f\"Accuracy: {acc:.4f}\")\n",
    "#    print(f\"Precision: {prec:.4f}\")\n",
    "#    print(f\"Recall: {rec:.4f}\")\n",
    "#    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "#    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "#    disp.plot(cmap=plt.cm.Blues)\n",
    "#    plt.title(f\"Confusion Matrix Fold {i}\")\n",
    "#    plt.show()\n",
    "\n",
    "    # Calcola importanza delle feature\n",
    "    importances = clf.feature_importances_\n",
    "    features = X.columns\n",
    "    importances_dict = {f'Imp_{feat}': imp for feat, imp in zip(features, importances)}\n",
    "\n",
    "    # Salva tutti i dati in un'unica riga\n",
    "    fold_results.append({\n",
    "        'Prova': Prova,\n",
    "        'KFold' : i,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion_Matrix': cm.tolist(),\n",
    "        ' ': None,  # cella vuota per separare le metriche dalle importances\n",
    "        **importances_dict\n",
    "    })\n",
    "\n",
    "df_kfold = pd.DataFrame(fold_results)\n",
    "\n",
    "# Aggiungi a eventuali dati già presenti\n",
    "if os.path.exists(excel_path):\n",
    "    with pd.ExcelFile(excel_path) as reader:\n",
    "        if 'KFold_CV_Test' in reader.sheet_names:\n",
    "            prev_kfold = pd.read_excel(reader, sheet_name='KFold_CV_Test')\n",
    "            df_kfold = pd.concat([prev_kfold, df_kfold], ignore_index=True)\n",
    "\n",
    "# Scrivi sul file\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    df_kfold.to_excel(writer, sheet_name='KFold_CV_Test', index=False)\n",
    "\n",
    "print(\"✅ Risultati K-Fold salvati su Excel.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:15:39.601906200Z",
     "start_time": "2025-06-01T20:15:32.517992500Z"
    }
   },
   "id": "f2336578911cf3fc",
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN-FOLD / TEST-HOLDOUT validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55e30b4b94d16471"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDAZIONE TRAIN-FOLD / TEST-HOLDOUT ===\n",
      "✅ Risultati nested fold salvati su Excel.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALIDAZIONE TRAIN-FOLD / TEST-HOLDOUT ===\")\n",
    "val_results = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train = X_train.iloc[train_idx]\n",
    "    y_fold_train = y_train.iloc[train_idx]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "    # Si testa sempre sullo stesso 20%\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    cm, acc, prec, rec, f1 = get_metrics(y_test, y_pred)\n",
    "\n",
    "#    print(f\"\\n--- Fold {i} (train su fold, test fisso) ---\")\n",
    "#    print(f\"Accuracy: {acc:.4f}\")\n",
    "#    print(f\"Precision: {prec:.4f}\")\n",
    "#    print(f\"Recall: {rec:.4f}\")\n",
    "#    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "#    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "#    disp.plot(cmap=plt.cm.Blues)\n",
    "#    plt.title(f\"Confusion Matrix Fold {i}\")\n",
    "#    plt.show()\n",
    "\n",
    "    # Feature importances\n",
    "    importances = clf.feature_importances_\n",
    "    importances_dict = {f'Imp_{feat}': imp for feat, imp in zip(X.columns, importances)}\n",
    "\n",
    "    # Salva i risultati del fold\n",
    "    val_results.append({\n",
    "        'Prova': Prova,\n",
    "        'KFold' : i,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion_Matrix': cm.tolist(),\n",
    "        ' ': None,  # cella vuota per separare le metriche dalle importances\n",
    "        **importances_dict\n",
    "    })\n",
    "\n",
    "# Crea il DataFrame\n",
    "df_nested = pd.DataFrame(val_results)\n",
    "\n",
    "# Se esiste già, aggiungi i dati al foglio\n",
    "if os.path.exists(excel_path):\n",
    "    with pd.ExcelFile(excel_path) as reader:\n",
    "        if 'TrainFold_TestFixed_Test' in reader.sheet_names:\n",
    "            prev_nested = pd.read_excel(reader, sheet_name='TrainFold_TestFixed_Test')\n",
    "            df_nested = pd.concat([prev_nested, df_nested], ignore_index=True)\n",
    "            \n",
    "# Salva su Excel\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    df_nested.to_excel(writer, sheet_name='TrainFold_TestFixed_Test', index=False)\n",
    "\n",
    "print(\"✅ Risultati nested fold salvati su Excel.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T20:15:45.743231600Z",
     "start_time": "2025-06-01T20:15:39.605578300Z"
    }
   },
   "id": "58aa474531f64e1e",
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creazione dataset test per complicanze"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "144f3d6e4cfb0b8d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suddivisione completata:\n",
      "  - Train (Classification_train.csv): 581 righe\n",
      "  - Test  (Classification_test.csv):  142  righe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Vengono letti i due file\n",
    "parte1 = pd.read_csv(\"Excel/Parte1.csv\")\n",
    "parte2 = pd.read_csv(\"Excel/parte2_v1.csv\")\n",
    "\n",
    "# 1.1) Viene rinominata la feature ID in Patiene_ID per perettere il merge\n",
    "parte2 = parte2.rename(columns={\"ID\": \"Patient_ID\"})\n",
    "\n",
    "# 1.2) Viene fatto un inner‐merge su Patient_ID:\n",
    "clust = parte1.merge(parte2, on=\"Patient_ID\", how=\"inner\")\n",
    "\n",
    "# 2) Viene letto Diagnostics.csv per ricavare i codici associati a ciascun paziente\n",
    "diag = pd.read_csv(\"Excel/Diagnostics.csv\")\n",
    "\n",
    "# 2.1) Per ogni paziente, viene prenso il Code con il maggior numero di occorrenze\n",
    "diag_unique = (\n",
    "    diag\n",
    "    .groupby(\"Patient_ID\")[\"Code\"]\n",
    "    .agg(lambda codes: codes.value_counts().idxmax())\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Code\": \"CodeStrat\"})\n",
    ")\n",
    "\n",
    "# 2.2) Se hai codici troppo rari vengono raggruppati sotto \"RARE\" per evitare errori di stratify\n",
    "code_counts = diag_unique[\"CodeStrat\"].value_counts()\n",
    "rare_codes = set(code_counts[code_counts < 2].index)\n",
    "diag_unique[\"CodeStrat\"] = diag_unique[\"CodeStrat\"].apply(\n",
    "    lambda c: \"RARE\" if c in rare_codes else c\n",
    ")\n",
    "\n",
    "# 3) Suddividi i soli pazienti “diagnosticati” 80/20, stratificando su CodeStrat\n",
    "diag_train_patients, diag_test_patients = train_test_split(\n",
    "    diag_unique[\"Patient_ID\"],\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=diag_unique[\"CodeStrat\"]\n",
    ")\n",
    "\n",
    "# 4) Trova i pazienti in clust che NON compaiono in diag_unique (ossia “senza diagnosi”)\n",
    "all_clust_patients = set(clust[\"Patient_ID\"].unique())\n",
    "diag_patients = set(diag_unique[\"Patient_ID\"].unique())\n",
    "no_diag_patients = list(all_clust_patients - diag_patients)\n",
    "\n",
    "# 5) Dividi i pazienti “senza diagnosi” (%) 80/20 in modo casuale\n",
    "no_diag_train, no_diag_test = train_test_split(\n",
    "    no_diag_patients,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) Costruisci la lista finale di test (diagnosticati nel 20% + senza diagnosi nel 20%)\n",
    "test_patients  = set(diag_test_patients)  | set(no_diag_test)\n",
    "train_patients = set(diag_train_patients) | set(no_diag_train)\n",
    "\n",
    "# Verifica che non ci siano sovrapposizioni\n",
    "assert train_patients.isdisjoint(test_patients), \"Errore: un paziente è in entrambi i set!\"\n",
    "\n",
    "# 7) Filtra clust in base a Patient_ID: \n",
    "#    - se Patient_ID ∈ test_patients → va in df_test\n",
    "#    - altrimenti va in df_train\n",
    "df_train = clust[~clust[\"Patient_ID\"].isin(test_patients)].copy()\n",
    "df_test  = clust[ clust[\"Patient_ID\"].isin(test_patients)].copy()\n",
    "\n",
    "# 8) Rimuovi di nuovo la colonna Patient_ID, come richiesto, prima di salvare\n",
    "df_train_final = df_train.drop(columns=[\"Patient_ID\"])\n",
    "df_test_final  = df_test.drop(columns=[\"Patient_ID\"])\n",
    "\n",
    "# 9) Salva i due file risultanti\n",
    "df_train_final.to_csv(\"Excel/Classification_train.csv\", index=False)\n",
    "df_test_final.to_csv(\"Excel/Classification_test.csv\",  index=False)\n",
    "\n",
    "print(\"Suddivisione completata:\")\n",
    "print(f\"  - Train (Classification_train.csv): {len(df_train_final)} righe\")\n",
    "print(f\"  - Test  (Classification_test.csv):  {len(df_test_final)}  righe\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T21:13:00.675369600Z",
     "start_time": "2025-06-01T21:13:00.424225200Z"
    }
   },
   "id": "9923dd9978c0932b",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5acf103d1dc4fc2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
